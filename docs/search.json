[
  {
    "objectID": "reading.html",
    "href": "reading.html",
    "title": "List of My Reading",
    "section": "",
    "text": "By James Ladyman and Karoline Wiesner\nMy final-year MSci project involved the study of a complex system, foreign exchange markets. Each individual currency was treated as a spin in the Ising model, and by applying E.T. Jaynes’s principle of maximum entropy, we discovered the structure of interactions between these entities. Remarkably, this simple model of magnetism can also describe the phase transitions of liquid-gas systems, as they fall into the same universality class. This model has even been extended to study how critical our brain is, giving rise to the field of the critical brain hypothesis. Therefore, many ideas during my research project were derived from neuroscience papers. The financial market and the brain: both are complex systems.\nAbout a year ago, I read a paper published by James Ladyman, a professor of philosophy of science at Bristol, titled ‘What is a Complex System?’ (Ladyman, Lambert, and Wiesner 2013). As I started my PhD at Bristol, my second supervisor, Francesco, mentioned that he also published a book about complexity. It became clear to me that I should read this book.\n\nLadyman, James, James Lambert, and Karoline Wiesner. 2013. “What Is a Complex System?” European Journal for Philosophy of Science 3: 33–67.\nChapter 1 presents The Truisms of Complexity Science as follows:\n1. More is different.\n2. Nonliving systems can generate order.\n3. Complexity can come from simplicity.\n4. Coordinated behaviour does not require an overall controller.\n5. Complex systems are often modelled as networks or information processing systems.\n6. There are various kinds of invariance and forms of universal behaviour in complex systems.\n7. Complexity science is computational and probabilistic.\n8. Complexity science involves multiple disciplines.\n9. There is a difference between the order that complex systems produce and the order of the complex systems themselves.\nFeatures that are necessary and sufficient for which kinds of complexity and complex system are as follows:\n1. Numerosity: complex systems involve many interactions among many components.\n2. Disorder and diversity: the interactions in a complex system are not coordinated or controlled centrally, and the components may differ.\n3. Feedback: the interactions in complex systems are iterated so that there is feedback from previous interactions on a time scale relevant to the system’s emergent dynamics.\n4. Non-equilibrium: complex systems are open to the environment and are often driven by something external.\n5. Spontaneous order and self-organisation: complex systems exhibit structure and order that arises out of the interactions among their parts.\n6. Nonlinearity: complex systems exhibit nonlinear dependence on parameters or external drivers.\n7. Robustness: the structure and function of complex systems is stable under relevant perturbations.\n8. Nested structure and modularity: there may be multiple scales of structure, clustering and specialisation of function in complex systems.\n9. History and memory: complex systems often require a very long history to exist and often store information about history.\n10. Adaptive behaviour: complex systems are often able to modify their behaviour depending on the state of the environment and the predictions they make about it.\n\n2024/9/27\nReading What is a Complex System? by James Ladyman and Karoline Wiesner. The purpose of reading is to understand how systems out of thermodynamic equilibrium (a feature of complex systems) relate to a wide range of concepts associated with complexity.\n• There was a video that helped clarify what is meant by a Markov chain and a stochastic process being stationary: Markov Chains Clearly Explained! Part - 1. His other videos are helpful as well. For example, when I saw \\(P_{ij}^{(n)} = A_{ij}^n\\) from the n-Step Transition Matrix video (Part 3), it was quite surprising. However, the Part 5 video on hidden Markov models wasn’t very helpful, but reading the appendix in the book was sufficient.\n• What I’ve felt from reading this book (currently at p.90) is that it presents a lot of different ideas and has interesting discussions on the history of science, but I only seem to get excited when it starts mentioning the brain. I’m not sure if I’m necessarily interested in quantifying complexity. From the Apple notes on 28/9/2024:\nAs I’m reading through What is a Complex System?, it seems that the description of a complex system arises as we have more representations at different scales of analysis. The concept of ‘complexity’ was destined to arise as the scientific era evolved.\nNon-equilibrium systems arise because we define what systems are in thermodynamic equilibrium, which are idealisations.\nThings that fail to be idealised fall into this dualistic concept: non-equilibrium.\n\n2024/10/3\nIt took about two weeks to finish this book. It was an accessible read until I reached chapter 4, which was Measuring Features of Complex Systems. In the end, I’m glad that I read it, even though I somehow felt rushed to finish as there were other responsibilities to manage continuously, such as setting up clusters for my university Mac, teaching, office setup, etc. Most importantly, from now on, I am going to focus on reading material that is directly related to my project since it may give the impression to my supervisors that I’m not working hard. Reading a book like this takes a long time, as there are many new terms I had not encountered before.\nAnyway, it was a good read. Would I buy a physical copy for myself? Maybe not, but it was a good one.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#what-is-a-complex-system",
    "href": "reading.html#what-is-a-complex-system",
    "title": "List of My Reading",
    "section": "",
    "text": "By James Ladyman and Karoline Wiesner\nMy final-year MSci project involved the study of a complex system, foreign exchange markets. Each individual currency was treated as a spin in the Ising model, and by applying E.T. Jaynes’s principle of maximum entropy, we discovered the structure of interactions between these entities. Remarkably, this simple model of magnetism can also describe the phase transitions of liquid-gas systems, as they fall into the same universality class. This model has even been extended to study how critical our brain is, giving rise to the field of the critical brain hypothesis. Therefore, many ideas during my research project were derived from neuroscience papers. The financial market and the brain: both are complex systems.\nAbout a year ago, I read a paper published by James Ladyman, a professor of philosophy of science at Bristol, titled ‘What is a Complex System?’ (Ladyman, Lambert, and Wiesner 2013). As I started my PhD at Bristol, my second supervisor, Francesco, mentioned that he also published a book about complexity. It became clear to me that I should read this book.\n\nLadyman, James, James Lambert, and Karoline Wiesner. 2013. “What Is a Complex System?” European Journal for Philosophy of Science 3: 33–67.\nChapter 1 presents The Truisms of Complexity Science as follows:\n1. More is different.\n2. Nonliving systems can generate order.\n3. Complexity can come from simplicity.\n4. Coordinated behaviour does not require an overall controller.\n5. Complex systems are often modelled as networks or information processing systems.\n6. There are various kinds of invariance and forms of universal behaviour in complex systems.\n7. Complexity science is computational and probabilistic.\n8. Complexity science involves multiple disciplines.\n9. There is a difference between the order that complex systems produce and the order of the complex systems themselves.\nFeatures that are necessary and sufficient for which kinds of complexity and complex system are as follows:\n1. Numerosity: complex systems involve many interactions among many components.\n2. Disorder and diversity: the interactions in a complex system are not coordinated or controlled centrally, and the components may differ.\n3. Feedback: the interactions in complex systems are iterated so that there is feedback from previous interactions on a time scale relevant to the system’s emergent dynamics.\n4. Non-equilibrium: complex systems are open to the environment and are often driven by something external.\n5. Spontaneous order and self-organisation: complex systems exhibit structure and order that arises out of the interactions among their parts.\n6. Nonlinearity: complex systems exhibit nonlinear dependence on parameters or external drivers.\n7. Robustness: the structure and function of complex systems is stable under relevant perturbations.\n8. Nested structure and modularity: there may be multiple scales of structure, clustering and specialisation of function in complex systems.\n9. History and memory: complex systems often require a very long history to exist and often store information about history.\n10. Adaptive behaviour: complex systems are often able to modify their behaviour depending on the state of the environment and the predictions they make about it.\n\n2024/9/27\nReading What is a Complex System? by James Ladyman and Karoline Wiesner. The purpose of reading is to understand how systems out of thermodynamic equilibrium (a feature of complex systems) relate to a wide range of concepts associated with complexity.\n• There was a video that helped clarify what is meant by a Markov chain and a stochastic process being stationary: Markov Chains Clearly Explained! Part - 1. His other videos are helpful as well. For example, when I saw \\(P_{ij}^{(n)} = A_{ij}^n\\) from the n-Step Transition Matrix video (Part 3), it was quite surprising. However, the Part 5 video on hidden Markov models wasn’t very helpful, but reading the appendix in the book was sufficient.\n• What I’ve felt from reading this book (currently at p.90) is that it presents a lot of different ideas and has interesting discussions on the history of science, but I only seem to get excited when it starts mentioning the brain. I’m not sure if I’m necessarily interested in quantifying complexity. From the Apple notes on 28/9/2024:\nAs I’m reading through What is a Complex System?, it seems that the description of a complex system arises as we have more representations at different scales of analysis. The concept of ‘complexity’ was destined to arise as the scientific era evolved.\nNon-equilibrium systems arise because we define what systems are in thermodynamic equilibrium, which are idealisations.\nThings that fail to be idealised fall into this dualistic concept: non-equilibrium.\n\n2024/10/3\nIt took about two weeks to finish this book. It was an accessible read until I reached chapter 4, which was Measuring Features of Complex Systems. In the end, I’m glad that I read it, even though I somehow felt rushed to finish as there were other responsibilities to manage continuously, such as setting up clusters for my university Mac, teaching, office setup, etc. Most importantly, from now on, I am going to focus on reading material that is directly related to my project since it may give the impression to my supervisors that I’m not working hard. Reading a book like this takes a long time, as there are many new terms I had not encountered before.\nAnyway, it was a good read. Would I buy a physical copy for myself? Maybe not, but it was a good one.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#hydrophobicity-across-length-scales-the-role-of-surface-criticality",
    "href": "reading.html#hydrophobicity-across-length-scales-the-role-of-surface-criticality",
    "title": "List of My Reading",
    "section": "2024/10/4 - Hydrophobicity Across Length Scales: The Role of Surface Criticality",
    "text": "2024/10/4 - Hydrophobicity Across Length Scales: The Role of Surface Criticality\nBy Mary Kathryn Coe\n\nQ1. Resolved\n\n\n\nOn page 35, Chapter 2.1\n\n\nI’m not entirely sure why the surface tension is defined in that way, as it’s normally considered force per unit area. So, I asked ChatGPT:\n\n\nClick to reveal the answer!\n\nThe definition of surface tension in this context is derived from the thermodynamic perspective, which connects surface tension to the grand potential difference between a bulk fluid and a system with an interface. The reasoning follows these steps:\n\nGrand Potential Definition: The grand potential, \\(\\Omega\\), is given by: \\[\n\\Omega = U - TS - \\mu N\n\\] This expression is crucial for systems where particle number and volume are allowed to fluctuate, such as liquid-vapour interfaces.\nSurface Tension from Thermodynamics: Surface tension, \\(\\gamma\\), represents the energy cost per unit area to create an interface. In thermodynamic terms, this corresponds to how much the grand potential increases when introducing an interface between coexisting phases (liquid and vapour).\nDifference between Bulk and Interface: The bulk grand potential, \\(\\Omega_b\\), corresponds to a system without an interface (uniform bulk). When an interface is introduced, the grand potential changes, denoted as \\(\\Omega_{\\text{ex}}\\), the excess grand potential: \\[\n\\Omega_{\\text{ex}} = \\Omega - \\Omega_b\n\\]\nSurface Tension as Excess Grand Potential per Unit Area: The surface tension is then defined as the excess grand potential per unit area, \\(A\\), required to form the interface: \\[\n\\gamma = \\frac{\\Omega_{\\text{ex}}}{A} = \\frac{\\Omega - \\Omega_b}{A}\n\\] This formulation makes sense because surface tension describes the energetic cost to create a unit area of interface between two phases.\n\nIn summary, while surface tension is often discussed in terms of force per unit length in simpler mechanical contexts, in thermodynamics, it is expressed as the excess grand potential per unit area. This ties into how interfaces modify the system’s energy in comparison to the bulk. The two definitions are equivalent but applied in different contexts: mechanical (force/length) vs. thermodynamic (energy/area).\n\n\n\nDerivation of equation 2.1.8, 2.1.9, 2.1.10\n\n\\[\n\\Omega_b(\\mu, V, T) = U - TS - \\mu N = -pV\n\\]\nTaking the differential:\n\\[\nd\\Omega_b = d(U - TS - \\mu N)\n\\]\nUsing the first law of thermodynamics:\n\\[\ndU = TdS - pdV + \\mu dN\n\\]\nDifferentiating \\(TS\\):\n\\[\nd(TS) = TdS + SdT\n\\]\nDifferentiating \\(\\mu N\\):\n\\[\nd(\\mu N) = \\mu dN + N d\\mu\n\\]\nSubstituting these into the differential for \\(\\Omega_b\\):\n\\[\nd\\Omega_b = (TdS - pdV + \\mu dN) - (TdS + SdT) - (\\mu dN + N d\\mu)\n\\]\nSimplifying:\n\\[\nd\\Omega_b = -S_b dT - N_b d\\mu - p dV\n\\]\nFor constant volume (\\(dV = 0\\)), the volume is assumed to remain constant because we are focusing on surface effects rather than bulk volume changes.:\n\\[\nd\\Omega_b = -S_b dT - N_b d\\mu\n\\]\nNow for equation (2.1.6):\n\\[\n\\Omega(\\mu, V, T) = U - TS - \\mu N = -pV + \\gamma A\n\\]\nTaking the differential:\n\\[\nd\\Omega = d(U - TS - \\mu N + \\gamma A)\n\\]\nSubstituting differentials:\n\\[\nd\\Omega = TdS - pdV + \\mu dN - (TdS + SdT) - (\\mu dN + N d\\mu) + \\gamma dA + A d\\gamma\n\\]\nSimplifying:\n\\[\nd\\Omega = -S dT - N d\\mu + \\gamma dA\n\\]\nNow subtract:\n\\[\nd\\Omega - d\\Omega_b = (-S dT - N d\\mu + \\gamma dA) - (-S_b dT - N_b d\\mu)\n\\]\nSimplifying:\n\\[\nd\\Omega - d\\Omega_b = -(S - S_b)dT - (N - N_b)d\\mu + \\gamma dA\n\\]\nThis is equation (2.1.8).\n\nFrom equation (2.1.7):\n\\[\n\\gamma = \\frac{\\Omega_{\\text{ex}}}{A} = \\frac{\\Omega - \\Omega_b}{A}\n\\]\nTaking the differential:\n\\[\nd\\gamma = \\frac{d\\Omega_{\\text{ex}}}{A} = \\frac{d\\Omega - d\\Omega_b}{A}\n\\]\nNow, use the result from equation (2.1.8):\n\\[\nd\\Omega - d\\Omega_b = -(S - S_b) dT - (N - N_b) d\\mu + \\gamma dA\n\\]\nSo:\n\\[\nd\\Omega_{\\text{ex}} = -s_{\\text{ex}} A dT - \\Gamma d\\mu + \\gamma dA\n\\]\nwhere:\n\n\\(s_{\\text{ex}} = \\frac{S - S_b}{A}\\) is the surface excess entropy per unit area,\n\\(\\Gamma = N - N_b\\) is the excess number of particles (adsorption).\n\nDividing through by \\(A\\), we get:\n\\[\n\\frac{d\\Omega_{\\text{ex}}}{A} = -s_{\\text{ex}} dT - \\frac{\\Gamma}{A} d\\mu + \\gamma \\frac{dA}{A}\n\\]\nNow, using the fact that \\(\\gamma \\frac{dA}{A}\\) is small for constant area, we can ignore this term, giving:\n\\[\ns_{\\text{ex}} dT + \\frac{\\Gamma}{A} d\\mu + d\\gamma = 0\n\\]\nThis is equation (2.1.9).\n\nRearrange equation (2.1.9):\n\\[\n\\frac{\\Gamma}{A} d\\mu = -s_{\\text{ex}} dT - d\\gamma\n\\]\nAt constant temperature (\\(dT = 0\\)), this simplifies to:\n\\[\n\\frac{\\Gamma}{A} d\\mu = -d\\gamma\n\\]\nNow, dividing both sides by \\(d\\mu\\), we get:\n\\[\n\\frac{\\Gamma}{A} = -\\left(\\frac{\\partial \\gamma}{\\partial \\mu}\\right)_T\n\\]\nThis is equation (2.1.10).\n\nAnd I need to ask about equations 2.1.11 to 2.1.13, as I cannot find reference [40].\n\n\nQ2. Resolved\n\n\n\nOn page 40, Chapter 2.3\n\n\nI wasn’t sure what the one-particle density is or its use, so I asked ChatGPT:\n\n\nClick to reveal the answer!\n\nThe one-particle density function, often denoted as \\(\\rho^{(1)}(\\mathbf{r})\\), represents the probability density of finding a particle at a specific position \\(\\mathbf{r}\\) in a system of particles. It describes how particles are distributed in space on average and is a key observable in statistical mechanics and the study of fluids.\nThe equation given is:\n\\[\n\\hat{\\rho}^{(1)}(\\mathbf{r}) = \\sum_{j=1}^{N} \\delta(\\mathbf{r} - \\mathbf{r}_j)\n\\]\nThis is the one-particle density operator, and it has the following meaning:\n\nDirac Delta Function: \\(\\delta(\\mathbf{r} - \\mathbf{r}_j)\\) is a Dirac delta function that ‘picks out’ the position of the \\(j\\)-th particle. It is equal to zero everywhere except at \\(\\mathbf{r}_j\\), where it is infinite, ensuring that it integrates to 1. Thus, it counts the presence of a particle at \\(\\mathbf{r}_j\\).\nSummation over All Particles: The sum runs over all particles \\(j\\) from 1 to \\(N\\), so this operator adds up the contributions from all particles in the system, effectively counting the number of particles at position \\(\\mathbf{r}\\).\n\nIn essence, the one-particle density operator \\(\\hat{\\rho}^{(1)}(\\mathbf{r})\\) gives the total number of particles at position \\(\\mathbf{r}\\) in the system. Since it is an operator, the actual one-particle density \\(\\langle \\hat{\\rho}^{(1)}(\\mathbf{r}) \\rangle\\), as defined in equation (2.3.2), is obtained by averaging over all possible configurations of the system in the given ensemble (such as the grand canonical ensemble).\nWhy Does It Look Like This?\nThe form of equation (2.3.1) stems from how we represent the microscopic distribution of particles in space. The Dirac delta function ensures that for each particle, its position contributes to the overall density only at the exact location where that particle exists. This is a standard way to construct density operators in statistical mechanics because:\n\nThe delta function precisely pinpoints the position of a particle.\nSumming over all particles ensures that the density operator accounts for the total number of particles present in the system.\n\nThis representation allows us to describe spatially inhomogeneous systems where the density may vary across different regions (e.g., near a surface or an external potential). For homogeneous fluids, where particles are evenly distributed, this density would reduce to a constant value (the bulk density \\(\\rho_b\\)).\nSummary\nThe one-particle density operator, \\(\\hat{\\rho}^{(1)}(\\mathbf{r})\\), reflects how particles are distributed in space at a microscopic level. The delta functions track the positions of individual particles, and summing over all particles gives the total density at position \\(\\mathbf{r}\\). The actual one-particle density function \\(\\rho^{(1)}(\\mathbf{r})\\) is then the ensemble average of this operator over all configurations of the system.\n\n\n\nQ3. Resolved\n\n\n\nOn page 43, Chapter 2.3\n\n\nI wasn’t sure why each variable is specifically chosen to form those equations. For example, in section 2.4.3, I do not understand why λ can be factored out of g on the right-hand side when λ has two different exponents, a and b. So, I asked ChatGPT:\n\n\nClick to reveal the answer!\n\nStarting with Equation (2.4.3):\n\\[\ng(\\lambda^a u, \\lambda^b v) = \\lambda g(u, v)\n\\]\nThis equation states that the function \\(g(u, v)\\) is a generalised homogeneous function. To understand why it takes this form and why \\(\\lambda\\) appears outside the function on the right-hand side, let’s break this down:\n\nHomogeneous Functions: A function is called homogeneous of degree \\(\\alpha\\) if, when all its arguments are scaled by a factor \\(\\lambda\\), the function itself scales by \\(\\lambda^\\alpha\\). Mathematically, for a homogeneous function \\(f(x, y)\\), we would have: \\[\nf(\\lambda x, \\lambda y) = \\lambda^\\alpha f(x, y)\n\\] This property is common in critical phenomena where we are dealing with power-law behaviour near critical points.\nGeneralised Homogeneous Functions: The function \\(g(u, v)\\) is a generalised homogeneous function because it allows different exponents for different variables. In this case, the variables \\(u\\) and \\(v\\) scale with different exponents \\(a\\) and \\(b\\) under the same scaling factor \\(\\lambda\\).\n\n\\(\\lambda^a u\\): Here, \\(u\\) is scaled by \\(\\lambda^a\\). This implies that \\(u\\) behaves differently under scaling compared to \\(v\\).\n\\(\\lambda^b v\\): Similarly, \\(v\\) is scaled by \\(\\lambda^b\\).\n\nWhy \\(\\lambda\\) can be factored out: The key to understanding why \\(\\lambda\\) can be factored out on the right-hand side despite having different exponents for \\(u\\) and \\(v\\) lies in the definition of a generalised homogeneous function. By construction, we impose that when the variables are scaled in a particular way (with exponents \\(a\\) and \\(b\\)), the function scales by \\(\\lambda\\) itself. This is a property we assign to \\(g(u, v)\\). It doesn’t mean that \\(\\lambda\\) is constant; rather, the entire function \\(g(u, v)\\) is designed to respond to the scaling in this specific manner.\nThe exponents \\(a\\) and \\(b\\) control how \\(u\\) and \\(v\\) individually respond to scaling, but the overall response of the function is to scale as \\(\\lambda\\). The scaling of the function on the right-hand side doesn’t contradict the fact that \\(\\lambda\\) has different exponents inside the arguments. This is simply a mathematical constraint on how the function \\(g(u, v)\\) behaves under scaling.\nIn other words, the form: \\[\ng(\\lambda^a u, \\lambda^b v)\n\\] ensures that the function scales uniformly by \\(\\lambda\\), despite the individual variables scaling differently.\n\nEquation (2.4.4):\nNow, let’s explain how this scaling idea applies to the singular part of the free energy in equation (2.4.4):\n\\[\n\\omega_{\\text{sing}}(\\delta \\mu, t) = \\lambda^{-d} \\omega_{\\text{sing}}(\\lambda^a \\delta \\mu, \\lambda^b t)\n\\]\n\nVariables:\n\n\\(\\delta \\mu = \\mu - \\mu_{\\text{co}}\\) is the difference between the chemical potential \\(\\mu\\) and the critical chemical potential \\(\\mu_{\\text{co}}\\).\n\\(t\\) is the reduced temperature: \\(t = \\frac{T - T_c}{T_c}\\).\n\\(d\\) is the dimensionality of the system (important for scaling relations).\n\nApplying the Scaling Relation: Here, the function \\(\\omega_{\\text{sing}}\\), which represents the singular part of the free energy, is assumed to be a generalised homogeneous function. This means it should follow a scaling law similar to the one introduced in equation (2.4.3).\nThe variables \\(\\delta \\mu\\) and \\(t\\) are scaled by \\(\\lambda^a\\) and \\(\\lambda^b\\), respectively, where \\(a\\) and \\(b\\) are scaling exponents that determine how \\(\\delta \\mu\\) and \\(t\\) change when scaled by \\(\\lambda\\).\nWhy \\(\\lambda^{-d}\\)?\n\nThe factor \\(\\lambda^{-d}\\) appears because we are dealing with free energy, which is an extensive quantity. In thermodynamics, extensive quantities scale with the volume of the system, and in \\(d\\)-dimensional space, the volume scales as \\(\\lambda^d\\). Since we are dealing with the singular part of the free energy, the scaling reflects the dimensionality of the system. The minus sign arises because, under scaling, the free energy decreases as the system grows larger.\n\nThe Free Choice of \\(\\lambda\\): The scaling factor \\(\\lambda\\) is arbitrary, meaning you can choose its value to simplify the equation. In this case, we choose \\(\\lambda^b = t^{-1}\\), which simplifies the second argument (the temperature dependence) of \\(\\omega_{\\text{sing}}\\). By doing this, we reduce the complexity of the equation and make the temperature scaling explicit.\n\nEquation (2.4.5):\nThis equation shows the final form after choosing \\(\\lambda^b = t^{-1}\\):\n\\[\n\\omega_{\\text{sing}}(\\delta \\mu, t) = t^{d/b} \\omega_{\\text{sing}}(t^{-a/b} \\delta \\mu, 1)\n\\]\n\nWhy this form? By setting \\(\\lambda^b = t^{-1}\\), we have scaled out the temperature dependence in the second argument of \\(\\omega_{\\text{sing}}\\). This isolates the temperature dependence into the prefactor \\(t^{d/b}\\), while leaving a scaling function that depends only on the ratio \\(\\delta \\mu / t^{a/b}\\), which is now the only scaling variable.\n\n\\(t^{d/b}\\): This represents how the singular part of the free energy depends on temperature near the critical point. The exponent \\(d/b\\) dictates how strongly the free energy diverges as \\(t \\to 0\\) (the critical point).\n\\(\\Sigma\\left( \\frac{\\delta \\mu}{t^{a/b}} \\right)\\): The function \\(\\Sigma\\) is the scaling function, which encodes the remaining dependence on the scaled chemical potential \\(\\delta \\mu / t^{a/b}\\).\n\n\nIn Summary: - In equation (2.4.3), the exponents \\(a\\) and \\(b\\) control how the variables \\(u\\) and \\(v\\) scale. Despite their different scaling behaviours, the overall function scales by \\(\\lambda\\) because it is designed as a generalised homogeneous function. - In equation (2.4.4), the singular part of the free energy near the critical point follows a similar scaling law, with the dimensionality \\(d\\) appearing as a factor to reflect the extensive nature of free energy. - In equation (2.4.5), we choose \\(\\lambda^b = t^{-1}\\) to simplify the scaling form, making the dependence on temperature explicit and reducing the complexity of the equation. The resulting expression separates the prefactor \\(t^{d/b}\\) and leaves a scaling function that depends only on the ratio \\(\\delta \\mu / t^{a/b}\\), which captures the critical behaviour.\n\n\n\nFluctuations? One correlated entity?\n\nIn the context of phase transitions, particularly near a critical point, the term “fluctuations” refers to fluctuations in the local density of the system. Let’s explain in detail what this means, especially in the context of critical opalescence and long-range correlations.\n\nDensity Fluctuations: When we talk about “density” in this context, we mean the number density of particles in a given volume. In simple terms, it’s how densely packed the particles are in a particular region. The density can fluctuate due to thermal motion, and these fluctuations are typically small and localised in non-critical conditions.\n\nHowever, as a system approaches the critical point of a phase transition (e.g., a liquid-vapour transition), the magnitude of these fluctuations grows dramatically.\n\nWhat happens near the critical point?\n\nCorrelation Length (\\(\\xi\\)): The correlation length \\(\\xi\\) represents the distance over which particles in the system are correlated or “communicate” with each other. Normally, this length is finite, and beyond this distance, the properties of the system in one region do not affect another region.\nNear the critical point, however, \\(\\xi \\to \\infty\\). This means that particles separated by very large distances (even macroscopic scales) are correlated with each other. The entire system essentially becomes one correlated entity.\nDiverging Fluctuations: As the correlation length grows, the fluctuations in the local density (i.e., how many particles are in a given region at any time) also grow larger. This happens because, when the correlation length is large, particles across large distances influence each other. As a result, fluctuations occur over all length scales, from small to very large.\nDensity Fluctuations: Specifically, these are fluctuations in the number of particles in a given volume. Imagine regions in a fluid where the particle density becomes temporarily higher or lower than the average. At the critical point, these fluctuations become very large and occur over very large distances, leading to the system appearing “cloudy” or “milky” — this is the phenomenon of critical opalescence.\n\n\nWhat exactly is fluctuating? At the critical point, density is the key fluctuating quantity, but more generally, the term “fluctuations” can refer to:\n\n\nOrder Parameter Fluctuations: Near a critical point, there is typically an order parameter that characterises the phase transition. For example, in the liquid-vapour transition, this order parameter could be the difference in density between the liquid and vapour phases. As the system approaches the critical point, the fluctuations in the order parameter become large.\nFor example, in a fluid near the liquid-vapour critical point:\n\nIn the liquid phase, particles are densely packed.\nIn the vapour phase, particles are more spread out.\n\nNear the critical point, there is no sharp distinction between liquid and vapour phases. Instead, the system fluctuates between states that resemble both phases. You might see a region that has the density of a liquid next to a region that has the density of a vapour, and these regions appear and disappear rapidly.\n\n\nWhy do these fluctuations occur? At the critical point, the system is in a delicate balance between two phases (e.g., liquid and vapour). The free energy landscape becomes very flat near the critical point, meaning that the system can fluctuate easily between different configurations with very little energy cost. These fluctuations span all length scales, leading to the phenomenon where large regions of the system can fluctuate in density.\nCritical Opalescence: The critical opalescence mentioned in the text is a direct consequence of these large fluctuations in density. Light passing through the fluid is scattered by the density fluctuations. Since the fluctuations occur over a wide range of length scales (from very small to very large), they scatter light across all wavelengths, leading to the “milky” appearance of the fluid near the critical point.\n\n\nWhy does light scatter? Light is scattered by regions of varying density because the refractive index of the fluid depends on its density. When the density fluctuates significantly over large distances, light passing through the fluid encounters regions of different densities, leading to scattering in all directions.\n\nSummary:\n\nWhat is fluctuating near the critical point?: Primarily, the number density (i.e., the number of particles per unit volume) fluctuates near the critical point.\nWhy do these fluctuations grow?: As the correlation length \\(\\xi\\) becomes large, regions of the system separated by large distances become correlated, allowing large-scale fluctuations in density.\nWhat’s the physical manifestation?: These fluctuations lead to observable phenomena such as critical opalescence, where light is scattered by the large fluctuations in density, causing the fluid to appear milky or cloudy.\n\nIn short, the “fluctuations” near the critical point refer to density fluctuations, which are tied to the divergence of the correlation length, and these fluctuations grow larger and affect larger regions of the system as the critical point is approached.\n\n\nFrom chapter 2.4.3, I decided to skim through this document as it became too technical to understand the entire thesis when it began with perturbation theory. Additionally, many concepts here are primarily from equilibrium statistical physics.\n\n\n\n\n\n\nNote\n\n\n\nChapter 2.4.5: Wetting and Drying Phase Transitions is very well written and full of useful knowledge. For the next project meeting, let’s prepare a slide to go over this.\n\n\nWetting and Drying Phase Transitions\nI’ve been reading up to Views of Hydrophobicity, but I realised that I’m not sure how much of the stuff I learn from this thesis is related to active systems. So, I decided to review all articles given by my supervisors.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Weekly Notes",
    "section": "",
    "text": "Busy weeks due to starting three teaching modules: Weekly update of my TSR (Teaching Support Roles). Looking back, I wish I could have completed more reading, as there isn’t much to talk about in terms of active matter research for the next project meeting on Monday.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#week-2",
    "href": "notes.html#week-2",
    "title": "Weekly Notes",
    "section": "",
    "text": "Busy weeks due to starting three teaching modules: Weekly update of my TSR (Teaching Support Roles). Looking back, I wish I could have completed more reading, as there isn’t much to talk about in terms of active matter research for the next project meeting on Monday.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#week-3",
    "href": "notes.html#week-3",
    "title": "Weekly Notes",
    "section": "Week 3",
    "text": "Week 3\n\n2024/9/30 Weekly project meeting\n\nThings to discuss\n\nMeeting with Max on Friday at noon (4th of Oct), thinking of going to Budapest Café\n‘Setting Expectations’ document\nTwo conferences to join:\n• The Dao of Complexity workshop\n• The Statistical Physics of Cognition\nSo, a trip to London, how to sort out things with Clarity, and other arrangements.\nSet up RDSF data storage (Though I checked, OneDrive for Business offers 2TB of storage: Overview of OneDrive for Business)\nBrief plan discussion: reading Mary Coe’s thesis, then Understanding Molecular Simulation book\nAre there MSci students working on this project? \n\n\n\n\n2024/10/2 Why are the clouds at the same height when I look at the cloudy sky?\n\n\n\nView from Brandon Hill\n\n\nWhen we observe clouds appearing at the same height, it’s often due to a phenomenon where a particular layer of the atmosphere has the right conditions for cloud formation. In the troposphere (the lowest layer of the atmosphere), clouds form when the air cools to its dew point, causing water vapour to condense into droplets or ice crystals. This typically happens at specific altitudes where temperature and pressure conditions are ideal for condensation.\nClouds that seem to form at the same height are likely part of the same atmospheric layer, known as a cloud base. The cloud base marks the altitude at which rising air reaches its dew point. If the conditions across the sky are uniform, we’ll see many clouds forming at roughly the same altitude, giving the illusion of a flat layer of clouds.\nCloud formation is fundamentally a non-equilibrium process. It results from dynamic atmospheric conditions like rising air currents, changes in temperature and pressure, and the continuous exchange of energy. These factors drive processes such as condensation and evaporation, which are inherently out of equilibrium. This aligns with the behaviour of active matter systems, where each particle consumes energy to move, keeping the system perpetually out of equilibrium.\nTo create an effective repulsion in our ABP simulations, we consider modifying the wall-fluid interactions to induce a torque that reorients particles away from the surface. One approach might be to introduce an anisotropic interaction potential near the wall. When an ABP approaches the surface, this potential could apply a torque that turns the particle’s propulsion direction away from the wall, effectively reducing its tendency to accumulate there.\n\n\n2024/10/5 Thesis writing\nAn idea of beginning the introduction of phase transitions\nThis thesis is ultimately about phase transitions. The central question is: what happens during a transition, for example, from water to ice, from a fluid to a superfluid, or from a paramagnet to a ferromagnet? The physics of such transitions is rich and complex. There are a number of different theoretical tools available to understand them. The study begins by examining thermodynamics and extends to the application of statistical mechanics to these transitions. Additionally, statistical field theory, a fully-fledged field theory similar to what is seen in particle physics or general relativity, will be applied. The goal is to use these approaches, along with concepts of symmetry and other properties, to understand different phases of matter.\nFor a long time, understanding phase transitions was a mystery, especially what’s known as a critical point. It’s an interesting situation: we have something as familiar as water, and it was not until the mid-20th century that significant progress was made. The phase behaviour of water can be examined using a pressure-temperature diagram:\n\n\n\nPhase diagram of water (Cmglee n.d.)\n\nCmglee. n.d. “Phase Diagram of Water Simplified.” https://commons.wikimedia.org/wiki/File:Phase_diagram_of_water_simplified.svg.\n\n\n\nOn the horizontal axis is temperature, and on the vertical axis is pressure.\nThe phase diagram for water features three regions: steam, water, and ice.\nThere’s a point called the triple point, where all three phases coexist, and a critical point, which marks the end of the liquid-gas boundary.\n\nWater is essential, as most of us are mostly water, and yet the physics of what happens at this critical point wasn’t understood for the longest time. It’s a system that is incredibly close to us, yet to understand it requires some of the deepest ideas in physics, particularly the renormalisation group.\nIn this sense, phase transitions form a uniquely positioned subject. They are simultaneously very close to our everyday experience and yet require extremely deep ideas to understand properly. Most other areas of physics are typically far removed from daily life. Quantum mechanics operates on microscopic scales, and astrophysics on cosmological scales, both far from what can be intuitively grasped. But here, some really deep, beautiful puzzles are found in the world around us.\nAn idea of beginning the introduction of renormalisation group\nSo we’re finally at the point now where we can look at the renormalisation group, or RG. We have gathered a bunch of pieces of evidence. Here’s our phase diagram; we have a critical point here. What have we seen near a critical point? We’ve seen that the correlation length goes to infinity; length scales diverge.\nWe’ve also seen the idea of universality, where different systems—for example, the liquid-gas phase transition and the Ising model—are the same. So we’ve observed that the correlation length \\(\\xi\\) goes to infinity, and many systems become similar near the critical point.\nThe renormalisation group is an idea that arises from these two observations. What’s really happening is that at the critical point, the system becomes scale-invariant. Suppose I have a physical system, and these are sort of blobs—say, of up spins or some kind of physical structure. If the correlation length is infinite, there are structures at all length scales. If I zoom out, if I coarse-grain the system, it should look similar. Because if I’m rescaling my system, and if the correlation length is finite, by zooming out, the correlation length of my new system becomes smaller. In my rescaled system, after I’ve zoomed out, things that were on long scales become short scales, and so the correlation length—which was on long scales—is now on short scales; the correlations become smaller.\nBut if the correlation length was infinite, when I zoomed out and rescaled my system, it would still look infinite. So what we see is that when I zoom out and rescale my system, I should see the same thing if the correlation length goes to infinity. And what this means immediately is that universality emerges. Because if I do that—if I have an idea where I zoom out, rescale, and it looks the same, zoom out, rescale, and it looks the same—near the critical point only, then all the microscopic details of my system just fall away. What I’m left with is some kind of universal theory, a universal theory of physics which doesn’t depend on the details of my model. So the fact that it was a liquid-gas transition or a magnet doesn’t make any difference to me. The only things that matter are the basic ideas of dimensionality and symmetry that underlie the transition, and everything else is irrelevant at the critical point because the correlation length diverges.\nThe renormalisation group is a way to implement this mathematically or in an actual model. So the whole idea is to take your system near criticality, rescale it, and study how it changes under these rescalings. What we hope to find are fixed points of the rescaling—so where you zoom out, and the system looks the same—and these fixed points become critical points of the system. The fixed points of the RG transformations correspond to critical points of the physical system. From this, you can derive the critical exponents from the sort of scaling near the critical points.\nSo this is the idea of the renormalisation group, and you can see how it emerges naturally and explains basically everything. There’s this idea that fluctuations become very important, and the correlation length diverges, and therefore we can look at this rescaling transformation. In this sense, the theory gives us the non-mean-field critical exponents, and we also see this universality emerge.\nSo the renormalisation group is really an idea that ties a lot of puzzles together, a lot of systems together, and it’s sufficiently important that it was awarded a Nobel Prize for pure condensed matter theory, which is a rare achievement for Ken Wilson in the 1970s.\nThat’s the basic idea of the renormalisation group. The problem ultimately with RG is that to implement it in anything other than the simplest of systems is a technical nightmare. It’s extremely difficult, takes a long time, and the results are very hard to obtain. However, people have done that, and it’s correct—it works; it’s just very hard. So that’s the idea, the philosophy of the RG, and it’s one of the most beautiful ideas in theoretical physics.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#week-4",
    "href": "notes.html#week-4",
    "title": "Weekly Notes",
    "section": "Week 4",
    "text": "Week 4\n\n2024/10/7 Weekly project meeting\n\nThings to discuss\n\nOverleaf template for the thesis, as I will start writing what I learn for either the introduction or theory sections. Found one: Link\n\n\n\n\n2024/10/8 Analytical Mechanics\nI’ve been watching Tom’s lectures on Analytical Mechanics once more. I’ve just finished part 2. The topics are:\n\nCalculus of variations\nLagrangian mechanics\nHamiltonian mechanics\n\nWhy did I start watching it?\nWell, during the last project meeting, I had a naïve question about whether we could formulate a Lagrangian for the drying scenario. For example, in my supervisor’s papers, they couldn’t manage to observe the drying transition in their setup. So, I asked if, when the trajectory of all particles is away from the surface, we could possibly derive an analytical equation of forces between the particles and the wall to induce torque. Just a pure naïve idea.\nNigel mentioned that the principle of least action makes sense in equilibrium settings, so I had to revise that concept. The Lagrangian is the kinetic energy minus the potential energy, whatever this quantity is. In non-equilibrium systems, energy keeps changing since active agents continuously consume energy. Also, the effective potential depends on the properties of the wall. So, the Lagrangian would become time-dependent, which now makes sense based on what Nigel said.\nThe beautiful theorem was revisited from his lectures: Noether’s theorem.\n\nIf the laws of physics don’t depend on time, energy is conserved.\nSimilarly, if the system has translational or rotational invariance, momentum or angular momentum is conserved, respectively.\n\nThis is beautiful: conserved quantities derive from the symmetries of the system. It also implies that since energy is not conserved for the system I’m interested in, the equations of motion would be time-dependent. I think it would be useful to pay attention to how the equations of motion for active Brownian particles are formulated once I revise their papers.\nAs for his lectures, I’ll continue with part 3 since I’ve already watched the first two. That said, I remember his lecture notes being better than the video recordings, although his video lectures on phase transitions were much more inspiring. So, a note for my future self: when revising the content of the analytical formulation of mechanics, read his lecture notes.\n\n\nPrograms I made\nBefore doing that, I made a Python program that:\n\ndownloads all lectures from Mediasite from the unit I select,\ndetects and removes sections of silence in both the video and audio tracks, applying a buffer of 0.5 seconds before and after each silence to make the transitions smoother,\nsyncs the video and audio together seamlessly to create an mp4 file (the original files were mov, which were almost 10x larger in size),\nkeeps the system active throughout the process by using caffeinate to prevent the computer from going to sleep,\ntracks and reports the download status, ensuring that any interrupted downloads are handled smoothly by automatically updating cookies for authentication,\nensures frame rate consistency to avoid any sync issues that might arise due to different frame rates,\nautomatically manages temporary files created during the silence detection process, keeping the system clean after the task is done.\n\nI’m not sure if I’m allowed to share this code on GitHub, but it was a fun project.\n\n\n\nDownloaded lectures\n\n\nWhy did I make it? Because…\n\nI’ve always felt lazy logging into Blackboard (the university website) every time I wanted to watch videos during my undergraduate years.\nI personally wanted to collect the lectures before they disappeared from the website.\nThere are many pauses in the lectures where the lecturer doesn’t speak while writing equations or words. Removing these pauses also saves total viewing time. Observe the difference:\n\nTotal time to watch all videos of AM (Analytical Mechanics):\n\nAM: 9 hours, 18 minutes, 34 seconds\nProcessed_AM: 8 hours, 23 minutes, 50 seconds\nProcessed_AM (no buffer of 0.5s): 7 hours, 23 minutes, 28 seconds\n\nTotal time to watch all videos of PT (Physics of Phase Transitions):\n\nPT: 11 hours, 42 minutes, 19 seconds\nProcessed_PT: 10 hours, 49 minutes, 18 seconds\nProcessed_PT (no buffer of 0.5s): 9 hours, 45 minutes, 29 seconds\n\nThe buffer is necessary as it makes the transitions feel more natural by introducing a brief pause rather than cutting the silences completely. Watch an example below (duration change: 18:09 (before processing) → 15:29 (after processing with buffers)):\n\n\nVideo\nAM 2.5a Noether’s Theorem\n\n\n\nA beautiful quote from Tom:\nFrom AM 3.4, classical mechanics is the geometry of phase space. All of the geometries (symplectic form, canonical transformations, etc.) describe the phenomenology of classical mechanics.\n\n\n\n2024/10/12 Making a presentation\n241016 Overleaf Presentation for Project Meeting",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project Description",
    "section": "",
    "text": "In terms of the project on ‘Active Drying’, drying is an equilibrium surface phase transition that occurs when a fluid at liquid-vapour coexistence is in contact with an infinite planar substrate that is sufficiently weakly attractive to the fluid particles, such that the vapour phase prefers to be at the substrate. Surface phase transitions can be first-order or critical, and previous results (see [1, 2]) show that drying is generally a critical surface phase transition.\nPrevious work on ABPs has shown that they can exhibit a first-order wetting transition (see [3]). This effect has been investigated by looking at liquid-vapour ‘slabs’ and droplets, showing consistent results. However, the mechanism by which the wetting transition is traversed does not seem to allow for the possibility of a drying transition. To have a chance of observing this, we need to understand the mechanism by which ABPs interact with a surface.\nThe aggregation effect at a hard wall arises because ABPs become trapped at the wall and cannot diffuse away quickly enough. The project will attempt to induce a drying transition by coupling the ABP velocity vector to the wall-fluid interaction in such a way as to produce a torque that creates an effective wall-fluid repulsion, possibly enabling a drying transition. Our task will be to modify a standard MD simulation package to introduce this torque and then proceed similarly to the Turci and Wilding paper in PRL 2021 (see [4]), to try to observe a drying transition. If evidence for this is seen, then we will try to quantify the character of the transition and analyse its critical properties.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#general",
    "href": "index.html#general",
    "title": "Project Description",
    "section": "General",
    "text": "General\nWeekly update of my TSR (Teaching Support Roles)\nDocument search\nCourses to audit\nMPAGS Training Modules\nPhysics PGR SharePoint site: These pages should provide you with a ‘one-stop-shop’ for all of your PGR needs, including information about progress procedures, forms and examinations.\nMyERP",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#it",
    "href": "index.html#it",
    "title": "Project Description",
    "section": "IT",
    "text": "IT\nACRC HPC Documentation and User Guides\nBuying and borrowing IT\nIntroducing HOOMD-blue: HOOMD-blue is a Python package that performs Molecular Dynamics and hard particle Monte Carlo simulations. HOOMD-blue is general and can be used to model nanoparticles, bead-spring polymers, active matter, and many other types of systems.\nData Science and Research Software Engineering Training\nBDDS Python workshops\nQuarto Tutorial",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Purpose of this Website",
    "section": "",
    "text": "Purpose of this Website\nAs I ended week 2 on the 27th of September, I thought it would be useful to keep track of the following from next week (week 3):\n\nWhat I did every day (mostly bookmarking or a summary)\nA brief summary of what was discussed in project meetings\nThings I hope to discuss or ask in the next project meeting\nUseful resources as links, since I often don’t keep track of what I bookmarked in Safari\n\nAuthor: Sohyun Park\nSupervisors: Prof. Nigel Wilding, Dr. Francesco Turci",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "tutorial-document.html",
    "href": "tutorial-document.html",
    "title": "Quarto Document",
    "section": "",
    "text": "Quarto Document",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#colors",
    "href": "tutorial-document.html#colors",
    "title": "Quarto Document",
    "section": "1.1 Colors",
    "text": "1.1 Colors\n\nRed\nGreen\nBlue",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#shapes",
    "href": "tutorial-document.html#shapes",
    "title": "Quarto Document",
    "section": "1.2 Shapes",
    "text": "1.2 Shapes\n\nSquare\nCircle\nTriangle",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#textures",
    "href": "tutorial-document.html#textures",
    "title": "Quarto Document",
    "section": "1.3 Textures",
    "text": "1.3 Textures\n\nSmooth\nBumpy\nFuzzy",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#equations",
    "href": "tutorial-document.html#equations",
    "title": "Quarto Document",
    "section": "1.4 Equations",
    "text": "1.4 Equations\nEinstein’s theory of special relatively that expresses the equivalence of mass and energy:\nE = mc^{2}",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#sec-plot",
    "href": "tutorial-document.html#sec-plot",
    "title": "Quarto Document",
    "section": "3.1 Plot",
    "text": "3.1 Plot\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Simple Plot",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#sec-equation",
    "href": "tutorial-document.html#sec-equation",
    "title": "Quarto Document",
    "section": "3.2 Equation",
    "text": "3.2 Equation\n\ns = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\overline{x})^2}\n\\tag{1}",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-basics.html",
    "href": "tutorial-basics.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "Quarto Basics\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "tutorial-computations.html",
    "href": "tutorial-computations.html",
    "title": "Quarto Computations",
    "section": "",
    "text": "Quarto Computations",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Computations"
    ]
  },
  {
    "objectID": "tutorial-computations.html#numpy",
    "href": "tutorial-computations.html#numpy",
    "title": "Quarto Computations",
    "section": "NumPy",
    "text": "NumPy\n\n\nCode\nimport numpy as np\na = np.arange(15).reshape(3, 5)\na\n\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Computations"
    ]
  },
  {
    "objectID": "tutorial-computations.html#matplotlib",
    "href": "tutorial-computations.html#matplotlib",
    "title": "Quarto Computations",
    "section": "Matplotlib",
    "text": "Matplotlib\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\n# fig.set_size_inches(12, 7)\nx = np.arange(10)\ny = 2.5 * np.sin(x / 20 * np.pi)\nyerr = np.linspace(0.05, 0.2, 10)\n\nplt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')\nplt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')\nplt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,\n             label='uplims=True, lolims=True')\n\nupperlimits = [True, False] * 5\nlowerlimits = [False, True] * 5\nplt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,\n             label='subsets of uplims and lolims')\n\nplt.legend(loc='lower right')\nplt.show(fig)\n\n\n\n\n\n\n\n\nFigure 1: Errorbar limit selector",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Computations"
    ]
  },
  {
    "objectID": "tutorial-computations.html#plotly",
    "href": "tutorial-computations.html#plotly",
    "title": "Quarto Computations",
    "section": "Plotly",
    "text": "Plotly\n\nCode\n\"\"\"\nThis allows our figure display to span out beyond the normal body text column.\n#| column: page\n\"\"\"\n\nimport plotly.express as px\nimport plotly.io as pio\ngapminder = px.data.gapminder()\ndef gapminder_plot(year):\n    gapminderYear = gapminder.query(\"year == \" + \n                                    str(year))\n    fig = px.scatter(gapminderYear, \n                     x=\"gdpPercap\", y=\"lifeExp\",\n                     size=\"pop\", size_max=60,\n                     hover_name=\"country\")\n    fig.show()\n    \ngapminder_plot(1957)\ngapminder_plot(2007)\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) Gapminder: 1957\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) Gapminder: 2007\n\n\n\n\n\n\n\nFigure 2: Life Expectancy and GDP",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Computations"
    ]
  },
  {
    "objectID": "theme.html",
    "href": "theme.html",
    "title": "Theme Showdown",
    "section": "",
    "text": "From a pool of 25 themes, the chosen ones are:\n• default\n• flatly\n• minty\n• sandstone\n• united\n• yeti\nCue the dramatic music…\n\n\n\nRound 1: flatly vs default\nWinner: flatly!\nRound 2: minty vs sandstone\nResult: A tie!\nRound 3: united vs yeti\nWinner: united!\n\n\n\nMatch 1: flatly vs minty\nWinner: flatly (Minty was fresh, but flatly had the edge!)\nMatch 2: united vs sandstone\nWinner: united!\n\n\n\n\nflatly vs united\nWinner: flatly! More professional, less adorable. But honestly, united put up a cute fight.\nFor now, flatly reigns supreme… but united is right there, ready for a rematch.",
    "crumbs": [
      "Website Design",
      "Theme Showdown"
    ]
  },
  {
    "objectID": "theme.html#semi-finals",
    "href": "theme.html#semi-finals",
    "title": "Theme Showdown",
    "section": "",
    "text": "Match 1: flatly vs minty\nWinner: flatly (Minty was fresh, but flatly had the edge!)\nMatch 2: united vs sandstone\nWinner: united!",
    "crumbs": [
      "Website Design",
      "Theme Showdown"
    ]
  },
  {
    "objectID": "theme.html#final-round",
    "href": "theme.html#final-round",
    "title": "Theme Showdown",
    "section": "",
    "text": "flatly vs united\nWinner: flatly! More professional, less adorable. But honestly, united put up a cute fight.\nFor now, flatly reigns supreme… but united is right there, ready for a rematch.",
    "crumbs": [
      "Website Design",
      "Theme Showdown"
    ]
  },
  {
    "objectID": "notes.html#introduction",
    "href": "notes.html#introduction",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Introduction",
    "text": "Introduction\nIn this presentation, we explore the theoretical underpinnings of hydrophobicity by examining wetting and drying phase transitions. The phenomena are driven by surface tension and critical temperature scaling, providing insights into surface phase transitions across different substrates.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#youngs-equation",
    "href": "notes.html#youngs-equation",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Young’s Equation",
    "text": "Young’s Equation\nConsider a droplet of liquid on a surface. The contact angle, denoted as θ, quantifies the affinity between the liquid and the surface. This angle is determined by resolving the surface tensions of the interfaces involved. The relationship is given by Young’s equation:\n\\[\n\\cos \\theta = \\frac{\\gamma_{sv} - \\gamma_{sl}}{\\gamma_{lv}} \\tag{1}\n\\]\nWhere: - \\(\\gamma_{sv}\\) is the surface-vapour tension - \\(\\gamma_{sl}\\) is the surface-liquid tension - \\(\\gamma_{lv}\\) is the liquid-vapour tension\n\nHydrophobic vs Hydrophilic\n\nHydrophobic surfaces: \\(90^\\circ &lt; \\theta &lt; 180^\\circ\\)\nHydrophilic surfaces: \\(0^\\circ &lt; \\theta &lt; 90^\\circ\\)\n\nAt two extremes: - \\(\\theta = 0^\\circ\\): Full wetting. - \\(\\theta = 180^\\circ\\): Perfect drying.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#surface-phase-transitions",
    "href": "notes.html#surface-phase-transitions",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Surface Phase Transitions",
    "text": "Surface Phase Transitions\nThe approach to the two limiting cases represents surface phase transitions—wetting for \\(\\theta = 0^\\circ\\) and drying for \\(\\theta = 180^\\circ\\).\n\nCahn’s Scaling Law\nCahn’s theory explains the scaling behaviour of surface tensions as the critical temperature \\(T_c\\) is approached. The surface tension vanishes following the power law:\n\\[\n\\gamma_{lv} \\sim (-t)^\\mu \\quad \\text{with} \\quad \\mu = (d-1)\\nu\n\\]\nFor a fluid with \\(d = 3\\), the critical exponent is \\(\\nu \\approx 0.63\\), yielding \\(\\mu \\approx 1.26\\).\n\n\nContact Angle Near \\(T_c\\)\nAs \\(T_c\\) is approached, the contact angle vanishes:\n\\[\n\\cos \\theta \\sim t^{\\beta - \\mu} \\quad \\text{with} \\quad \\beta \\approx 0.33\n\\]\nThus, \\(\\cos \\theta\\) diverges as \\(t^{-0.93}\\), but is constrained to \\(|\\cos \\theta| \\leq 1\\).",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#wetting-and-drying-transitions",
    "href": "notes.html#wetting-and-drying-transitions",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Wetting and Drying Transitions",
    "text": "Wetting and Drying Transitions\nThe phase transition occurs at a specific temperature: - Wetting occurs at \\(T_W\\), where \\(\\cos \\theta = 1\\). - Drying occurs at \\(T_D\\), where \\(\\cos \\theta = -1\\).\nAt these temperatures, a macroscopic layer of liquid or vapour forms at the surface.\n\nCritical vs First-Order Transitions\nThe nature of these transitions can be: - Critical: The equilibrium film width \\(\\ell_{eq}\\) grows continuously as \\(T_D\\) is approached. - First-order: The film width jumps discontinuously.\nBelow are illustrations of critical and first-order transitions for drying:\n\n# Insert visual figure using code or links to diagrams",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#predrying-transitions",
    "href": "notes.html#predrying-transitions",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Predrying Transitions",
    "text": "Predrying Transitions\nIn systems undergoing first-order drying, predrying can occur at temperatures \\(T &gt; T_D\\). In this case, the vapour film transitions from a thin to a thick layer before the full drying transition at \\(T_D\\).\n\nScaling Behaviour\nThe equilibrium film width \\(\\ell_{eq}\\) exhibits different behaviours depending on the order of the transition:\n\\[\n\\ell_{eq} = \\frac{-\\Gamma}{A_{sv} \\Delta \\rho}\n\\]\nWhere: - \\(\\Gamma\\) is the adsorption - \\(A_{sv}\\) is the surface area of the substrate-vapour interface - \\(\\Delta \\rho = \\rho_l - \\rho_v\\) is the difference between the liquid and vapour densities",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#conclusion",
    "href": "notes.html#conclusion",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Conclusion",
    "text": "Conclusion\nWetting and drying phase transitions provide critical insights into the behaviour of hydrophobic and hydrophilic surfaces. These transitions are influenced by surface tension, temperature, and the nature of interactions between fluid and substrate. Theoretical models such as Cahn’s scaling law and simulation studies guide our understanding of these phenomena.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#references",
    "href": "notes.html#references",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "",
    "text": "A beautiful quote from Tom:\nFrom AM 3.4, classical mechanics is the geometry of phase space. All of the geometries (symplectic form, canonical transformations, etc.) describe the phenomenology of classical mechanics.\n2024/10/12 Making a presentation\nIntroduction\nIn this presentation, I’m going to explain the theoretical foundations of hydrophobicity by examining wetting and drying phase transitions. The phenomena are driven by surface tension and critical temperature scaling.\nYoung’s Equation\nConsider a droplet of liquid on a surface. The contact angle, denoted as θ, quantifies the affinity between the liquid and the surface. This angle is determined by resolving the surface tensions of the interfaces involved. The relationship is given by Young’s equation:\n\\[\n\\cos \\theta = \\frac{\\gamma_{sv} - \\gamma_{sl}}{\\gamma_{lv}} \\tag{1}\n\\]\nWhere: - \\(\\gamma_{sv}\\) is the surface-vapour tension - \\(\\gamma_{sl}\\) is the surface-liquid tension - \\(\\gamma_{lv}\\) is the liquid-vapour tension\nHydrophobic vs Hydrophilic\n\nHydrophobic surfaces: \\(90^\\circ &lt; \\theta &lt; 180^\\circ\\)\nHydrophilic surfaces: \\(0^\\circ &lt; \\theta &lt; 90^\\circ\\)\n\nAt two extremes:\n\n\\(\\theta = 0^\\circ\\): Full wetting.\n\\(\\theta = 180^\\circ\\): Perfect drying.\n\nSurface Phase Transitions\nThe approach to the two limiting cases represents surface phase transitions—wetting for \\(\\theta = 0^\\circ\\) and drying for \\(\\theta = 180^\\circ\\).\nCahn’s Scaling Law\nCahn’s theory explains the scaling behaviour of surface tensions as the critical temperature \\(T_c\\) is approached (see: (Cahn 1977)). The surface tension vanishes following the power law:\n\\[\n\\gamma_{lv} \\sim (-t)^\\mu \\quad \\text{with} \\quad \\mu = (d-1)\\nu\n\\]\nFor a fluid with \\(d = 3\\), the critical exponent is \\(\\nu \\approx 0.63\\), yielding \\(\\mu \\approx 1.26\\).\nContact Angle Near \\(T_c\\)\nAs \\(T_c\\) is approached, the contact angle vanishes:\n\\[\n\\cos \\theta \\sim t^{\\beta - \\mu} \\quad \\text{with} \\quad \\beta \\approx 0.33\n\\]\nThus, \\(\\cos \\theta\\) diverges as \\(t^{-0.93}\\), but is constrained to \\(|\\cos \\theta| \\leq 1\\).\nWhy \\(\\beta - \\mu\\)?\nNear the critical temperature, both \\(\\gamma_{sv}\\) and \\(\\gamma_{sl}\\) vanish. As \\(T\\) approaches \\(T_c\\), \\(\\gamma_{lv}\\) tends to zero as \\((-t)^\\mu\\). The numerator \\(\\gamma_{sv} - \\gamma_{sl}\\) scales with the order parameter difference \\(\\Delta \\rho = \\rho_l - \\rho_v\\), because the distinction between liquid and vapour phases diminishes. This gives:\n\\[\n\\gamma_{sv} - \\gamma_{sl} \\sim (-t)^\\beta\n\\]\nBy substituting the scaling behaviours of \\(\\gamma_{lv}\\) and \\(\\gamma_{sv} - \\gamma_{sl}\\) into Young’s equation, we get:\n\\[\n\\cos \\theta \\sim \\frac{(-t)^\\beta}{(-t)^\\mu} = t^{\\beta - \\mu}\n\\]\nWetting and Drying Transitions\nThe phase transition occurs at a specific temperature:\n\nWetting occurs at \\(T_W\\), where \\(\\cos \\theta = 1\\).\nDrying occurs at \\(T_D\\), where \\(\\cos \\theta = -1\\).\n\nAt these temperatures, a macroscopic layer of liquid or vapour forms at the surface.\nCritical vs First-Order Transitions\nThe nature of these transitions can be:\n\nCritical: The equilibrium film width \\(\\ell_{eq}\\) grows continuously as \\(T_D\\) is approached.\nFirst-order: The film width jumps discontinuously.\n\nBelow are illustrations of critical and first-order transitions for drying:\n\nPredrying Transitions\nIn systems undergoing first-order drying, predrying can occur at temperatures \\(T &gt; T_D\\). In this case, the vapour film transitions from a thin to a thick layer before the full drying transition at \\(T_D\\).\nScaling Behaviour\nThe equilibrium film width \\(\\ell_{eq}\\) exhibits different behaviours depending on the order of the transition:\n\\[\n\\ell_{eq} = \\frac{-\\Gamma}{A_{sv} \\Delta \\rho}\n\\]\nWhere:\n\n\\(\\Gamma = \\int \\mathrm{d}r \\, [\\rho(r) - \\rho_b]\\) is the adsorption, which describes the excess of fluid near the surface relative to the bulk.\n\\(A_{sv}\\) is the surface area of the substrate-vapour interface\n\\(\\Delta \\rho = \\rho_l - \\rho_v\\) is the difference between the liquid and vapour densities\n\nFor \\(\\rho(r)\\), we know from the statistical mechanics of fluids that the one-particle density \\(\\rho^{(1)}(r)\\) can be written in the form of an ensemble average.:\n\\[\n\\langle \\hat{\\rho}^{(1)}(r) \\rangle = \\frac{1}{\\Xi} \\sum_{j=1}^\\infty \\frac{e^{\\beta \\mu N}}{\\Lambda^{3N}(N-1)!} \\int \\mathrm{d}r^{N-1} e^{-\\beta[\\Phi(r^N) + V_{\\text{ext}}(r^N)]}\n\\]\nThis defines the fluid’s spatially varying density at a microscopic level.\n\n\n\n\n\n\n\n\nCahn, John W. 1977. “Critical Point Wetting.” The Journal of Chemical Physics 66 (8): 3667–72.\n\n\nCmglee. n.d. “Phase Diagram of Water Simplified.” https://commons.wikimedia.org/wiki/File:Phase_diagram_of_water_simplified.svg.",
    "crumbs": [
      "Project Notes",
      "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions"
    ]
  },
  {
    "objectID": "reading.html#colloquium-inclusions-boundaries-and-disorder-in-scalar-active-matter",
    "href": "reading.html#colloquium-inclusions-boundaries-and-disorder-in-scalar-active-matter",
    "title": "List of My Reading",
    "section": "2024/10/14 - Colloquium: Inclusions, boundaries, and disorder in scalar active matter",
    "text": "2024/10/14 - Colloquium: Inclusions, boundaries, and disorder in scalar active matter\nThree different models of active matter are discussed here: ABP (active Brownian particles), RTP (run-and-tumble particles), and AOUP (active Ornstein-Uhlenbeck particles). ABP is a model that I’m going to investigate initially because it’s the simplest particle model, but I want to be aware that there are other models. From page 3, what is interesting is this, since I saw somewhere that detailed balance doesn’t hold for non-equilibrium systems:\n\n\n\n\n\n\nNote\n\n\n\nOn the contrary, in a harmonic potential AOUPs always have a steady state given by a centered Gaussian distribution (Szamel, 2014) and, somewhat surprisingly, their dynamics obey detailed balance (Fodor et al., 2016).\n\n\nI wanted to revisit the detailed balance algorithm more deeply, so I started reading Understanding Molecular Simulation.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "notes.html#week-5",
    "href": "notes.html#week-5",
    "title": "Weekly Notes",
    "section": "Week 5",
    "text": "Week 5\n\n2024/10/17 Summary of project meetings so far\n\n\nPre-PhD discussions with Nigel\n\nSelf-propelled particles are a big topic at the moment. Many researchers are interested in active matter, which is different from passive matter, the subject we mostly learn about in our undergraduate studies of statistical mechanics. Statistical mechanics is a beautiful way of describing systems with a vast number of components. When we have an Avogadro number of components, we cannot expect to write down the equations of motion for each one and solve them because there are simply too many. Instead, we have to step back and ask, “What can we say about the averages and the fluctuations?” People like Boltzmann and Gibbs gave us a great theory for understanding equilibrium statistical mechanics. Now, we want to move beyond that to study non-equilibrium systems.\nPhysicists want to have simple models that capture the essential elements of the real systems they are studying. We start with basic models, like self-propelled particles. Francesco and Nigel have been studying these, and they are fascinating. For example, purely repulsive particles in an equilibrium system would not exhibit much behaviour. They do not have a liquid-vapour transition because attraction is required for condensation from vapour into liquid. Purely repulsive particles can crystallise, but they do not exhibit a liquid-vapour transition.\nHowever, if we take those same purely repulsive particles and make them self-propelling, a liquid-vapour transition suddenly emerges. This is an example of how interactions between many simply interacting bodies can lead to large-scale emergent properties. These active Brownian particles, which are self-propelled and repulsive, show this phase transition into a liquid-vapour phase. This is not just about bulk phase transitions but also about what happens at surfaces. How do they interact with a surface? Can phase transitions occur at a surface, such as wetting or drying transitions?\nThese are very new areas of research. Francesco and Nigel have been exploring some of these questions, and their recent papers reflect that. They realise there are many interesting directions this research could take. One of the main tools here, since we do not have advanced theories like those for equilibrium systems, is computer simulation. We are fortunate to have great high-performance computing resources in Bristol. Some of these problems need high performance, as they require large systems with many particles.\nAnother area Nigel is interested in is hydrophobic interactions. We have particles that are either entirely hydrophobic or amphiphilic, meaning they have both hydrophobic and hydrophilic parts. These particles can self-assemble into large-scale structures, but there are still aspects of that process that are not well understood. For instance, it is known experimentally that heating amphiphiles promotes self-assembly. Normally, heating increases disorder, so why is it different here? What is happening with hydrophobic or amphiphilic interactions that leads to this interesting temperature dependence? This is not well understood at the moment, and it is an open question that he hopes to address soon using computer simulation.\nHe thinks there may be other factors at play here, relating to surface phase transitions. They have made some progress in understanding how water behaves at a planar surface, like a hydrophobic planar surface. That behaviour is essentially the physics of a drying transition. All these things are connected. Applying our understanding of what happens at a planar surface to hydrophobic entities like solute particles, molecules, amphiphiles, or proteins brings basic physics closer to biological systems and non-equilibrium systems. This is an exciting area because it links traditional equilibrium phase transition concepts to how nature exploits them for function.\nProfessor Paul Bartlett in the chemistry department also works in this area, conducting colloidal experiments. Colloids are used in a range of industrial products, such as paints, shampoos, personal care products, and foodstuffs. They are important in the industry, but making new products with new functionality is challenging because these products are often mixtures of many different components. The space of possible combinations is enormous, making it difficult to determine the right mix to achieve a desired property.\nThe question is whether simulation, something like digital twins, can help us find useful mixtures more efficiently and avoid trial-and-error. Much of what happens now is trial and error. If we could create a digital twin or simulation based on real data, it could guide us. AI and neural networks could be used to incorporate real data into simulations, teaching the system the interactions between the components to reproduce real behaviour. Then, we could explore new directions, new types of mixtures, or new areas of phase space more quickly and at lower cost.\nWe are still some way from that goal, but he believes we are ready to take the first steps in that direction. Simulations with an AI or neural network overlay, which learn how to reproduce experimental data, would be the first step.\nRegarding the specific project, how do active Brownian particles interact with a surface? What if that surface is flexible, like a lipid membrane, or penetrable so that the particles can pass through it like a thin membrane? We want to study these systems through simulation and use our knowledge of statistical physics to understand them.\nIt is not biology, but it is not far removed. There are many exciting aspects to explore, but the focus will be on a simulation approach. One attractive feature of statistical physics is the abundance of problems to choose from, allowing researchers to make real progress, often with just one or two people. In particle physics or astrophysics, researchers are often part of large collaborations and do not have the freedom to determine the direction of the research. Success in such fields is shared among many, whereas in our field, a small number of people can have a significant impact, giving more freedom and satisfaction. Some of the work may be patented, but most of it is published in the open literature, with your name at the top, not among a thousand others. You really feel that your contribution is valued.\nFor active Brownian particles, we use a method called Brownian Dynamics. It is similar to Molecular Dynamics, but for passive particles in equilibrium, Molecular Dynamics is quite straightforward. We calculate all the forces; every particle interacts via some potential, and we determine the force on each particle due to the others it interacts with. Then, we integrate the equations of motion, Newton’s equations, through time.\nEssentially, we evolve the particle positions through time in a stepwise manner. We increase time from T to T+ΔT and determine where all the particles will be, then iterate and repeat this process. The particles move around the system according to Newton’s equations of motion, making it physically realistic.\nFor active Brownian particles, in addition to the passive interactions between the particles, each particle experiences a driving force along a specific direction, which we represent as an arrow. In the case of Janus particles, this driving force may be due to a chemical reaction, such as with peroxide, which creates movement along a particular vector. Because of interactions with other particles, this vector diffuses over time, and we treat this stochastically using random numbers. Like in Molecular Dynamics, we advance the system in time steps, but we also add a random displacement to the driving force. At each time step, the arrow may point in a slightly different direction, and the force acts along that direction. The dynamics of the system result from both the passive interaction forces between the particles and the driving force, which can diffuse over time. This is the algorithm we use, known as Active Brownian Dynamics.\nThe term “Brownian” refers to the diffusion of the force vector for each particle. The programs for these simulations are quite simple. We do not always need to write our own from scratch, as some major Molecular Dynamics packages can handle this for us. In some cases, we can use pre-existing, optimised community codes that are efficient, fast, and parallelised. However, we often do specialised work that requires writing our own code, so there is plenty of opportunity to develop custom code as well.\nActive Brownian Dynamics is distinct from Monte Carlo methods, which are mainly used for equilibrium systems. Unlike Molecular Dynamics, which evolves the system according to Newton’s equations of motion, Monte Carlo does not produce realistic dynamics. In Monte Carlo simulations, the transition from one configuration of particles to the next is not physically realistic, but the generated configurations are correctly distributed according to their Boltzmann weights in terms of free energy. Monte Carlo is used to generate equilibrium-distributed configurations from which we can measure various properties, but it does not provide dynamical information.\nMonte Carlo methods are very efficient at generating different, uncorrelated configurations because they allow for large jumps between configurations, exploiting fictitious dynamics. In contrast, Molecular Dynamics or Active Brownian Dynamics involve gradual changes, making it more like a shuffle that takes longer to reach a completely different configuration. Monte Carlo methods enable much larger leaps, making them more efficient at collecting uncorrelated statistics.\nMonte Carlo is better at exploring phase space. Nigel has spent much of his career developing algorithms to explore new areas in configuration space. For example, to study the phase transition from a liquid state to a crystal state, where particles arrange on a lattice, it is difficult to determine the parameters at which the transition occurs using Molecular Dynamics because of effects like hysteresis. One of the innovations he developed for Monte Carlo was a method that, instead of gradually nucleating a crystal and letting it grow, jumps completely from a fluid configuration to a crystal configuration in a single move. This approach, called “Phase Switch Monte Carlo”, allows the system to explore each state and then jump back, enabling the calculation of free energy differences and the precise determination of coexistence points.\nThis is the sort of technique that makes Monte Carlo powerful, something Molecular Dynamics cannot easily do. Different phenomena require different techniques. If we are interested in dynamics, we need one technique; if we are interested in phase transitions between very different structures, we need another. The lack of symmetry is what makes these problems challenging. By exploiting fictitious dynamics in Monte Carlo, we can map a liquid configuration onto a solid configuration, even though this cannot physically occur in reality. In practice, a small region would nucleate and grow into a crystal, eventually filling the entire simulation box. If we are only interested in the coexistence conditions, then Monte Carlo is far more powerful.\nI have enjoyed Tom’s phase transitions course, which introduced many important ideas. It covered models of phase transitions, Landau theory, and aspects of the renormalisation group. It also discussed the different ensembles, the relationship between the system and its surroundings, and the various free energies relevant to each case.\n\nSupervisor: The other thing that may drive you—okay, this is different. This is morphogenesis. This concept dates back to the 19th century. It’s related to mathematical physics. The question is, how does shape emerge? At that time, it typically boiled down to geometries. But then, it’s not always just geometry. What I want to say is, if you are interested in biological motivation or the connection between physics and biology, that’s another possibility—taking the angle of active matter as a model for biological systems.\nWe’re not direct experts in this, but we have connections that we could explore if you’re interested in linking the theoretical approach to some kind of experimental test. That’s always possible. The last angle is linking artificial systems, like robots. In Bristol, there’s a big robotics centre—actually, the largest nationwide—the Bristol Robotics Laboratory. It’s a collaboration between us and the University of the West of England (UWE), but it’s not here on this campus; it’s at the UWE campus.\nThere are researchers here who do this as well. The most famous, at least relevant to my interests, is Sabine Hauert. You will see she has a series of TED Talks. If you watch them, you’ll get a sense of what she does. She’s interested in swarm robotics. There’s a very small step between what she does and what we’re discussing. I don’t know if all her talks are available online—there should be plenty of material. Watching them might give you a sense of the kinds of things they do.\nSwarm robotics, in many ways, aligns with what we’re doing in active matter. It’s just at a different scale. They study behaviours from the very small—the microscopic level—all the way to larger scales. For example, they have projects involving drug delivery. You can say that this would be a kind of morphogenesis. These are actual videos of “kilobots,” which are two-dimensional systems made up of many small robots. They have limited sensing but can communicate.\nIn the systems we study, there’s no direct communication like that; there’s not even inherent activity in the same way. However, with swarms, they study how collective behaviour and communication can lead to emergent properties. Even in engineering, they’re already thinking in terms that we consider in physics. Our role is to support them with a somewhat broader picture.\nWhat we do now is provide model systems, classes of problems, and minimal models. That’s what we can contribute from the physics side. We try to reduce the complexity and say, “If you perturb these few ingredients, you have this space of parameters that opens into different phases—maybe phase transitions or some kind of controlled behaviour.” That’s what we can do as physicists.\nTypically, engineers aren’t interested in that level of abstraction. They say, “I have all these parameters; I perturb them, and this is what happens.” We try to provide a coherent picture. However, there is the possibility of collaborating. We can take some generic problems that have the potential to interface with biology, engineering, and other aspects of physics.\nIt also depends on whether you want to go for a more theoretical or more applied angle. We can start with the theoretical basics to get the foundation covered, and then we can move in different directions later. It’s really a matter of seeing what your driving interest is. If you’re not sure, we’ll provide you with an initial exploratory project to work on. This will give you the opportunity to identify what aspect you find the most interesting and would like to pursue for the next couple of years.\nIf you wanted to, there would be the opportunity to engage with this. For instance, there’s no issue in asking Sabine, “We have this good PhD student who is interested in testing her results in the field.” I managed to set up the kilobots, saw them moving around. It wasn’t very exciting, but it was doable. If I can do it, you can do it too.\nI’ve already chatted with Sabine because I’ll have a PhD student myself next year or the following year, beyond you. The idea is to set up multiple projects in that direction. She’s interested, so if swarms are a topic that interests you, we can have an interdisciplinary project in that direction. That wouldn’t be hard to arrange.\nSupervisor: So, entropy is not just disorder, okay? This notion of entropy as disorder is a popular message that simplifies a lot of the storytelling around statistical mechanics. The classical example that we often give in this department is that if you take only hard spheres—a purely repulsive system—there is no energy, right, apart from infinite energy when they overlap. So, the energy is always zero because all the acceptable configurations are at zero energy. Then, the free energy is only entropic, just entropy.\nYou might think, “Okay, with only entropy, you will just have disorder.” That would be the natural assumption. In this simple system, there’s no kind of interaction, just hard spheres. However, what you actually observe is that if you increase the density of such systems, you spontaneously observe a transition from a fluid to a crystal.\nYou: Really?\nSupervisor: Yes, you spontaneously observe this transition. The crystal that forms is essentially FCC-like (face-centred cubic). So somehow, even though this system is maximising entropy—because there is only entropy involved—it results in an ordered state at these very high densities. This occurs because the crystal packing is better, so every single sphere has a little bit of extra wiggle room, more local configurations. The effective volume accessible to each sphere is larger, even though they are in an ordered configuration.\nSo, you can obtain crystalline order by maximising entropy. It’s quite counterintuitive. You would think that maximising entropy means just getting disorder and taking the system very far away from any kind of ordered pattern. But in fact, no. If you take this simplest system, it goes into the most obvious ordered state—a crystal—by maximising entropy.\nThis idea that entropy equals disorder is just a story we tell that simplifies things. Entropy can take you in many different directions; it’s not just disorder. It means the choice of possible states is broader than what you may think initially. Entropic interactions do not necessarily lead to disorder; on the contrary, they can lead to packing, to crystallisation, and so on.\nYou: Interesting. How does this relate to active systems?\nSupervisor: In the case of active systems, this construction becomes even further apart. Energy is there, but it is not the only relevant force. You have these intrinsic forces in such systems, so they cannot be written directly in the same form. The notion of accessible states, the statistical nature of entropy, can still come back if you have at least steady states. The free energy becomes more about the log of the probability distribution, as you’ve seen in your project.\nYour project was all equilibrium, but not really. The financial data was some kind of steady state; you assumed some steady state. All this is to say that, for example, self-assembly in biological systems—biological molecules or macromolecules—yes, it is driven by free energies, at least in our picture, because you have some solvent, and they tend to minimise the free energy. But the energy landscape is very complicated due to this competition between many different states that entropy actually can favour.\nThese states are not necessarily disordered; they are just different from the ones that energy alone would pull toward. The main picture here is that not all of the rest is disorder. It’s essentially about simple basins versus complex basins in free energy. That’s the problem, for example, in protein folding. The free energy landscape is very structured, not just because of the energy, but also because of all the possibilities that entropy adds to it. It’s not just about misfolding—all the different configurations represent different structures.\nYou: I’m curious about how life emerges. That’s the question I want to answer.\nSupervisor: If you want to narrow it down to something that’s researchable, maybe you want to clarify—emerges from what, under what conditions? Maybe imagine a particular setup. This kind of problem, for example, has been tackled here in chemistry by taking the approach of constructing proto-cellular structures from purely synthetic components. It’s sort of a bottom-up approach, asking, “If I add this, this, and this ingredient, how far can I go?”\nThis is the group of Professor Stephen Mann. They are quite successful and have managed to produce very plausible membrane-like structures that assemble similarly to how natural membranes do. From our point of view in physics, it means many different things. The active matter angle is more about how, for example, morphogenesis occurs—not how life itself emerges, but how patterns in biological structures emerge. This is a related question but leans more towards mathematical and physical understanding.\nFor instance, you could try to cast it into a set of partial differential equations. I was trying to reproduce a paper on this topic recently. It’s fascinating because this isn’t active matter at all; it’s simply a set of differential equations. The study focuses on diatoms—not diatomic molecules, but little organisms that produce intricate shells. The paper attempts to create a minimal model that can manifest similar patterns.\nWhat they do is take the obvious route: they write down a partial differential equation, essentially a reaction-diffusion model. Turing demonstrated that with suitable parameters, these models break into bifurcation-like phenomena, leading to pattern formation. They explore their model by selecting certain initial conditions and letting the system evolve, showing how various patterns emerge.\nYou: Is this computationally intensive?\nSupervisor: Not necessarily. It’s just about evolving a partial differential equation on paper. It’s fairly simple—around four equations in total. I should be able to reproduce it; I’m just fighting with the package I’m trying to use. This is an example of a minimal model. In the spirit of this, you’re assuming some nonlinearity in the relationships between suitable fields. This is doable in physics.\nThere is always an element of relaxation towards equilibrium in these models. Active matter, typically, is more concerned with steady states far from equilibrium, where you have strong intrinsic forces driving the system, pushing it away from equilibrium. However, you may still observe phases that resemble equilibrium phases, like a crystalline or liquid phase with characteristic density fluctuations.\nYou can’t always address where the driving force is coming from. In these models, even in the partial differential equations, we often put some ingredients in by hand. The central questions, like “How does life emerge?” are not fully addressed in physics; they might belong more to chemistry because they involve molecular-level processes. Life involves some kind of chemical cycle that transforms into a stable mechanism.\nYou: So, the emergence of life is more of a chemical question?\nSupervisor: Yes, at its core, it’s more of a chemical question. It’s about looking at chemical networks and how they transform into stable mechanisms. We’re not really experts in chemical networks, but there’s a lot of work on understanding non-equilibrium chemical networks and their structures. You’re asking an enormous question—how does life emerge? One way to approach this is by narrowing it down to a more specific problem that can be explored from a particular perspective.\nYou need to accept a set of assumptions and focus on a particular aspect, whether it’s morphogenesis, collective behaviour in bio-inspired systems, or understanding how chemical networks are affected by non-equilibrium forces, or what is the best mathematical description that allows you to categorise different chemical networks. This is a very broad question, and it’s huge in scope. We need to be inspired by that motivation but narrow it down to something achievable in the span of four years. It sounds like a lot of time, but it’s not that much.\nI think we’ve preempted a conversation that we’ll probably have with you in the next few months. This is just food for thought for now. If you need input, you can drop by. Let’s say you find an article and want to know more about its background or how it links to what you’ve done or plan to do—we can help with that.\nYou: Can I ask you one question? All these physics fields—or I would probably call them games because they are like tools to play with to a large extent—what are the things that you’re most curious about?\nSupervisor: Personally, I’m interested in going towards the biophysics angle. I’ve been working on abstract models of liquids for a long time. For example, I’ve simulated liquids in four dimensions on a curved three-dimensional surface. No one can actually sample those configurations; they don’t exist in reality. So, I would like to connect back to more concrete problems inspired by biophysics.\nMorphogenesis is one of the directions I would like to explore—using our understanding of active systems to make sense of the emergence of patterns in biological systems. But you can’t just jump into this field directly; you need to take a particular angle. In my case, it would be coupled to things like network formation or slow dynamics—problems where I already have some expertise.\nAt the same time, we are currently working on a basic but significant question: what drives the hydrophobic effect? When I give talks on biophysics, I can’t properly explain the hydrophobic effect—the one I mentioned earlier. If you talk to different people in this department or in chemistry, each person will tell you a different story about what they think causes the hydrophobic effect.\nSome will say, and you’ll find this on Wikipedia, that around a hydrophobic solute, water forms an ice-like structure, creating some sort of clathrate shape. It’s a story people tell themselves, but there’s no real evidence for it. The idea comes from an interpretation of experimental results. The experiments do show that there is some enthalpic and possibly entropic contribution, but it’s not clear that there is a structural change like that occurring. It’s more like a heuristic story we tell ourselves.\nYou: That makes sense. So, it’s more of a model or hypothesis?\nSupervisor: Yes, exactly. For example, if you look at surface phase transitions in liquids, these are actual phase transitions. You have a well-defined surface in contact with the liquid, and everything is more straightforward. The wall is infinite, so integrals converge, and you get proper correlation lengths. However, these transitions are challenging to sample numerically because you need to simulate both the bulk and the surface, ensuring that the surface is large enough to capture all the fluctuations.\nNow, when you introduce a solute in a fluid, everything is rounded by definition—there’s no thermodynamic limit in the same way. The solute has a finite surface, and the interactions are what they are. This solute might be roughly the same size as the fluid molecules, so there has to be a crossover regime where you can’t really coarse-grain. The question is, how much of this physics of surface phase transitions still controls those effects?\nHere, I’m not even talking about activity. We have looked at wetting and drying in active systems, but for hydrophobicity and related phenomena, the question is, can we relate the strength of the effect? Let’s say we take a protein. In this picture, a protein is just a blob, and it’s more complicated than that, but you could say it’s a blob with areas that are hydrophobic and others that are hydrophilic. What does that truly mean?\nStatistically, you see that it tends to stay in contact with other proteins like itself and away from water. That’s actually all you know—it tends to come together and not expose itself to water. For example, if you put it on a membrane, it sticks in one way and not the other. There are many other ideas linked to being hydrophobic, but it’s not a mechanistic explanation. We are just saying, “That’s what we observe.” It’s more of an effective, statistical statement.\nYou: So, it’s more about the properties of the solvent around the molecule?\nSupervisor: Yes, exactly. We want to measure the properties of the solvent around it. In this case, the solvent is water, but it could be something else. Quantities that seem to be interesting include some kind of measure of local compressibility. What does it mean to be “local”? What is the compressibility when you only have a few molecules? Can you still define it?\nIt seems like we can define something. Correlating the existing metrics of hydrophobicity with whatever we have as a metric for the behaviour of the fluid around a molecule is something we’re exploring at the moment. Before jumping straight into proteins, we look at simpler cases that we can control.\nFor example, we have a solute, a rigid molecule shaped like a lock—a sphere with a concavity. The curvature of this object changes, which is relevant because proteins have complex shapes with pockets and holes. The solvent here is water, and we observe effects that are not captured by true dynamics. We use Molecular Dynamics with some bias. If you let it go without bias, the solute just sticks there and never gets out. Even though the particles are all repulsive apart from the water, the solvent pushes the solute into the cavity, creating a large effective potential that keeps it there.\nThat’s the hydrophobic force—an effective potential of mean force, which can be seen as the logarithm of a probability. So, we’re trying to calculate the logarithm of this probability.\nYou: That makes a lot of sense. This is very enlightening.\nSupervisor: Yes, it’s a fascinating topic. There’s a lot more to explore, especially in how we understand the interactions between solutes and solvents. In the end, it’s about connecting abstract models back to real, observable phenomena and seeing how far we can push those models to explain complex behaviour in biological systems.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#hydrophobic-vs-hydrophilic",
    "href": "notes.html#hydrophobic-vs-hydrophilic",
    "title": "Weekly Notes",
    "section": "Hydrophobic vs Hydrophilic",
    "text": "Hydrophobic vs Hydrophilic\n\nHydrophobic surfaces: \\(90^\\circ &lt; \\theta &lt; 180^\\circ\\)\nHydrophilic surfaces: \\(0^\\circ &lt; \\theta &lt; 90^\\circ\\)\n\nAt two extremes:\n\n\\(\\theta = 0^\\circ\\): Full wetting.\n\\(\\theta = 180^\\circ\\): Perfect drying.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#cahns-scaling-law",
    "href": "notes.html#cahns-scaling-law",
    "title": "Weekly Notes",
    "section": "Cahn’s Scaling Law",
    "text": "Cahn’s Scaling Law\nCahn’s theory explains the scaling behaviour of surface tensions as the critical temperature \\(T_c\\) is approached (see: (Cahn 1977)). The surface tension vanishes following the power law:\n\nCahn, John W. 1977. “Critical Point Wetting.” The Journal of Chemical Physics 66 (8): 3667–72.\n\\[\n\\gamma_{lv} \\sim (-t)^\\mu \\quad \\text{with} \\quad \\mu = (d-1)\\nu\n\\]\nFor a fluid with \\(d = 3\\), the critical exponent is \\(\\nu \\approx 0.63\\), yielding \\(\\mu \\approx 1.26\\).",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#contact-angle-near-t_c",
    "href": "notes.html#contact-angle-near-t_c",
    "title": "Weekly Notes",
    "section": "Contact Angle Near \\(T_c\\)",
    "text": "Contact Angle Near \\(T_c\\)\nAs \\(T_c\\) is approached, the contact angle vanishes:\n\\[\n\\cos \\theta \\sim t^{\\beta - \\mu} \\quad \\text{with} \\quad \\beta \\approx 0.33\n\\]\nThus, \\(\\cos \\theta\\) diverges as \\(t^{-0.93}\\), but is constrained to \\(|\\cos \\theta| \\leq 1\\).\n\nWhy \\(\\beta - \\mu\\)?\nNear the critical temperature, both \\(\\gamma_{sv}\\) and \\(\\gamma_{sl}\\) vanish. As \\(T\\) approaches \\(T_c\\), \\(\\gamma_{lv}\\) tends to zero as \\((-t)^\\mu\\). The numerator \\(\\gamma_{sv} - \\gamma_{sl}\\) scales with the order parameter difference \\(\\Delta \\rho = \\rho_l - \\rho_v\\), because the distinction between liquid and vapour phases diminishes. This gives:\n\\[\n\\gamma_{sv} - \\gamma_{sl} \\sim (-t)^\\beta\n\\]\nBy substituting the scaling behaviours of \\(\\gamma_{lv}\\) and \\(\\gamma_{sv} - \\gamma_{sl}\\) into Young’s equation, we get:\n\\[\n\\cos \\theta \\sim \\frac{(-t)^\\beta}{(-t)^\\mu} = t^{\\beta - \\mu}\n\\]",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#critical-vs-first-order-transitions",
    "href": "notes.html#critical-vs-first-order-transitions",
    "title": "Weekly Notes",
    "section": "Critical vs First-Order Transitions",
    "text": "Critical vs First-Order Transitions\nThe nature of these transitions can be:\n\nCritical: The equilibrium film width \\(\\ell_{eq}\\) grows continuously as \\(T_D\\) is approached.\nFirst-order: The film width jumps discontinuously.\n\nBelow are illustrations of critical and first-order transitions for drying:",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#scaling-behaviour",
    "href": "notes.html#scaling-behaviour",
    "title": "Weekly Notes",
    "section": "Scaling Behaviour",
    "text": "Scaling Behaviour\nThe equilibrium film width \\(\\ell_{eq}\\) exhibits different behaviours depending on the order of the transition:\n\\[\n\\ell_{eq} = \\frac{-\\Gamma}{A_{sv} \\Delta \\rho}\n\\]\nWhere:\n\n\\(\\Gamma = \\int \\mathrm{d}r \\, [\\rho(r) - \\rho_b]\\) is the adsorption, which describes the excess of fluid near the surface relative to the bulk.\n\\(A_{sv}\\) is the surface area of the substrate-vapour interface\n\\(\\Delta \\rho = \\rho_l - \\rho_v\\) is the difference between the liquid and vapour densities\n\nFor \\(\\rho(r)\\), we know from the statistical mechanics of fluids that the one-particle density \\(\\rho^{(1)}(r)\\) can be written in the form of an ensemble average.:\n\\[\n\\langle \\hat{\\rho}^{(1)}(r) \\rangle = \\frac{1}{\\Xi} \\sum_{j=1}^\\infty \\frac{e^{\\beta \\mu N}}{\\Lambda^{3N}(N-1)!} \\int \\mathrm{d}r^{N-1} e^{-\\beta[\\Phi(r^N) + V_{\\text{ext}}(r^N)]}\n\\]\nThis defines the fluid’s spatially varying density at a microscopic level.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "reading.html#what-is-a-complex-system-completed",
    "href": "reading.html#what-is-a-complex-system-completed",
    "title": "List of My Reading",
    "section": "",
    "text": "By James Ladyman and Karoline Wiesner\nMy final-year MSci project involved the study of a complex system, foreign exchange markets. Each individual currency was treated as a spin in the Ising model, and by applying E.T. Jaynes’s principle of maximum entropy, we discovered the structure of interactions between these entities. Remarkably, this simple model of magnetism can also describe the phase transitions of liquid-gas systems, as they fall into the same universality class. This model has even been extended to study how critical our brain is, giving rise to the field of the critical brain hypothesis. Therefore, many ideas during my research project were derived from neuroscience papers. The financial market and the brain: both are complex systems.\nAbout a year ago, I read a paper published by James Ladyman, a professor of philosophy of science at Bristol, titled ‘What is a Complex System?’ (Ladyman, Lambert, and Wiesner 2013). As I started my PhD at Bristol, my second supervisor, Francesco, mentioned that he also published a book about complexity. It became clear to me that I should read this book.\n\nLadyman, James, James Lambert, and Karoline Wiesner. 2013. “What Is a Complex System?” European Journal for Philosophy of Science 3: 33–67.\nChapter 1 presents The Truisms of Complexity Science as follows:\n1. More is different.\n2. Nonliving systems can generate order.\n3. Complexity can come from simplicity.\n4. Coordinated behaviour does not require an overall controller.\n5. Complex systems are often modelled as networks or information processing systems.\n6. There are various kinds of invariance and forms of universal behaviour in complex systems.\n7. Complexity science is computational and probabilistic.\n8. Complexity science involves multiple disciplines.\n9. There is a difference between the order that complex systems produce and the order of the complex systems themselves.\nFeatures that are necessary and sufficient for which kinds of complexity and complex system are as follows:\n1. Numerosity: complex systems involve many interactions among many components.\n2. Disorder and diversity: the interactions in a complex system are not coordinated or controlled centrally, and the components may differ.\n3. Feedback: the interactions in complex systems are iterated so that there is feedback from previous interactions on a time scale relevant to the system’s emergent dynamics.\n4. Non-equilibrium: complex systems are open to the environment and are often driven by something external.\n5. Spontaneous order and self-organisation: complex systems exhibit structure and order that arises out of the interactions among their parts.\n6. Nonlinearity: complex systems exhibit nonlinear dependence on parameters or external drivers.\n7. Robustness: the structure and function of complex systems is stable under relevant perturbations.\n8. Nested structure and modularity: there may be multiple scales of structure, clustering and specialisation of function in complex systems.\n9. History and memory: complex systems often require a very long history to exist and often store information about history.\n10. Adaptive behaviour: complex systems are often able to modify their behaviour depending on the state of the environment and the predictions they make about it.\n\n2024/9/27\nReading What is a Complex System? by James Ladyman and Karoline Wiesner. The purpose of reading is to understand how systems out of thermodynamic equilibrium (a feature of complex systems) relate to a wide range of concepts associated with complexity.\n• There was a video that helped clarify what is meant by a Markov chain and a stochastic process being stationary: Markov Chains Clearly Explained! Part - 1. His other videos are helpful as well. For example, when I saw \\(P_{ij}^{(n)} = A_{ij}^n\\) from the n-Step Transition Matrix video (Part 3), it was quite surprising. However, the Part 5 video on hidden Markov models wasn’t very helpful, but reading the appendix in the book was sufficient.\n• What I’ve felt from reading this book (currently at p.90) is that it presents a lot of different ideas and has interesting discussions on the history of science, but I only seem to get excited when it starts mentioning the brain. I’m not sure if I’m necessarily interested in quantifying complexity. From the Apple notes on 28/9/2024:\nAs I’m reading through What is a Complex System?, it seems that the description of a complex system arises as we have more representations at different scales of analysis. The concept of ‘complexity’ was destined to arise as the scientific era evolved.\nNon-equilibrium systems arise because we define what systems are in thermodynamic equilibrium, which are idealisations.\nThings that fail to be idealised fall into this dualistic concept: non-equilibrium.\n\n2024/10/3\nIt took about two weeks to finish this book. It was an accessible read until I reached chapter 4, which was Measuring Features of Complex Systems. In the end, I’m glad that I read it, even though I somehow felt rushed to finish as there were other responsibilities to manage continuously, such as setting up clusters for my university Mac, teaching, office setup, etc. Most importantly, from now on, I am going to focus on reading material that is directly related to my project since it may give the impression to my supervisors that I’m not working hard. Reading a book like this takes a long time, as there are many new terms I had not encountered before.\nAnyway, it was a good read. Would I buy a physical copy for myself? Maybe not, but it was a good one.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#on-the-motion-of-small-particles-suspended-in-liquids-at-rest-required-by-the-molecular-kinetic-theory-of-heat-completed",
    "href": "reading.html#on-the-motion-of-small-particles-suspended-in-liquids-at-rest-required-by-the-molecular-kinetic-theory-of-heat-completed",
    "title": "List of My Reading",
    "section": "2024/10/18 - 10/19 On the Motion of Small Particles Suspended in Liquids at Rest Required by the Molecular-Kinetic Theory of Heat (Completed)",
    "text": "2024/10/18 - 10/19 On the Motion of Small Particles Suspended in Liquids at Rest Required by the Molecular-Kinetic Theory of Heat (Completed)\nBy Albert Einstein\nSince I am studying a model called active Brownian particles, a thought occurred to me: I should actually read where this understanding of Brownian motion came from! This was done by the very man: Albert Einstein in 1905 (Einstein et al. 1905). I normally leave my iPad in my office so whenever I needed derivations, I used a pen and note, quite a classical way while reading a classical paper:\n\nEinstein, Albert et al. 1905. “On the Motion of Small Particles Suspended in Liquids at Rest Required by the Molecular-Kinetic Theory of Heat.” Annalen Der Physik 17 (549-560): 208.\nMy handwritten notes\nI’m so glad that I read this. It reminded me of how much I loved studying thermodynamics during my undergraduate studies. It was kind of like studying geometry (or topology). I consider a system subject to constraints and investigate how the particle behaves given the state variables. The paper wasn’t straightforward to read, as I had to remind myself of several concepts such as the continuous form of entropy, perturbation theory, and Fourier transforms to find a solution for diffusion equations.\nI really liked the way he displayed his logic. He considers a system: a pure solvent and suspended particles. These particles are confined by a semi-permeable wall, which only allows solvent to flow, not the solute. Then he thinks about the existing concept of osmotic pressure, which inhibits the solvent from moving to equalise concentrations. He then makes several reasonable assumptions, such as:\n\nSuspended particles have slow and irregular motions due to the solvent’s molecules.\nThe motions of those individual particles are independent of each other.\n\nHe then links the diffusion coefficient to two different contexts. The first is through the theory of diffusion, involving concepts of entropy and free energy. The second is through the theory of thermal molecular motion, where he considers the rate of change of density. By uniting these two theories, he arrives at the root-mean-square displacement of the suspended particle. Wow!\nI’m so happy to have chosen a theoretical physics PhD. Thinking about how theories interconnect reveals a universal understanding. I’m looking forward to reading more of his articles.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "notes.html#week-6",
    "href": "notes.html#week-6",
    "title": "Weekly Notes",
    "section": "Week 6",
    "text": "Week 6\n\n2024/10/23 Molsim-2025 School Application\nMolSim-2025 is 2-week school that offers a program focusing on basic and advanced molecular simulation methods. It targets PhD candidates and postdoctoral researchers who want to understand and apply these methods to study topics in physics, chemistry or biology. The program has an integrated and balanced structure of lectures and hands-on exercises. It offers a recap of the basic theory relevant for molecular simulation, followed by a thorough covering of basic and advanced simulation techniques. All techniques will be accompanied by illustrative examples of present day research.\n\nMy application (submitted on 241023)\nI am a PhD student in theoretical and computational physics at the University of Bristol, specialising in the simulation of surface phase transitions with a focus on active matter using high-performance computing. My supervisors are Nigel Wilding, who has strongly recommended this winter school and has done substantial work on developing efficient algorithms for MC simulations [1-4], and Francesco Turci, who specialises in disordered systems. My initial project involves investigating drying (surface) phase transitions using Monte Carlo and molecular dynamics techniques, introducing an additional parameter to how they approached the problem in PRL 2021 [5].\nI am highly motivated to join this winter school since it is my first year of PhD studies and also my first time learning how to conduct molecular simulations. I am currently reading the book Understanding Molecular Simulation to grasp the methodologies, and I was very excited by the fact that the authors of this book will be present for the lectures. Even though this book is enjoyable to read, many concepts within it are quite new to me, so it will be a great opportunity to deepen my knowledge in this area and meet colleagues with similar interests. The latter reason is also very important since I am the only PhD student learning models of active particles for surface phase transitions in my department.\nPlease let me know if you would like me to send my CV. For now, I have attached my personal statement, which I submitted for my PhD application, showing my dynamic and multifaceted experience. I hope this wonderful opportunity will materialise.\n\n[1] N.B. Wilding, A.D. Bruce, Density fluctuations and field mixing in the critical fluid, J. Phys. Condens. Matter 4 (12) (1992) 3087–3108\n[2] N.B. Wilding, Critical point and coexistence curve properties of the Lennard-Jones fluid: a finite-size scaling study, Phys. Rev. E 52 (1) (1995) 602–611\n[3] A.D. Bruce, N.B. Wilding, G.J. Ackland, Free energy of crystalline solids: a lattice-switch Monte Carlo method, Phys. Rev. Lett. 79 (16) (1997) 3002–3005\n[4] S. Consta, N.B. Wilding, D. Frenkel, Z. Alexandrowicz, Recoil growth: an efficient simulation method for multi-polymer systems, J. Chem. Phys. 110 (6) (1999) 3220–3228\n[5] F. Turci, N.B. Wilding, Phase separation and multibody effects in three-dimensional active Brownian particles, Phys. Rev. Lett. 126.3 (2021) 038002.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "reading.html#understanding-molecular-simulation",
    "href": "reading.html#understanding-molecular-simulation",
    "title": "List of My Reading",
    "section": "2024/10/23 - Understanding Molecular Simulation",
    "text": "2024/10/23 - Understanding Molecular Simulation\nBy Dann Frenkel and Berend Smit\n\nInteresting discussion of what it means by a ‘model’ in science\n\n\n\nOn page 6, Introduction",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#understanding-molecular-simulation-3rd-ed",
    "href": "reading.html#understanding-molecular-simulation-3rd-ed",
    "title": "List of My Reading",
    "section": "2024/10/23 - Understanding Molecular Simulation, 3rd Ed",
    "text": "2024/10/23 - Understanding Molecular Simulation, 3rd Ed\nBy Daan Frenkel and Berend Smit\n\nChapter 1. Introduction\nBefore starting our project, I was a little concerned about whether computational simulation is a valid way to understand the fundamental order of nature. I thought that what we do as physicists is observe systems through experiments, theorise with idealised objects to gain predictive power, and then test the theory by observing again. This thought process was probably inspired by a Kantian perspective; I read his Critique of Pure Reason in high school, where he posits that causality is a presupposition of reasoning. For those purposes, idealised objects had to be mathematical (forms or images from Plato) and needed to represent what I directly observe in empirical reality.\n\n\n\nOn page 2, Introduction\n\n\nI’m still not completely certain if computer simulation is truly the way to go, but as I work through it, I hope it will make more sense to me.\n\n\n\n\nOn page 6, Introduction\n\n\nQuite an interesting discussion on what ‘model’ means in science.\n\n\n\nChapter 2. Thermodynamics and Statistical Mechanics\n\n\n\nOn page 22, 2.1.2 Chemical Potential and Equilibrium\n\n\nImportant equilibrium conditions!\n\n\n\n\nOn page 22, 2.1.2 Chemical Potential and Equilibrium\n\n\nI’m not entirely sure why the extremum condition applies specifically to variations in \\(E_1\\), \\(N_1\\), and \\(V_1\\). So, I asked ChatGPT:\n\n\nClick to reveal the answer!\n\nYou’re right to question this subtle point.\nThe key insight lies in understanding that when we talk about maximising the total entropy \\(S_{\\text{tot}} = S_1 + S_2\\) of a closed system with fixed total values of energy, volume, and particle number, the only degrees of freedom available for changes are the distributions of \\(E\\), \\(V\\), and \\(N\\) between the subsystems.\nHere’s the reasoning in detail:\n\nFixed Total Quantities Impose Constraints: Because the total energy \\(E_{\\text{tot}} = E_1 + E_2\\), volume \\(V_{\\text{tot}} = V_1 + V_2\\), and particle number \\(N_{\\text{tot}} = N_1 + N_2\\) are fixed, any changes in \\(E_1\\), \\(V_1\\), or \\(N_1\\) must be offset by corresponding changes in \\(E_2\\), \\(V_2\\), and \\(N_2\\) to keep these sums constant.\nEntropy Extremum Condition: In equilibrium thermodynamics, the Second Law requires that the total entropy of an isolated (or closed) system reaches a maximum (or an extremum) at equilibrium. However, since the system is closed and these total quantities are fixed, the system cannot simply increase or decrease its overall \\(E_{\\text{tot}}\\), \\(V_{\\text{tot}}\\), or \\(N_{\\text{tot}}\\). Instead, any adjustments to entropy must occur through redistributions of these quantities between subsystems.\nRedistribution as the Only Available Variation: Thus, the extremisation of \\(S_{\\text{tot}}\\) must occur specifically with respect to the variables that can change within the subsystems while satisfying the overall conservation constraints. Since \\(E_{\\text{tot}}\\), \\(V_{\\text{tot}}\\), and \\(N_{\\text{tot}}\\) are fixed, the only available independent variations in \\(S_{\\text{tot}}\\) come from redistributing \\(E_1\\), \\(V_1\\), and \\(N_1\\) (and equivalently \\(E_2\\), \\(V_2\\), and \\(N_2\\)).\nDerivative Vanishing Condition: For \\(S_{\\text{tot}} = S_1 + S_2\\) to be maximised (or extremised) under these constraints, the derivatives of \\(S_{\\text{tot}}\\) with respect to \\(E_1\\), \\(V_1\\), and \\(N_1\\) must vanish. This is because any non-zero derivative would indicate that entropy could be further increased by adjusting these redistributive variables, contradicting the requirement of equilibrium.\n\nIn summary, we require the extremum condition with respect to \\(E_1\\), \\(V_1\\), and \\(N_1\\) specifically because these are the variables that can be redistributed between the subsystems within the constraint of fixed total quantities. This is why the extremum condition applies to these specific derivatives, and not to other parameters.\n\n\n\n\n\nOn page 30, 2.2.3 Towards Classical Statistical Mechanics\n\n\nI’m not sure how to derive the transition from 2.2.17 to 2.2.18.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "conferences.html",
    "href": "conferences.html",
    "title": "Conferences",
    "section": "",
    "text": "By Dan Lucas from St Andrews\n\n\nAn attractive approach to tackling the complexity of a turbulent fluid flow is to view the behaviour from a dynamical systems perspective. That is, we might imagine a trajectory navigating a high dimensional phase space directed by the stable and unstable manifolds of simple invariant states. In a fluid system such invariant states might take the form of steady equilibria, travelling waves or time periodic orbits. In many cases isolating such states is challenging due to their high instability. In this talk we will introduce the method of “time-delayed feedback control” as a method to passively stabilise travelling wave solutions to the Navier-Stokes equations. We will survey recent results in this project where we successfully stabilise states from two-dimensional turbulence and turbulence in a rectilinear periodic pipe. In both cases travelling waves can be stabilised at relatively large Reynolds numbers, and in each situation we outline some novel approaches to improve the performance of the control. Along the way we perform linear stability analyses and use methods from control theory to validate our results and help us gain additional insight into the mechanisms underpinning successful control. We also demonstrate gradient descent methods to help optimise control parameters to avoid laborious searching and enable states to be obtained from speculative guesses.\n\n\n\nBefore going to the seminar, there were concepts I couldn’t understand from the abstract, which were:\n\nStable and unstable manifolds of simple invariant states:\n\nIn dynamical systems, an invariant state refers to a state of the system that doesn’t change over time (e.g., a fixed point or periodic orbit). Manifolds are like paths or surfaces that represent how nearby points behave relative to this state.\n\nA stable manifold means nearby points move towards the invariant state (like water flowing towards a drain).\nAn unstable manifold means nearby points move away from the state (like a ball rolling off the top of a hill).\n\n\nSteady equilibria, travelling waves, or time-periodic orbits:\n\nSteady equilibria are solutions where the system remains constant over time.\nTravelling waves are wave-like solutions that maintain their shape while moving through space.\nTime-periodic orbits are states where the system repeats its behaviour in a regular time cycle, like a pendulum swinging back and forth.\n\nTime-delayed feedback control:\n\nThis is a control method where the system’s state is monitored and the control signal applied is based on the system’s past states, not its current state. The delay refers to how far back in time you look to adjust the system’s behaviour. It’s often used to stabilise unstable systems by correcting them with delayed feedback.\n\nRectilinear periodic pipe:\n\nThis describes a straight (rectilinear) pipe where the fluid flow repeats itself periodically along the length of the pipe, meaning the pipe’s conditions are the same at regular intervals.\n\nReynolds numbers:\n\nThe Reynolds number is a dimensionless quantity used to predict the flow behaviour of fluids. It compares inertial forces to viscous forces and helps determine whether a flow will be laminar (smooth) or turbulent (chaotic). Larger Reynolds numbers typically indicate turbulent flow.\n\nControl theory:\n\nThis is a field of study focused on how to control systems to achieve desired behaviours. It involves using mathematical models to design strategies (called controllers) to manage dynamic systems (like fluid flow) so they behave in a certain way.\n\nSpeculative guesses:\n\nThis refers to making educated guesses about the system’s parameters or initial conditions to find solutions or states of interest without fully knowing the correct values in advance. It suggests an exploratory approach.\n\n\n\n\n\nI could not understand most of what he talked about, so my mind wandered. I began thinking about how I could approach solving an important problem in the vaping industry: the ideal position (such as height) of the coil in the atomiser. The object of interest can be described as follows:\n\nThere are posts (positive/negative), and the coil is usually installed between them. The coil is typically made from Kanthal A1, NiChrome80, or SS316L, listed in order of common usage.\nSince the coil is wrapped in a helical form, cotton is inserted inside the coil to hold the e-liquid (it’s easier to see than explain—check my business Instagram).\nThe e-liquid consists of four components: PG (propylene glycol), which has low viscosity, VG (vegetable glycerin), which has high viscosity, flavourings (dissolved in PG), and nicotine (also dissolved in PG). We can approximate the e-liquid as a mix of PG and VG.\nAir typically enters from the side, while the vaporised liquid and air leave through the top of the atomiser by suction.\nThe temperature of the coil continuously changes, affecting the vaporisation process. This temperature is influenced by how incoming air interacts with the droplets formed on the coil.\n\nThe vaping industry mainly focuses on finding the optimal coil height by using computational aerodynamics software to maximise airflow around the coil, prolonging its life. However, not all of these simulations reflect reality very well because:\n\nThe power, which dictates the temperature gradient of the coil, is determined by the user’s settings on their vape mod.\nEveryone’s suction strength varies.\nCoil diameters differ.\nThe ratio of PG to VG depends on the user—typically 50/50 for mouth-to-lung vaping and 30/70 for direct-lung vaping.\nAmbient temperature, which affects the incoming air, differs from user to user.\nDepending on how the cotton is placed inside the atomiser, the available volume for airflow can change.\nThere is a continuous influx of liquid through the cotton during vaporisation.\n\nSo, many factors are involved. I was thinking that I could approach this problem from a different angle: how can I make all the droplets evaporate effectively? I’m not sure if I can tackle this in the context of active drying, but it’s a possibility.",
    "crumbs": [
      "Project Notes",
      "Conferences"
    ]
  },
  {
    "objectID": "conferences.html#stabilising-travelling-waves-from-fluid-turbulence.",
    "href": "conferences.html#stabilising-travelling-waves-from-fluid-turbulence.",
    "title": "Conferences",
    "section": "",
    "text": "By Dan Lucas from St Andrews\n\n\nAn attractive approach to tackling the complexity of a turbulent fluid flow is to view the behaviour from a dynamical systems perspective. That is, we might imagine a trajectory navigating a high dimensional phase space directed by the stable and unstable manifolds of simple invariant states. In a fluid system such invariant states might take the form of steady equilibria, travelling waves or time periodic orbits. In many cases isolating such states is challenging due to their high instability. In this talk we will introduce the method of “time-delayed feedback control” as a method to passively stabilise travelling wave solutions to the Navier-Stokes equations. We will survey recent results in this project where we successfully stabilise states from two-dimensional turbulence and turbulence in a rectilinear periodic pipe. In both cases travelling waves can be stabilised at relatively large Reynolds numbers, and in each situation we outline some novel approaches to improve the performance of the control. Along the way we perform linear stability analyses and use methods from control theory to validate our results and help us gain additional insight into the mechanisms underpinning successful control. We also demonstrate gradient descent methods to help optimise control parameters to avoid laborious searching and enable states to be obtained from speculative guesses.\n\n\n\nBefore going to the seminar, there were concepts I couldn’t understand from the abstract, which were:\n\nStable and unstable manifolds of simple invariant states:\n\nIn dynamical systems, an invariant state refers to a state of the system that doesn’t change over time (e.g., a fixed point or periodic orbit). Manifolds are like paths or surfaces that represent how nearby points behave relative to this state.\n\nA stable manifold means nearby points move towards the invariant state (like water flowing towards a drain).\nAn unstable manifold means nearby points move away from the state (like a ball rolling off the top of a hill).\n\n\nSteady equilibria, travelling waves, or time-periodic orbits:\n\nSteady equilibria are solutions where the system remains constant over time.\nTravelling waves are wave-like solutions that maintain their shape while moving through space.\nTime-periodic orbits are states where the system repeats its behaviour in a regular time cycle, like a pendulum swinging back and forth.\n\nTime-delayed feedback control:\n\nThis is a control method where the system’s state is monitored and the control signal applied is based on the system’s past states, not its current state. The delay refers to how far back in time you look to adjust the system’s behaviour. It’s often used to stabilise unstable systems by correcting them with delayed feedback.\n\nRectilinear periodic pipe:\n\nThis describes a straight (rectilinear) pipe where the fluid flow repeats itself periodically along the length of the pipe, meaning the pipe’s conditions are the same at regular intervals.\n\nReynolds numbers:\n\nThe Reynolds number is a dimensionless quantity used to predict the flow behaviour of fluids. It compares inertial forces to viscous forces and helps determine whether a flow will be laminar (smooth) or turbulent (chaotic). Larger Reynolds numbers typically indicate turbulent flow.\n\nControl theory:\n\nThis is a field of study focused on how to control systems to achieve desired behaviours. It involves using mathematical models to design strategies (called controllers) to manage dynamic systems (like fluid flow) so they behave in a certain way.\n\nSpeculative guesses:\n\nThis refers to making educated guesses about the system’s parameters or initial conditions to find solutions or states of interest without fully knowing the correct values in advance. It suggests an exploratory approach.\n\n\n\n\n\nI could not understand most of what he talked about, so my mind wandered. I began thinking about how I could approach solving an important problem in the vaping industry: the ideal position (such as height) of the coil in the atomiser. The object of interest can be described as follows:\n\nThere are posts (positive/negative), and the coil is usually installed between them. The coil is typically made from Kanthal A1, NiChrome80, or SS316L, listed in order of common usage.\nSince the coil is wrapped in a helical form, cotton is inserted inside the coil to hold the e-liquid (it’s easier to see than explain—check my business Instagram).\nThe e-liquid consists of four components: PG (propylene glycol), which has low viscosity, VG (vegetable glycerin), which has high viscosity, flavourings (dissolved in PG), and nicotine (also dissolved in PG). We can approximate the e-liquid as a mix of PG and VG.\nAir typically enters from the side, while the vaporised liquid and air leave through the top of the atomiser by suction.\nThe temperature of the coil continuously changes, affecting the vaporisation process. This temperature is influenced by how incoming air interacts with the droplets formed on the coil.\n\nThe vaping industry mainly focuses on finding the optimal coil height by using computational aerodynamics software to maximise airflow around the coil, prolonging its life. However, not all of these simulations reflect reality very well because:\n\nThe power, which dictates the temperature gradient of the coil, is determined by the user’s settings on their vape mod.\nEveryone’s suction strength varies.\nCoil diameters differ.\nThe ratio of PG to VG depends on the user—typically 50/50 for mouth-to-lung vaping and 30/70 for direct-lung vaping.\nAmbient temperature, which affects the incoming air, differs from user to user.\nDepending on how the cotton is placed inside the atomiser, the available volume for airflow can change.\nThere is a continuous influx of liquid through the cotton during vaporisation.\n\nSo, many factors are involved. I was thinking that I could approach this problem from a different angle: how can I make all the droplets evaporate effectively? I’m not sure if I can tackle this in the context of active drying, but it’s a possibility.",
    "crumbs": [
      "Project Notes",
      "Conferences"
    ]
  },
  {
    "objectID": "index.html#groups",
    "href": "index.html#groups",
    "title": "Project Description",
    "section": "Groups",
    "text": "Groups\nSoft matter community\nUpcoming events by Nonlinear and Complex Physics Group",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "notes.html#week-7",
    "href": "notes.html#week-7",
    "title": "Weekly Notes",
    "section": "Week 7",
    "text": "Week 7\n\n2024/10/28 Weekly project meeting\n\nThings to discuss\n\nAsk about page 51 on Molecular Dynamics Simulation showing notes written on Zotero.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#week-8",
    "href": "notes.html#week-8",
    "title": "Weekly Notes",
    "section": "Week 8",
    "text": "Week 8\n\n2024/11/4 Weekly project meeting\n\nThings to discuss (written on 11/2)\nThe meaning of our project\nI’m afraid to say I’m not exactly certain about the importance of our initial project. To me, a naive student who has just finished my undergraduate in physics, physics operates by explaining experimental results in mathematical language. As a few examples,\n\nIn the early 20th century, we realised that classical physics could not explain the ultraviolet catastrophe. So Planck suggested that light exists in quanta to explain the spectra we observe, which later led to Bohr’s theory of electron orbits. Furthermore, when classical electrodynamics dictated that an orbiting electron around the nucleus should radiate energy and spiral inward, the new language of quantum mechanics was formulated to explain the stability of atoms. This process of experimental results leading to theoretical explanations, which further lead to contradictions with existing theories, gives birth to new frameworks that unite our understanding.\nThe Michelson–Morley experiment investigating the existence of the ether disproved their hypothesis, supporting the constancy of the speed of light, \\(c\\). This result led Einstein to create a theory which completely changed the very meanings of our familiar, intuitive concepts: space and time. Another example of how experimental results led to the unification of our understanding; for example, magnetism can be understood as electricity in special relativity.\nEven in statistical physics, the phase diagram of water has been known for a long time, but the critical point wasn’t understood until the mid to late 20th century. It was not until Ken Wilson formulated the renormalisation group to explain this by rescaling physical theories (which I find one of the most beautiful results in theoretical physics).\n\nI have chosen the active matter project because it’s a framework that makes the most sense to me, due to my keen interest in philosophy throughout the years, which has made me already familiar with the active matter paradigm shift. If I describe briefly,\n\nThe reason that Newtonian mechanics completely altered how scientific disciplines have progressed is because I believe it gave us what it means by our ‘understanding’. Before Newton, Ptolemy’s circles and epicycles could fully explain the orbits of planetary bodies. However, we don’t consider this as ‘understanding’ the underlying mechanism because what Ptolemy did was to focus only on motions. What Newton did, on the other hand, was to identify the ‘cause’ of motion, which is force. By introducing that concept, he was able to derive his simple equation of universal gravity, which applies to many different things, not just planetary bodies, but also projectiles, pendulums, etc. So by his theory, we were able to unite many phenomena that we observe into one unified framework.\nHowever, he still couldn’t explain how these forces arise, so he devised what are called ‘active principles’. I could elaborate further, but I shall stop here. The important thing about this new revelation was that, from the Newtonian way of thinking, we conceived matter as passive and forces as active. We thought of matter as inert, dead, inanimate. No wonder the physicalist view in biology had failed by the end of the 19th century, which led to the ‘century of the gene’ in the 20th century, explaining everything in terms of gene expression. However, thinking only about genes couldn’t explain many things in biology such as: what is the difference between animate and inanimate things? Why are there many phenotypes from the same genotype? So what Thomas Huxley hoped for at the end of the 19th century, the physical language to explain biological matter, has resurfaced at the beginning of the 21st century with the emergence of active matter, where soft matter physicists could contribute. All in all, at Oxford University, there is the phrase: ‘Biology is soft matter come alive’. I would interpret it as ‘active’, though.\n\nThe significance of this new way of thinking is apparent to me. However, what is not clear is how observing a drying transition by active particles contributes to our understanding of drying phenomena. The reason I kept asking about real life examples is because I wasn’t sure if our results (if we observe drying) would explain the experimental results that need to be explained. If there are such cases, this would be recognised as our correct understanding of drying phenomena, which leads us to think that moving out of equilibrium statistical physics is a more reasonable direction. Since we are dealing with such limited, specific systems where liquid and vapour coexist, which is already quite rare in nature, for this project meeting, I hoped to gain more understanding of the significance of our project.\n\n\n\n2024/11/4 Diary\nI was quite ill last week, taking antibiotics from Monday to Saturday. While having more rest at home than in other weeks, I was thinking about a few things.\n\nAt first, I started thinking about why I began to study physics. It was driven by mysteries and curiosities. At that time, I didn’t know that what I was going to learn were representations of empirical observations; rather, I naively thought it would be the objective truth. As I was learning physics in my undergraduate studies, I realised there are different mathematical formalisms to describe the same physical phenomena, such as Newtonian, Lagrangian, and Hamiltonian mechanics in classical physics, or matrix mechanics by Heisenberg and wave mechanics by Schrödinger in quantum mechanics. Then my approach to physics became more like, ‘Which model makes the most sense to me for understanding the observed?’ or ‘Which models lead to more universal descriptions of all things?’ Then I encountered the concept of universality: many of the microscopic details that go into the theory are irrelevant under successive renormalisation group transformations; the theory that describes the critical point is universal. I thought this idea was stunningly beautiful, and I thought that if I were to start my PhD, I would investigate how these patterns that interlace between different systems or physical theories emerge.\nThen, as I was ill and thinking about many things, I reflected on what has captivated me in my undergraduate studies. The list follows like this:\n\n\nYear 1, The breakdown of the concept, simultaneity: As I was learning special relativity, I realised that the notion I thought was objective was actually subjective. The only way to know what time is is to look at the clock. This idea was profound for me, emphasising that time is not an absolute entity. Einstein’s theory was so elegant in the way that it started with only two assumptions: first, the speed of light is constant based on empirical observations; second, the laws of physics are the same in all inertial frames of reference. The consequences of the theory based on these assumptions were just mind-blowing.\nYear 2, first term, quantum tunnelling: I was always fascinated by the implications of quantum mechanics even before starting my physics degree. Once I was able to explain the phenomenon of the wavefunction trapped in a finite potential well, which allows for the probability of finding a particle on the other side of the barrier, it was really mind-blowing that I clearly understood the mathematics but had no idea how to comprehend it, even though this phenomenon is the foundation of all technologies I use now.\nYear 2, second term, Maxwell’s equations: The universal descriptions that entail all electromagnetic phenomena in such simple mathematical forms using vector calculus. I was struck by the beauty of these equations that I set them as a screensaver since my second year. Also, when I went to Edinburgh and saw a statue of Maxwell with his equations, I couldn’t leave that place for a long time.\nYear 3, second term, the principle of least action: This whole Lagrangian formulation of classical mechanics was the clearest and the deepest representation I had encountered. The path is chosen to extremise the action, which was such a profound perspective that changed all my ways of thinking. I could go on and on about this, but in short, I loved the encounter with this concept so much that I used it as a nickname on several websites.\nYear 4, first term, the whole ideas from general relativity: This subject was of tremendous importance to me. When I decided to study physics, this was the subject I intended to study and also the reason I chose the MSci, as it was a unit for fourth-year students. I would say this is the subject which made me come to university. The idea that if we also include a time axis overlaid onto three spatial axes, an object at rest in a given reference frame moves at the speed of light along the time axis. Then, if an object moves, since the magnitude of speed is bound by the speed of light, the component that lies on the time axis decreases as it increases along the spatial axes. It was such a beautiful way of describing how time and space were interlinked. Also, the calculus of variations, which led to Lagrangian mechanics that I loved, started from considering the straight line in a global perspective. Einstein’s theory started with thinking about the straight line (or rather more precisely, a vector) from a local perspective with parallel transport. The implication of this led to Einstein’s field equations, which imply matter tells spacetime how to curve and spacetime tells matter how to move. These whole ideas related very closely to some of my psychedelic experiences, and I thought it was THE subject I would want to learn much more deeply in the coming years, before I met the language of the physics of phase transitions.\nYear 4, second term, the whole ideas from the physics of phase transitions: The correlation length diverging at criticality, universality classes, critical exponents telling how to rescale a physical theory—all these were so fascinating that it almost felt like meta-foundational physics, like physics of physical theory. Also, a lot of concepts extended even beyond the realm of physics, such as the critical brain hypothesis in neuroscience. This perspective of considering the structure of physical theory was like the final lesson of my undergraduate studies and the one that I wanted to pursue for the rest of my physics career. At that time, I met Nigel, my supervisor, who told me that in statistical physics, a small number of people make a great contribution to the field, unlike astrophysicists or particle physicists who are in large groups and don’t get to decide the direction. That was the moment I felt that I chose the field of statistical physics, even though I have loved concepts from astrophysics (general relativity) and particle physics (quantum field theory).\n\n\nNow I’m in week 8 of my PhD, but I can’t say that I have been really engaged with my research. The biggest fault is down to me: I haven’t read references thoroughly so that I can appreciate the field in general yet. I’m the person who likes to read one book over and over again (and it seems that the only professor who likes to do this is Sandu Popescu), but I am kind of overwhelmed by how many references were uploaded in Zotero and the time it takes for me to read understanding molecular simulation. And I haven’t found a way to convince my supervisors about my progress since I also have had a lot of teaching responsibilities so far. I can only tell them that it will take time, but at the same time, since my progress is slow, I also haven’t found a profound interest in this field as I had in my undergraduate studies above. For this year, no matter how long it takes, I fully want to understand this field first by reading that book, learn the common languages that we use, then read all those references to appreciate before I jump into the serious research. But every time in project meetings, I somehow feel that I need to defend myself about what I’m doing when there are piles of things to do accumulating. In short, I feel overwhelmed.\nThat was the reason that my rest has been quite different and sidetracked this time in week 7. I started reading Physics and Beyond written by Werner Heisenberg (see 2024/11/2 - 11/8 Physics and Beyond). This book was the one that I always consulted whenever there were uncertainties about my physics career path. I am on page 108 now out of 268. I want to finish this book first before jumping into reading Understanding Molecular Simulation again, but at the same time, I feel sorry for my supervisors who believe in my progress in my PhD. It’s all difficult now in terms of my situation.\n\n\n\n2024/11/5 Being a Good Lecturer\nAfter attending Hermes’ lecture, I realised there are certain qualities that a very good lecturer shares. To describe him:\nHe was smiling, making jokes, honest, sharing life stories, and, most importantly, engaging with students at their level.\nOften, when I talk with certain lecturers or fellow physics PGRs, they ramble without checking if the person listening is following. Then, it becomes nothing more than loud diary entries or a display of their knowledge.\nI also felt that the language barrier was irrelevant. Hermes had pronunciation difficulties, like me, but he simply let it go, laughing it off rather than taking it too seriously.\nI was quite surprised by how many qualities he actually shared with another of my favourite lecturers, Sandu Popescu. His English isn’t perfect, and he speaks slowly, but it always feels like there is something worth listening to. Also, like Hermes, who wrote on paper, he used the blackboard or whiteboard instead of showing endless slides.\nHermes made plenty of eye contact and made excellent use of pauses during his explanations, allowing students to form images in their minds for better understanding.\nThere was much for me to learn from his approach to lecturing. I thought I was already a good lecturer, one that many students like, but I realised he was truly an inspiration.\n\n\n2024/11/8 Active Brownian Particles (ABPs)\nI now understand why ABPs drive the system out of equilibrium. I love deriving everything mathematically from first principles.\nThings I have understood:\n\nThe equations of motion for ABPs are described by overdamped Langevin equations.\nPersistent fluxes of particles prevent the detailed balance condition; therefore, there is no Boltzmann distribution.\n\nI’m still uncertain about the concept of entropy production. When I finish reading Physics and Beyond, I’ll start reading Understanding Molecular Simulation again, as it has a chapter that discusses this concept.\nThese are the PDF notes that I wrote: My handwritten notes",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "reading.html#physics-and-beyond",
    "href": "reading.html#physics-and-beyond",
    "title": "List of My Reading",
    "section": "2024/11/2 - Physics and Beyond",
    "text": "2024/11/2 - Physics and Beyond\nBy Werner Heisenberg\nIt’s the first time I’m reading this in English; the title of the book in Korean is 부분과 전체, which might translate to Parts and the Whole. I’ve read it many times, and it has been highly influential to me as a physicist. To capture some of the things I want to revisit later:\n\n\n\nOn page 72, Fresh Fields (1926-1927)\n\n\n\n\n\nOn page 80, Fresh Fields (1926-1927)\n\n\nSo far, having read up to page 89, it has all been interesting, except for Chapter 4 (Lessons in Politics and History).",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#sec-physics-and-beyond",
    "href": "reading.html#sec-physics-and-beyond",
    "title": "List of My Reading",
    "section": "2024/11/2 - 11/8 Physics and Beyond",
    "text": "2024/11/2 - 11/8 Physics and Beyond\nIt’s the first time I’m reading this in English; the title of the book in Korean is 부분과 전체, which might translate to Parts and the Whole. I’ve read it many times since I was in high school, and it has been highly influential to me as a physicist throughout my life. To capture some of the things I want to revisit later:\n\n\n\nOn page 72, Fresh Fields (1926-1927)\n\n\n\n\n\nOn page 80, Fresh Fields (1926-1927)\n\n\nSo far, having read up to page 89, it has all been interesting, except for Chapter 4 (Lessons in Politics and History). When the development of quantum mechanics was presented in Chapters 5 and 6, I felt my heart pounding as I read through, just as it did when I first read this book.\n\n\n\n\nOn page 97, Atomic Physics and Pragmatism (1929)\n\n\nAll these realms were so enjoyable to learn, but now, as a researcher, I wonder what realms are still open. I thought one of them was non-equilibrium statistical thermodynamics, but it appears to me that this theory can’t be axiomatic, implying that all systems out of equilibrium must be described differently.\n\n\n\n\nOn page 109, The Relationship between Biology, Physics and Chemistry (1930-1932)\n\n\n‘Forms through which matter streams,’ as Niels Bohr described, was the most profound characterisation of organisms I have encountered. I used to wonder, what about this structure remains? Does it actually persist, given that a person changes continuously as a child becomes an adult? Is it simply the name (person) we assign that represents this form of life? With this thought, is there really a distinction between animate and inanimate objects?\nWhen I read Timaeus by Plato in high school, he described geometrical forms as the basis of earth, fire, water, and air, even though I found his idea peculiar (like everything is made of right triangles). At the time, I thought the properties that define those mathematical forms were responsible for things maintaining their structure in the way we perceive over time (since all forms change shape as time passes). My master’s research was also focused on investigating this homeostasis in an ideal structure—not physical but something we perceive—in the context of foreign exchange markets. This line of thought was also captured well by Bohr’s question: “Can quantum mechanics explain nature’s tendency to form structures through which matter with fixed chemical properties can stream for a limited time?”\n\n\n\n\nOn page 128, Discussions about Language (1933)\n\n\nThis passage about the discovery of antimatter was one of the reasons I chose a physics degree, along with my ambition to formally study general relativity. Although I didn’t score well in my particle physics exam in third year due to a part-time job over winter break, it remains one of the most exciting units I’ve taken. Here are the scores and ratings I gave to my third and fourth-year units with scores of at least 4:\n5: Physics Research Project (83), Quantum Physics (88)\n4.5: Physics of Phase Transitions (85), Analytical Mechanics (66), Particle Physics (58)\n4: Quantum Information Theory (68), Foundations of Modern Physics (68), Methods of Theoretical Physics (63)\nIt’s surprising that general relativity only scored 3.5, but it seems I didn’t enjoy the lecture style despite being fascinated by the content.\n\n\n\n\nOn page 134, Discussions about Language (1933)\n\n\nThis passage captures the reason I didn’t pursue a philosophy degree, despite having written about philosophy of science. What I realised was the fluidity of language, and at some point, reading philosophy felt like participating in a language game. Even when I wrote The Self and the World, I included definitions for the terms I used in the document. In contrast, I conceived mathematical objects as eternal, grounded solidly in axioms. I couldn’t imagine any language other than this to describe the hidden secrets of the universe as eternal representations—unchanging, unmoved, and concrete.\n\n\n\n\nOn page 136, Discussions about Language (1933)\n\n\nAnother book I feel closely connected to is the Tao Te Ching by Lao Tzu. It’s probably not an exaggeration to say that my whole life has been devoted to understanding the meaning of Tao, even though Lao Tzu paradoxically said, “The Tao that can be described is not the eternal Tao.” One day, I realised it is synonymous with the Truth or, in more Platonic terms, the formless. Ineffable. That’s when I wrote this diary: Is “It” a Joke?\n\n\n\n\nOn page 191, Toward a New Beginning (1941-1945)\n\n\nThese chapters were not enjoyable to read: Revolution and University Life (1933), Individual Behavior in the Face of Political Disaster (1937-1941), Toward a New Beginning (1941-1945). I’m certainly not interested in politics or wars. However, the ending of this third chapter is beautifully written; beauty transcends ideology, opinions, and dualistic views of the world.\n\n\n\n\nOn page 195, The Responsibility of the Scientist (1945-1950)\n\n\nThis reveals the reason why I chose the theme of active matter. I was curious about how this model suddenly emerged in the last 25 years. Why is there such a sudden interest in this concept? What possibilities does it offer for understanding nature? What contributions can I make to the circle of human knowledge?\n\nWhile reading through the Scientific and Political Disputes (1956-1957) chapter, I realised I can only concentrate when scientific matters are being discussed, such as the conversations between Heisenberg and Wolfgang about the mathematical theory for elementary particles.\n\n\n\nOn page 230, The Unified Field Theory (1957-1958)\n\n\nHowever, everything that followed from the passage in the image was fascinating—especially when they started discussing symmetries (for example, the universe as a whole does not necessarily have to be symmetrical with respect to the operations under which natural laws remain invariant), relativistic quantum field theories, and more, which captivated me when I studied particle physics in my third year.\n\n\n\n\nOn page 240, Elementary Particles and Platonic Philosophy (1961-1965)\n\n\nI have finally finished the book. Although the above image is not the last page, I love this book because it begins with Plato and ends with him. Timaeus was the book that shaped my worldview when I was in high school, and when I first encountered Physics and Beyond, I was thrilled to find someone in history who asked the same questions I had while reading Timaeus. Since then, my understanding has deepened, and I’ve come to see that all philosophy traces back to Plato. Last year, a decade later, I finally had a direct understanding of what Plato described through some interesting experiences.\nEven beyond that passage in the screenshot, it’s truly worth rereading. People at that time were already considering how natural laws conform to different scales, touching on the concept of emergence, which is the field I’m now working in.\nWhile reading this book, I rewatched the documentary series Atom, hosted by Jim Al-Khalili. I remember watching it for the first time in high school, loving it immensely, and being struck by statements from scholars like Paul Dirac, who said, “A physical law must possess mathematical beauty.” I was curious about what QED implied once I start learning about it at university, such as how all the matter we see is just reflections of virtual particles. That was a long time ago, and that naïveté and curiosity are what led me to study physics. I’d like to close with what Jim Al-Khalili said (even though I largely disagree with his interpretation of quantum mechanics in that chapter) at the end of The Illusion of Reality (the third episode):\nIn the last 100 years, we have peered deep inside the atom, the basic building block of the universe. And inside this tiny object, we have found a strange new world governed by exotic laws that at times seem to defy reason. Atoms present us with dizzying contradictions. They can behave both as particles or waves. They appear to be in more than one place at the same time. They force us to rethink what we mean by past and future, by cause and effect. And they tell us strange things about where the universe came from and where it’s going. Pretty amazing stuff for something that’s just a millionth of a millimetre across. That’s why Niels Bohr, the father of atomic physics, once said that, when it comes to atoms, language can only be used as poetry. What’s fascinating to me is that although we’ve learnt an incredible amount about atoms and their behaviour, our scientific journey has only just begun. Because although we know how a single atom or just a few atoms behave, the way trillions of them come together in concert to create the world around us is still largely a mystery. To give you one dramatic example, the atoms that make up my body are identical to the atoms in the rocks, the trees, the air, even the stars. And yet, they come together to create a conscious being who can ask the question, “What is an atom?” Explaining all that is surely the next great challenge in science.\nI realised that the “next great challenge” he mentioned is the same one I’ve been drawn to since studying physics: the emergence of consciousness. The most intriguing question, perhaps prompted by an interesting dream I had last night, is how our mind creates the world and perceives it simultaneously when we dream. When I’m awake, I can do this partially; for instance, when I think about walking down the corridor where my office is located, I can imagine it while experiencing it at the same time. It seems this ability expands to full capacity in dreams. Once I understand that mechanism, I might be able to discern, even when awake, whether I am still dreaming and creating the world while simultaneously perceiving it.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#sec-physicsandbeyond",
    "href": "reading.html#sec-physicsandbeyond",
    "title": "List of My Reading",
    "section": "2024/11/2 - Physics and Beyond",
    "text": "2024/11/2 - Physics and Beyond\nBy Werner Heisenberg\nIt’s the first time I’m reading this in English; the title of the book in Korean is 부분과 전체, which might translate to Parts and the Whole. I’ve read it many times, and it has been highly influential to me as a physicist. To capture some of the things I want to revisit later:\n\n\n\nOn page 72, Fresh Fields (1926-1927)\n\n\n\n\n\nOn page 80, Fresh Fields (1926-1927)\n\n\nSo far, having read up to page 89, it has all been interesting, except for Chapter 4 (Lessons in Politics and History).",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#the-science-of-interstellar",
    "href": "reading.html#the-science-of-interstellar",
    "title": "List of My Reading",
    "section": "2024/11/9 - The Science of Interstellar",
    "text": "2024/11/9 - The Science of Interstellar\nToday, I rewatched the film Interstellar, perhaps for the fourth time. I was deeply impressed by Nolan’s previous film Inception, which I also probably watched five times already. Yesterday, I had to give Inception a 10-star rating for the first time in my life, as it just gets more interesting with every viewing. This 10-star rating is very special to me, as I have never rated any of the over 3,000 films I’ve watched so highly.\nAfter this purely fantastical journey, I became curious about the science behind it. Then I realised that the scientific adviser for the film, Kip Thorne, actually wrote a book about it! I knew I had planned to read Understanding Molecular Simulation today, but since it’s the weekend, I decided to indulge in this investigation instead.\n\n\n\nOn page 45, 4. Warped Time and Space, and Tidal Gravity\n\n\nWhat a beautiful way of interpreting general relativity!\n\nSo far, having read up to Chapter 6, it has all been very interesting! But since Monday has arrived, it’s time to focus on my project again…",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "notes.html#week-9",
    "href": "notes.html#week-9",
    "title": "Weekly Notes",
    "section": "Week 9",
    "text": "Week 9\n\n2024/11/13 Mathpix and Stochastic Processes\nFrancesco introduced a tool called Mathpix, which looks incredibly useful for converting handwritten notes from Goodnotes into formal documents!\nHe also shared his draft notes on random walks and stochastic processes: Handbook of Particle-Based Simulation of Fluids, which I can download as .ipynb files to experiment with the code.\nI also received a very warm, thoughtful email from Nigel, my supervisor. Since I’d like to reread it often, I want to preserve it here with password protection:\n\n  \nHi Sohyun,\n\nThanks for your messages. (I thought I’d move things to email as it is easier for longer conversations).\n\nHappy to talk about your worries at the next meeting. I would just say at the start that I am not currently concerned about your progress. It is normal at the start of a PhD to spend quite a lot of time “getting the lay of the land”. We appreciate that with such a lot of new ideas and concepts that things can sometimes feel overwhelming, and it can be difficult to separate the wood from the trees. Please be reassured that this is normal.\n\nI would also say that it is very positive that you are engaged with a good number of complex concepts and trying to understand them in your own terms. The students who don’t make progress are usually the ones who are not engaged, so again I think you can be reassured.\n\nWhen it comes to reading papers/books that we have suggested, I just wanted to comment that the way most scientists absorb information is usually not by sitting down, reading a paper from cover to cover and not putting it down until they understand every detail. If we did that, we wouldn’t get anything done. Rather we try to first get a feel for roughly what the message of the paper is talking about, the main ideas if you like, and very often that is as far as it goes. If there is something that we find particularly interesting or relevant we might go into more detail, but I would say that that is the exception. So, if we put papers on Zotero and suggest you look at them, we don’t mean that you need to read and understand everything in great detail. In most cases you might just want to get a rough idea of what a paper is about (AI can help here) and build up some familiarity with the types of approaches and language that are used in the field.\n\nAnother comment I would make is that it is not always necessary as a PhD student to be able to derive from first principles the methods and results that underpin your approach. Of course, we need to feel reasonably confident that we are using tried and tested techniques for which the strengths and weakness are known. But often the methods themselves might be based on decades of work drawn from vast fields of literature. If one were not to try to make progress until all the foundations had been thoroughly understood, then there would be little time left to measure things. I think it is quite right and proper that you want to understand where the Langevin or ABP equations of motion comes from and how this translates into a time stepping algorithm for evolving the system. But I don’t think one necessarily needs to go all the way back to Einstein’s original papers to do this.  You told me (I think in the context of your project last year) that you felt the need to understand all the foundations before making progress on the project. I would just comment that often we learn ‘on the job’, so to say, ie we might start calculating or measuring without understanding completely what we are doing or why, but the process of ‘doing’ builds that understanding.\n\nAnyway, I hope these comments help and we can certainly discuss further next week.\n\nBest wishes,\n\nNigel\n  \n\n\n\n  Type password to reveal the email:\n  \n  Submit\n\n\n\n\n\n2024/11/14 Reading Francesco’s notes\n\n\n\nOn One dimensional motion and its statistics, Random Walks\n\n\nDerivations: My handwritten notes\nStill not sure how \\(l^2\\) came in the equations…\n\n\n2024/11/16 Watching Biophysics Lectures\n \nI decided to watch lectures from weeks 1, 2, 4, and 5.\n\n\n\n\nLecture, Monday October 3rd\n\n\nThis was quite interesting, as it implies that the duality of the self and the world subsists in the possibility of having the same copy of DNA (which is also unique to myself compared to others, even though the similarity in DNA is striking across humans: approximately 99.9% of the DNA sequence is the same between any two individuals). For example, the clothes I am wearing now do not share the same DNA.\nHowever, after finishing week 1, it wasn’t very exciting. I would like to work with (idealised) mathematical objects that have relevance to empirical reality, not specific details about how DNA is formed, which chemicals and bonds are involved, or what functions it performs. When I was learning astrophysics, I remember only liking cosmology out of all the astrophysics fields because the other fields felt like the biology of stars, planets, or galaxies. It seems I’m not very interested in these phenomenological descriptions.\n\n\n\n\nLecture, Monday October 10th\n\n\nThis is quite interesting: the protein coat (capsid) enclosing the nucleic acid of a virus is always a multiple of 60, such as 60, 120, or 180. Why this number? Why does it exhibit icosahedral symmetry?\n\n\n\nLecture, Monday October 10th\n\n\nAlso, this is fascinating. Nature can achieve self-assembly, but humans cannot, no matter how much we know about the individual ingredients that make up these structures (even though the lecturer mentioned that simple structures like helices, such as the tobacco mosaic virus, can be made in the lab). Most of the lecture content here was quite engaging as it covered protein folding. I remembered writing an essay on AlphaFold for coursework while attending these lectures two years ago.\nBut no, as soon as the next lecture started covering the chemistry of protein folding and the formation of primary and secondary structures, it completely lost me. It took a few hours to complete one lecture because it was just not interesting.\n\n\n\n\nLecture, October 19th\n\n\nI guess that’s why I’m more interested in fungal networks than other life forms, which I find mundane…? After this quiz, there were further explanations about hydrogen bonding, van der Waals attractions, hydrophobic effects, and so on, but they were all nightmares and torturous. There were mentions of entropic costs, enthalpically favourable states, and other concepts, but without equations, it all felt hollow and uninteresting. I’m not sure if this was the lecturer’s fault or the content itself, but I hated learning about all this. No wonder I gave a score of just 2 for this unit. I wanted to stop, but I know the upcoming topics will be about self-assembly or molecular motors, so I decided to persevere… What torture!\n\nIt really makes me reflect on what I’ve been genuinely interested in. From my third year, I started rating how much I enjoyed each unit after finishing it, based on this scale:\n\n5: I found it very interesting\n\n4: Interesting\n\n3: Okay\n\n2: Not much fun\n\n1: I hated it\n\nHere are the scores for my third-year and fourth-year units:\n\n5: Quantum Physics / Physics Research Project\n\n4.5: Particle Physics, Analytical Mechanics / The Physics of Phase Transitions\n\n4: Methods of Theoretical Physics / Foundations of Modern Physics, General Relativity and Cosmology, Quantum Information Theory\n\n3.5: Materials Physics\n3: Environmental Physics\n\n2.5: Solid State Physics / Applied Materials Physics\n\n2: Biophysics\n\nBased on this, how did I end up choosing my PhD project in active matter? There were complex reasons for choosing it, as I outlined in 2024/11/14 - Skimming through papers, but honestly, I should have thought it through more carefully (even though, at the time, I believed I was being very thorough).\nSo, what am I really interested in? It would be the foundations of modern physics—fairly mathematical work, possibly with computational approaches. However, I’m also deeply interested in neuroscience. I thought I would find a connection by pursuing active matter projects, as studying the brain is a subset of biology. Now, I’m not so sure…\nSo, I looked at the programme for the conference Statistical Physics of Cognition (happening in 10 days) to see if there is any research linking active matter and neuroscience. Although I couldn’t find one specifically about active matter, I found one that connects with non-equilibrium statistical physics:\n\n\n\nStatistical Physics of Cognition - Programme\n\n\nIt appears that the researcher has written several papers on this topic with William Bialek, who authored a seminal paper on applying the maximum entropy principle to elucidate collective behaviour in retinal neuronal networks (see Schneidman et al. (2006)), which inspired my MSci research project.\n\nSchneidman, Elad, Michael J Berry, Ronen Segev, and William Bialek. 2006. “Weak Pairwise Correlations Imply Strongly Correlated Network States in a Neural Population.” Nature 440 (7087): 1007–12.\n\n———. 2022b. “Emergence of Local Irreversibility in Complex Interacting Systems.” Physical Review E 106 (3): 034102.\nThe abstract of the first paper (Lynn et al. (2022b)):\n\nLiving systems are fundamentally irreversible, breaking detailed balance and establishing an arrow of time. But how does the evident arrow of time for a whole system arise from the interactions among its multiple elements? We show that the local evidence for the arrow of time, which is the entropy production for thermodynamic systems, can be decomposed. First, it can be split into two components: an independent term reflecting the dynamics of individual elements and an interaction term driven by the dependencies among elements. Adapting tools from nonequilibrium physics, we further decompose the interaction term into contributions from pairs of elements, triplets, and higher-order terms. We illustrate our methods on models of cellular sensing and logical computations, as well as on patterns of neural activity in the retina as it responds to visual inputs. We find that neural activity can define the arrow of time even when the visual inputs do not, and that the dominant contribution to this breaking of detailed balance comes from interactions among pairs of neurons.\n\nThe abstract of the second paper (Lynn et al. (2022a)):\n\nLynn, Christopher W, Caroline M Holmes, William Bialek, and David J Schwab. 2022a. “Decomposing the Local Arrow of Time in Interacting Systems.” Physical Review Letters 129 (11): 118101.\n\nWe show that the evidence for a local arrow of time, which is equivalent to the entropy production in thermodynamic systems, can be decomposed. In a system with many degrees of freedom, there is a term that arises from the irreversible dynamics of the individual variables, and then a series of non-negative terms contributed by correlations among pairs, triplets, and higher-order combinations of variables. We illustrate this decomposition on simple models of noisy logical computations, and then apply it to the analysis of patterns of neural activity in the retina as it responds to complex dynamic visual scenes. We find that neural activity breaks detailed balance even when the visual inputs do not, and that this irreversibility arises primarily from interactions between pairs of neurons.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "reading.html#section",
    "href": "reading.html#section",
    "title": "List of My Reading",
    "section": "2024/11/14 -",
    "text": "2024/11/14 -\nSkimming through many papers since I forgot to charge my iPad in my office… I think this is productive ahead of my Monday project meeting next week.\nWetting Transition of Active Brownian Particles on a Thin Membrane (Turci and Wilding 2021)\nTurci, Francesco, and Nigel B Wilding. 2021. “Wetting Transition of Active Brownian Particles on a Thin Membrane.” Physical Review Letters 127 (23): 238002.\n\nCited by 20\nUnfortunately, not very interesting. The authors (my supervisors) investigated how ABPs interact with a planar surface with barrier strength \\(\\epsilon_w\\); decreasing this makes the surface resemble a thin penetrable (or porous) membrane. The research implications would relate to artificial colloids or elementary swarm robots, but I’m not particularly interested in those systems.\n\n\n\nOn page 2, Partial and complete wetting of droplets of active Brownian particles\n\n\nThe above image is a good summary of what the research is about, which coincidentally is from what I’m currently reading:\nPartial and Complete Wetting of Droplets of Active Brownian Particles (Turci, Jack, and Wilding 2024)\nTurci, Francesco, Robert L Jack, and Nigel B Wilding. 2024. “Partial and Complete Wetting of Droplets of Active Brownian Particles.” Soft Matter 20 (9): 2060–74.\n\nCited by 5\nIt became quite technical from Chapter 2 onward, so I only read the introduction and the discussion & conclusions sections. I think I’m becoming increasingly uncertain about wanting to pursue my project. It doesn’t seem particularly mysterious or intriguing. I guess the highlight message of this article was that despite the inherent non-equilibrium mechanisms that engender phase separation and non-trivial flow patterns, the large-scale properties of the active model can be mapped onto an equilibrium one, which mean that we can use the techniques from equilibrium stats. mechanics such as renormalisation group which I was hoping to learn deeply in my PhD. If there were a connection between surface phase transitions of vapour inside the cap of my RDA (rebuildable dripping atomiser), it might be more interesting, but I think this is likely an equilibrium condition rather than something relevant to active matter. I must ask next week.\nMotility-Induced Phase Separation (Cates and Tailleur 2015)\nCates, Michael E, and Julien Tailleur. 2015. “Motility-Induced Phase Separation.” Annu. Rev. Condens. Matter Phys. 6 (1): 219–44.\n\nCited by 1579 (Really surprising of the attention of this topics!)\nThis one also, I only read the introduction and conclusion. I wanted to read this because MIPS keeps coming back in all papers. Essentialy, that phenomenon happens because active particles generically accumulate where they move more slowly. What’s really worth noting is that run and tumple model is inspired by e-coli and active brownian particles are inspired by synthetic colloids. Am I interested in both systems? No… I think I should have done this activity of skimming a lot of papers to see what I’m really interested in.\nProliferating active matter (Hallatschek et al. 2023)\nHallatschek, Oskar, Sujit S Datta, Knut Drescher, Jörn Dunkel, Jens Elgeti, Bartek Waclaw, and Ned S Wingreen. 2023. “Proliferating Active Matter.” Nature Reviews Physics 5 (7): 407–19.\n\nCited by 48\nI thought I should probably read papers which inspired me to investigate active matter in PhD almost about a year ago once more. One particular article, even though it didn’t seem popular to gain attractions, was written by Keller: ‘Active matter, then and now’ (Keller 2016). I loved reading it so much so I probably read like three times. Skipping that one this time, I now started to read the one with the title above: ‘Proliferating active matter’ which I also really enjoyed reading. I remember the start of the article was quite strong, mentioning the book that I often reread, ‘What is life?’ by Erwin Schrodinger, and also mentioning about the reshaping the chess board by active matter paradigm (which was probably something to do with breaking the mass reversal symmetry?) as if I play the physics game like a chess.\n\n\n\nKeller, Evelyn Fox. 2016. “Active Matter, Then and Now.” History and Philosophy of the Life Sciences 38: 1–11.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#skimming-through-papers",
    "href": "reading.html#skimming-through-papers",
    "title": "List of My Reading",
    "section": "2024/11/14 - Skimming through papers",
    "text": "2024/11/14 - Skimming through papers\nSkimming through many papers since I forgot to charge my iPad in my office… I think this is productive ahead of my Monday project meeting next week.\nWetting Transition of Active Brownian Particles on a Thin Membrane (Turci and Wilding 2021)\nTurci, Francesco, and Nigel B Wilding. 2021. “Wetting Transition of Active Brownian Particles on a Thin Membrane.” Physical Review Letters 127 (23): 238002.\n\nCited by 20\nUnfortunately, not very interesting. The authors (my supervisors) investigated how ABPs interact with a planar surface with barrier strength \\(\\epsilon_w\\); decreasing this makes the surface resemble a thin penetrable (or porous) membrane. The research implications relate to artificial colloids or elementary swarm robots, but I’m not particularly interested in those systems.\n\n\n\nOn page 2, Partial and complete wetting of droplets of active Brownian particles\n\n\nThe above image is a good summary of what the research is about, which coincidentally is from what I’m currently reading:\n\nPartial and Complete Wetting of Droplets of Active Brownian Particles (Turci, Jack, and Wilding 2024)\nTurci, Francesco, Robert L Jack, and Nigel B Wilding. 2024. “Partial and Complete Wetting of Droplets of Active Brownian Particles.” Soft Matter 20 (9): 2060–74.\n\nCited by 5\nIt became quite technical from Chapter 2 onward, so I only read the introduction and the discussion & conclusions sections. I think I’m becoming increasingly uncertain about wanting to pursue my project. It doesn’t seem particularly mysterious or evoking my curiosity. I guess the highlight of this article was that, despite the inherent non-equilibrium mechanisms that engender phase separation and non-trivial flow patterns, the large-scale properties of the active model can be mapped onto an equilibrium one. This means that we can use techniques from equilibrium statistical mechanics, such as the renormalisation group, which I was hoping to learn deeply in my PhD.\nIf there were a connection between surface phase transitions of vapour inside the cap of my RDA (rebuildable dripping atomiser), it might be more interesting and relating to my interests, but I think this is likely an equilibrium condition rather than something relevant to active matter. I must ask next week.\n\nMotility-Induced Phase Separation (Cates and Tailleur 2015)\nCates, Michael E, and Julien Tailleur. 2015. “Motility-Induced Phase Separation.” Annu. Rev. Condens. Matter Phys. 6 (1): 219–44.\n\nCited by 1579 (Really surprising how much attention this topic has received!)\nThis one, too, I only read the introduction and conclusion. I wanted to read this because MIPS keeps appearing in many papers. Essentially, this phenomenon occurs because active particles generically accumulate where they move more slowly. What’s really worth noting is that the run-and-tumble model is inspired by E. coli, while active Brownian particles are inspired by synthetic colloids. Am I interested in either system? No… I think I should have done this activity of skimming through many papers earlier to clarify what I’m truly interested in.\n\nProliferating Active Matter (Hallatschek et al. 2023)\nHallatschek, Oskar, Sujit S Datta, Knut Drescher, Jörn Dunkel, Jens Elgeti, Bartek Waclaw, and Ned S Wingreen. 2023. “Proliferating Active Matter.” Nature Reviews Physics 5 (7): 407–19.\n\nCited by 48\nI thought I should probably reread papers that inspired me to investigate active matter in my PhD almost a year ago. One particular article, even though it didn’t seem to attract much attention, was written by Keller: ‘Active Matter, Then and Now’ (Keller 2016). I loved reading it so much that I probably read it three times. Skipping that one this time, I started reading Proliferating Active Matter, which I also really enjoyed.\n\nKeller, Evelyn Fox. 2016. “Active Matter, Then and Now.” History and Philosophy of the Life Sciences 38: 1–11.\nI remember the start of the article being quite strong, mentioning the book that I often reread, What Is Life? by Erwin Schrödinger, and also discussing the reshaping of the chessboard by the active matter paradigm (which was probably something to do with breaking mass-reversal symmetry?)—as if I were playing the physics game like chess.\nWhile reading (just before the chapter Motile Proliferating Matter), I got sidetracked because it wasn’t really interesting anymore. It seems my interests have shifted over the summer or possibly during the final stages of my MSci research project. It sort of makes sense when I think about it in terms of the timeline.\n\nIn January 2024, when I was applying for my PhD {#sec-applying-phd}:\n\nI wanted to pursue new physics. It appeared to me that two of my favourite fields (general relativity and quantum field theory) were already well-explored and largely complete.\nI also wanted to gain more employability skills, as I hoped to work at Google DeepMind after my PhD. I thought a computationally driven project was essential for this.\nEager to formally learn about the renormalisation group, I looked into areas of new physics related to it. That’s when I discovered the emerging field of active matter. After reading Keller’s account of its history and emergence, as well as McGivern’s account of how it relates to cognition (which I should probably revisit tomorrow), I thought, this is it! This was the field I wanted to pursue! I met Nigel, who was such a brilliant and thoughtful person to talk with, and Francesco, who had already been amazing. I applied directly (and, in fact, only applied for this PhD, while my peers were applying for multiple positions).\nIn my second term, I was also in the process of interviewing with Samsung Electronics, as they invited me to take on a data scientist role (this happened in early January, I think?). The role and salary were very attractive. However, once I received the final offer, I had to think deeply about what I truly wanted to do. There were still unsolved mysteries I hoped to explore during my PhD. I was curious about fundamental questions like: What is time? What is space? I thought about relativity theory and how Einstein built his ideas around the concept of a universal clock, which doesn’t depend on the mechanisms of the clock itself. But these clocks are also within a gravitational field. Can we truly justify that reading a clock on an observer’s wrist is the correct way to measure time, given that all clocks are fundamentally based on electromagnetism?\nSo, I declined the job offer and decided to stay at university for another four years. Over the summer holidays, I continued reading neuroscience books (particularly on dreams and altered states of consciousness) and considered which topics would be interesting to investigate throughout my PhD.\n\nBefore sleeping, I started reading Time and Quantum Clocks: A Review of Recent Developments (Altaie, Hodgson, and Beige 2022), which I found very interesting.\n\nAltaie, M Basil, Daniel Hodgson, and Almut Beige. 2022. “Time and Quantum Clocks: A Review of Recent Developments.” Frontiers in Physics 10: 897305.\n\nActive Materials: Minimal Models of Cognition? (McGivern 2020)\nMcGivern, Patrick. 2020. “Active Materials: Minimal Models of Cognition?” Adaptive Behavior 28 (6): 441–51.\n\nCited by 20\nThe next morning, I felt compelled to reread this paper, as it was one of the key works that truly ignited my interest in the field of active matter.\n\n\n\nOn page 8, 3. Minimal models\n\n\nYes! This was it! I was curious whether a simple model of active matter that I learn could be applied to biological systems by examining if they fall into the same universality class as a realistic model through the application of renormalisation group techniques.\nVery interesting. I’m glad I read it once again. I wonder what my supervisors think about this article. I thought it would be useful to clarify whether I truly want to pursue the field of biophysics (as I’m not particularly interested in synthetic colloids), so I decided to revisit the biophysics lectures (see 2024/11/16 Watching Biophysics Lectures).",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#sec-skimming-papers",
    "href": "reading.html#sec-skimming-papers",
    "title": "List of My Reading",
    "section": "2024/11/14 - Skimming through papers",
    "text": "2024/11/14 - Skimming through papers\nSkimming through many papers since I forgot to charge my iPad in my office… I think this is productive ahead of my Monday project meeting next week.\nWetting Transition of Active Brownian Particles on a Thin Membrane (Turci and Wilding 2021)\nTurci, Francesco, and Nigel B Wilding. 2021. “Wetting Transition of Active Brownian Particles on a Thin Membrane.” Physical Review Letters 127 (23): 238002.\n\nCited by 20\nUnfortunately, not very interesting. The authors (my supervisors) investigated how ABPs interact with a planar surface with barrier strength \\(\\epsilon_w\\); decreasing this makes the surface resemble a thin penetrable (or porous) membrane. The research implications relate to artificial colloids or elementary swarm robots, but I’m not particularly interested in those systems.\n\n\n\nOn page 2, Partial and complete wetting of droplets of active Brownian particles\n\n\nThe above image is a good summary of what the research is about, which coincidentally is from what I’m currently reading:\n\nPartial and Complete Wetting of Droplets of Active Brownian Particles (Turci, Jack, and Wilding 2024)\nTurci, Francesco, Robert L Jack, and Nigel B Wilding. 2024. “Partial and Complete Wetting of Droplets of Active Brownian Particles.” Soft Matter 20 (9): 2060–74.\n\nCited by 5\nIt became quite technical from Chapter 2 onward, so I only read the introduction and the discussion & conclusions sections. I think I’m becoming increasingly uncertain about wanting to pursue my project. It doesn’t seem particularly mysterious or evoking my curiosity. I guess the highlight of this article was that, despite the inherent non-equilibrium mechanisms that engender phase separation and non-trivial flow patterns, the large-scale properties of the active model can be mapped onto an equilibrium one. This means that we can use techniques from equilibrium statistical mechanics, such as the renormalisation group, which I was hoping to learn deeply in my PhD.\nIf there were a connection between surface phase transitions of vapour inside the cap of my RDA (rebuildable dripping atomiser), it might be more interesting and relating to my interests, but I think this is likely an equilibrium condition rather than something relevant to active matter. I must ask next week.\n\nMotility-Induced Phase Separation (Cates and Tailleur 2015)\nCates, Michael E, and Julien Tailleur. 2015. “Motility-Induced Phase Separation.” Annu. Rev. Condens. Matter Phys. 6 (1): 219–44.\n\nCited by 1579 (Really surprising how much attention this topic has received!)\nThis one, too, I only read the introduction and conclusion. I wanted to read this because MIPS keeps appearing in many papers. Essentially, this phenomenon occurs because active particles generically accumulate where they move more slowly. What’s really worth noting is that the run-and-tumble model is inspired by E. coli, while active Brownian particles are inspired by synthetic colloids. Am I interested in either system? No… I think I should have done this activity of skimming through many papers earlier to clarify what I’m truly interested in.\n\nProliferating Active Matter (Hallatschek et al. 2023)\nHallatschek, Oskar, Sujit S Datta, Knut Drescher, Jörn Dunkel, Jens Elgeti, Bartek Waclaw, and Ned S Wingreen. 2023. “Proliferating Active Matter.” Nature Reviews Physics 5 (7): 407–19.\n\nCited by 48\nI thought I should probably reread papers that inspired me to investigate active matter in my PhD almost a year ago. One particular article, even though it didn’t seem to attract much attention, was written by Keller: ‘Active Matter, Then and Now’ (Keller 2016). I loved reading it so much that I probably read it three times. Skipping that one this time, I started reading Proliferating Active Matter, which I also really enjoyed.\n\nKeller, Evelyn Fox. 2016. “Active Matter, Then and Now.” History and Philosophy of the Life Sciences 38: 1–11.\nI remember the start of the article being quite strong, mentioning the book that I often reread, What Is Life? by Erwin Schrödinger, and also discussing the reshaping of the chessboard by the active matter paradigm (which was probably something to do with breaking mass-reversal symmetry?)—as if I were playing the physics game like chess.\nWhile reading (just before the chapter Motile Proliferating Matter), I got sidetracked because it wasn’t really interesting anymore. It seems my interests have shifted over the summer or possibly during the final stages of my MSci research project. It sort of makes sense when I think about it in terms of the timeline.\nIn January 2024, when I was applying for my PhD:\n\nI wanted to pursue new physics. It appeared to me that two of my favourite fields (general relativity and quantum field theory) were already well-explored and largely complete.\nI also wanted to gain more employability skills, as I hoped to work at Google DeepMind after my PhD. I thought a computationally driven project was essential for this.\nEager to formally learn about the renormalisation group, I looked into areas of new physics related to it. That’s when I discovered the emerging field of active matter. After reading Keller’s account of its history and emergence, as well as McGivern’s account of how it relates to cognition (which I should probably revisit tomorrow), I thought, this is it! This was the field I wanted to pursue! I met Nigel, who was such a brilliant and thoughtful person to talk with, and Francesco, who had already been amazing. I applied directly (and, in fact, only applied for this PhD, while my peers were applying for multiple positions).\nIn my second term, I was also in the process of interviewing with Samsung Electronics, as they invited me to take on a data scientist role (this happened in early January, I think?). The role and salary were very attractive. However, once I received the final offer, I had to think deeply about what I truly wanted to do. There were still unsolved mysteries I hoped to explore during my PhD. I was curious about fundamental questions like: What is time? What is space? I thought about relativity theory and how Einstein built his ideas around the concept of a universal clock, which doesn’t depend on the mechanisms of the clock itself. But these clocks are also within a gravitational field. Can we truly justify that reading a clock on an observer’s wrist is the correct way to measure time, given that all clocks are fundamentally based on electromagnetism?\nSo, I declined the job offer and decided to stay at university for another four years. Over the summer holidays, I continued reading neuroscience books (particularly on dreams and altered states of consciousness) and considered which topics would be interesting to investigate throughout my PhD.\n\nBefore sleeping, I started reading Time and Quantum Clocks: A Review of Recent Developments (Altaie, Hodgson, and Beige 2022), which I found very interesting.\n\nAltaie, M Basil, Daniel Hodgson, and Almut Beige. 2022. “Time and Quantum Clocks: A Review of Recent Developments.” Frontiers in Physics 10: 897305.\n\nActive Materials: Minimal Models of Cognition? (McGivern 2020)\nMcGivern, Patrick. 2020. “Active Materials: Minimal Models of Cognition?” Adaptive Behavior 28 (6): 441–51.\n\nCited by 20\nThe next morning, I felt compelled to reread this paper, as it was one of the key works that truly ignited my interest in the field of active matter.\n\n\n\nOn page 8, 3. Minimal models\n\n\nYes! This was it! I was curious whether a simple model of active matter that I learn could be applied to biological systems by examining if they fall into the same universality class as a realistic model through the application of renormalisation group techniques.\nVery interesting. I’m glad I read it once again. I wonder what my supervisors think about this article. I thought it would be useful to clarify whether I truly want to pursue the field of biophysics (as I’m not particularly interested in synthetic colloids), so I decided to revisit the biophysics lectures (see 2024/11/16 Watching Biophysics Lectures).",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  }
]