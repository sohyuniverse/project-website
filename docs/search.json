[
  {
    "objectID": "reading.html",
    "href": "reading.html",
    "title": "List of My Reading",
    "section": "",
    "text": "By James Ladyman and Karoline Wiesner\nMy final-year MSci project involved the study of a complex system, foreign exchange markets. Each individual currency was treated as a spin in the Ising model, and by applying E.T. Jaynes’s principle of maximum entropy, we discovered the structure of interactions between these entities. Remarkably, this simple model of magnetism can also describe the phase transitions of liquid-gas systems, as they fall into the same universality class. This model has even been extended to study how critical our brain is, giving rise to the field of the critical brain hypothesis. Therefore, many ideas during my research project were derived from neuroscience papers. The financial market and the brain: both are complex systems.\nAbout a year ago, I read a paper published by James Ladyman, a professor of philosophy of science at Bristol, titled ‘What is a Complex System?’ (Ladyman, Lambert, and Wiesner 2013). As I started my PhD at Bristol, my second supervisor, Francesco, mentioned that he also published a book about complexity. It became clear to me that I should read this book.\n\nLadyman, James, James Lambert, and Karoline Wiesner. 2013. “What Is a Complex System?” European Journal for Philosophy of Science 3: 33–67.\nChapter 1 presents The Truisms of Complexity Science as follows:\n1. More is different.\n2. Nonliving systems can generate order.\n3. Complexity can come from simplicity.\n4. Coordinated behaviour does not require an overall controller.\n5. Complex systems are often modelled as networks or information processing systems.\n6. There are various kinds of invariance and forms of universal behaviour in complex systems.\n7. Complexity science is computational and probabilistic.\n8. Complexity science involves multiple disciplines.\n9. There is a difference between the order that complex systems produce and the order of the complex systems themselves.\nFeatures that are necessary and sufficient for which kinds of complexity and complex system are as follows:\n1. Numerosity: complex systems involve many interactions among many components.\n2. Disorder and diversity: the interactions in a complex system are not coordinated or controlled centrally, and the components may differ.\n3. Feedback: the interactions in complex systems are iterated so that there is feedback from previous interactions on a time scale relevant to the system’s emergent dynamics.\n4. Non-equilibrium: complex systems are open to the environment and are often driven by something external.\n5. Spontaneous order and self-organisation: complex systems exhibit structure and order that arises out of the interactions among their parts.\n6. Nonlinearity: complex systems exhibit nonlinear dependence on parameters or external drivers.\n7. Robustness: the structure and function of complex systems is stable under relevant perturbations.\n8. Nested structure and modularity: there may be multiple scales of structure, clustering and specialisation of function in complex systems.\n9. History and memory: complex systems often require a very long history to exist and often store information about history.\n10. Adaptive behaviour: complex systems are often able to modify their behaviour depending on the state of the environment and the predictions they make about it.\n\n2024/9/27\nReading What is a Complex System? by James Ladyman and Karoline Wiesner. The purpose of reading is to understand how systems out of thermodynamic equilibrium (a feature of complex systems) relate to a wide range of concepts associated with complexity.\n• There was a video that helped clarify what is meant by a Markov chain and a stochastic process being stationary: Markov Chains Clearly Explained! Part - 1. His other videos are helpful as well. For example, when I saw \\(P_{ij}^{(n)} = A_{ij}^n\\) from the n-Step Transition Matrix video (Part 3), it was quite surprising. However, the Part 5 video on hidden Markov models wasn’t very helpful, but reading the appendix in the book was sufficient.\n• What I’ve felt from reading this book (currently at p.90) is that it presents a lot of different ideas and has interesting discussions on the history of science, but I only seem to get excited when it starts mentioning the brain. I’m not sure if I’m necessarily interested in quantifying complexity. From the Apple notes on 28/9/2024:\nAs I’m reading through What is a Complex System?, it seems that the description of a complex system arises as we have more representations at different scales of analysis. The concept of ‘complexity’ was destined to arise as the scientific era evolved.\nNon-equilibrium systems arise because we define what systems are in thermodynamic equilibrium, which are idealisations.\nThings that fail to be idealised fall into this dualistic concept: non-equilibrium.\n\n2024/10/3\nIt took about two weeks to finish this book. It was an accessible read until I reached chapter 4, which was Measuring Features of Complex Systems. In the end, I’m glad that I read it, even though I somehow felt rushed to finish as there were other responsibilities to manage continuously, such as setting up clusters for my university Mac, teaching, office setup, etc. Most importantly, from now on, I am going to focus on reading material that is directly related to my project since it may give the impression to my supervisors that I’m not working hard. Reading a book like this takes a long time, as there are many new terms I had not encountered before.\nAnyway, it was a good read. Would I buy a physical copy for myself? Maybe not, but it was a good one.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#what-is-a-complex-system",
    "href": "reading.html#what-is-a-complex-system",
    "title": "List of My Reading",
    "section": "",
    "text": "By James Ladyman and Karoline Wiesner\nMy final-year MSci project involved the study of a complex system, foreign exchange markets. Each individual currency was treated as a spin in the Ising model, and by applying E.T. Jaynes’s principle of maximum entropy, we discovered the structure of interactions between these entities. Remarkably, this simple model of magnetism can also describe the phase transitions of liquid-gas systems, as they fall into the same universality class. This model has even been extended to study how critical our brain is, giving rise to the field of the critical brain hypothesis. Therefore, many ideas during my research project were derived from neuroscience papers. The financial market and the brain: both are complex systems.\nAbout a year ago, I read a paper published by James Ladyman, a professor of philosophy of science at Bristol, titled ‘What is a Complex System?’ (Ladyman, Lambert, and Wiesner 2013). As I started my PhD at Bristol, my second supervisor, Francesco, mentioned that he also published a book about complexity. It became clear to me that I should read this book.\n\nLadyman, James, James Lambert, and Karoline Wiesner. 2013. “What Is a Complex System?” European Journal for Philosophy of Science 3: 33–67.\nChapter 1 presents The Truisms of Complexity Science as follows:\n1. More is different.\n2. Nonliving systems can generate order.\n3. Complexity can come from simplicity.\n4. Coordinated behaviour does not require an overall controller.\n5. Complex systems are often modelled as networks or information processing systems.\n6. There are various kinds of invariance and forms of universal behaviour in complex systems.\n7. Complexity science is computational and probabilistic.\n8. Complexity science involves multiple disciplines.\n9. There is a difference between the order that complex systems produce and the order of the complex systems themselves.\nFeatures that are necessary and sufficient for which kinds of complexity and complex system are as follows:\n1. Numerosity: complex systems involve many interactions among many components.\n2. Disorder and diversity: the interactions in a complex system are not coordinated or controlled centrally, and the components may differ.\n3. Feedback: the interactions in complex systems are iterated so that there is feedback from previous interactions on a time scale relevant to the system’s emergent dynamics.\n4. Non-equilibrium: complex systems are open to the environment and are often driven by something external.\n5. Spontaneous order and self-organisation: complex systems exhibit structure and order that arises out of the interactions among their parts.\n6. Nonlinearity: complex systems exhibit nonlinear dependence on parameters or external drivers.\n7. Robustness: the structure and function of complex systems is stable under relevant perturbations.\n8. Nested structure and modularity: there may be multiple scales of structure, clustering and specialisation of function in complex systems.\n9. History and memory: complex systems often require a very long history to exist and often store information about history.\n10. Adaptive behaviour: complex systems are often able to modify their behaviour depending on the state of the environment and the predictions they make about it.\n\n2024/9/27\nReading What is a Complex System? by James Ladyman and Karoline Wiesner. The purpose of reading is to understand how systems out of thermodynamic equilibrium (a feature of complex systems) relate to a wide range of concepts associated with complexity.\n• There was a video that helped clarify what is meant by a Markov chain and a stochastic process being stationary: Markov Chains Clearly Explained! Part - 1. His other videos are helpful as well. For example, when I saw \\(P_{ij}^{(n)} = A_{ij}^n\\) from the n-Step Transition Matrix video (Part 3), it was quite surprising. However, the Part 5 video on hidden Markov models wasn’t very helpful, but reading the appendix in the book was sufficient.\n• What I’ve felt from reading this book (currently at p.90) is that it presents a lot of different ideas and has interesting discussions on the history of science, but I only seem to get excited when it starts mentioning the brain. I’m not sure if I’m necessarily interested in quantifying complexity. From the Apple notes on 28/9/2024:\nAs I’m reading through What is a Complex System?, it seems that the description of a complex system arises as we have more representations at different scales of analysis. The concept of ‘complexity’ was destined to arise as the scientific era evolved.\nNon-equilibrium systems arise because we define what systems are in thermodynamic equilibrium, which are idealisations.\nThings that fail to be idealised fall into this dualistic concept: non-equilibrium.\n\n2024/10/3\nIt took about two weeks to finish this book. It was an accessible read until I reached chapter 4, which was Measuring Features of Complex Systems. In the end, I’m glad that I read it, even though I somehow felt rushed to finish as there were other responsibilities to manage continuously, such as setting up clusters for my university Mac, teaching, office setup, etc. Most importantly, from now on, I am going to focus on reading material that is directly related to my project since it may give the impression to my supervisors that I’m not working hard. Reading a book like this takes a long time, as there are many new terms I had not encountered before.\nAnyway, it was a good read. Would I buy a physical copy for myself? Maybe not, but it was a good one.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#hydrophobicity-across-length-scales-the-role-of-surface-criticality",
    "href": "reading.html#hydrophobicity-across-length-scales-the-role-of-surface-criticality",
    "title": "List of My Reading",
    "section": "2024/10/4 - Hydrophobicity Across Length Scales: The Role of Surface Criticality",
    "text": "2024/10/4 - Hydrophobicity Across Length Scales: The Role of Surface Criticality\nBy Mary Kathryn Coe\n\nQ1. Resolved\n\n\n\nOn page 35, Chapter 2.1\n\n\nI’m not entirely sure why the surface tension is defined in that way, as it’s normally considered force per unit area. So, I asked ChatGPT:\n\n\nClick to reveal the answer!\n\nThe definition of surface tension in this context is derived from the thermodynamic perspective, which connects surface tension to the grand potential difference between a bulk fluid and a system with an interface. The reasoning follows these steps:\n\nGrand Potential Definition: The grand potential, \\(\\Omega\\), is given by: \\[\n\\Omega = U - TS - \\mu N\n\\] This expression is crucial for systems where particle number and volume are allowed to fluctuate, such as liquid-vapour interfaces.\nSurface Tension from Thermodynamics: Surface tension, \\(\\gamma\\), represents the energy cost per unit area to create an interface. In thermodynamic terms, this corresponds to how much the grand potential increases when introducing an interface between coexisting phases (liquid and vapour).\nDifference between Bulk and Interface: The bulk grand potential, \\(\\Omega_b\\), corresponds to a system without an interface (uniform bulk). When an interface is introduced, the grand potential changes, denoted as \\(\\Omega_{\\text{ex}}\\), the excess grand potential: \\[\n\\Omega_{\\text{ex}} = \\Omega - \\Omega_b\n\\]\nSurface Tension as Excess Grand Potential per Unit Area: The surface tension is then defined as the excess grand potential per unit area, \\(A\\), required to form the interface: \\[\n\\gamma = \\frac{\\Omega_{\\text{ex}}}{A} = \\frac{\\Omega - \\Omega_b}{A}\n\\] This formulation makes sense because surface tension describes the energetic cost to create a unit area of interface between two phases.\n\nIn summary, while surface tension is often discussed in terms of force per unit length in simpler mechanical contexts, in thermodynamics, it is expressed as the excess grand potential per unit area. This ties into how interfaces modify the system’s energy in comparison to the bulk. The two definitions are equivalent but applied in different contexts: mechanical (force/length) vs. thermodynamic (energy/area).\n\n\n\nDerivation of equation 2.1.8, 2.1.9, 2.1.10\n\n\\[\n\\Omega_b(\\mu, V, T) = U - TS - \\mu N = -pV\n\\]\nTaking the differential:\n\\[\nd\\Omega_b = d(U - TS - \\mu N)\n\\]\nUsing the first law of thermodynamics:\n\\[\ndU = TdS - pdV + \\mu dN\n\\]\nDifferentiating \\(TS\\):\n\\[\nd(TS) = TdS + SdT\n\\]\nDifferentiating \\(\\mu N\\):\n\\[\nd(\\mu N) = \\mu dN + N d\\mu\n\\]\nSubstituting these into the differential for \\(\\Omega_b\\):\n\\[\nd\\Omega_b = (TdS - pdV + \\mu dN) - (TdS + SdT) - (\\mu dN + N d\\mu)\n\\]\nSimplifying:\n\\[\nd\\Omega_b = -S_b dT - N_b d\\mu - p dV\n\\]\nFor constant volume (\\(dV = 0\\)), the volume is assumed to remain constant because we are focusing on surface effects rather than bulk volume changes.:\n\\[\nd\\Omega_b = -S_b dT - N_b d\\mu\n\\]\nNow for equation (2.1.6):\n\\[\n\\Omega(\\mu, V, T) = U - TS - \\mu N = -pV + \\gamma A\n\\]\nTaking the differential:\n\\[\nd\\Omega = d(U - TS - \\mu N + \\gamma A)\n\\]\nSubstituting differentials:\n\\[\nd\\Omega = TdS - pdV + \\mu dN - (TdS + SdT) - (\\mu dN + N d\\mu) + \\gamma dA + A d\\gamma\n\\]\nSimplifying:\n\\[\nd\\Omega = -S dT - N d\\mu + \\gamma dA\n\\]\nNow subtract:\n\\[\nd\\Omega - d\\Omega_b = (-S dT - N d\\mu + \\gamma dA) - (-S_b dT - N_b d\\mu)\n\\]\nSimplifying:\n\\[\nd\\Omega - d\\Omega_b = -(S - S_b)dT - (N - N_b)d\\mu + \\gamma dA\n\\]\nThis is equation (2.1.8).\n\nFrom equation (2.1.7):\n\\[\n\\gamma = \\frac{\\Omega_{\\text{ex}}}{A} = \\frac{\\Omega - \\Omega_b}{A}\n\\]\nTaking the differential:\n\\[\nd\\gamma = \\frac{d\\Omega_{\\text{ex}}}{A} = \\frac{d\\Omega - d\\Omega_b}{A}\n\\]\nNow, use the result from equation (2.1.8):\n\\[\nd\\Omega - d\\Omega_b = -(S - S_b) dT - (N - N_b) d\\mu + \\gamma dA\n\\]\nSo:\n\\[\nd\\Omega_{\\text{ex}} = -s_{\\text{ex}} A dT - \\Gamma d\\mu + \\gamma dA\n\\]\nwhere:\n\n\\(s_{\\text{ex}} = \\frac{S - S_b}{A}\\) is the surface excess entropy per unit area,\n\\(\\Gamma = N - N_b\\) is the excess number of particles (adsorption).\n\nDividing through by \\(A\\), we get:\n\\[\n\\frac{d\\Omega_{\\text{ex}}}{A} = -s_{\\text{ex}} dT - \\frac{\\Gamma}{A} d\\mu + \\gamma \\frac{dA}{A}\n\\]\nNow, using the fact that \\(\\gamma \\frac{dA}{A}\\) is small for constant area, we can ignore this term, giving:\n\\[\ns_{\\text{ex}} dT + \\frac{\\Gamma}{A} d\\mu + d\\gamma = 0\n\\]\nThis is equation (2.1.9).\n\nRearrange equation (2.1.9):\n\\[\n\\frac{\\Gamma}{A} d\\mu = -s_{\\text{ex}} dT - d\\gamma\n\\]\nAt constant temperature (\\(dT = 0\\)), this simplifies to:\n\\[\n\\frac{\\Gamma}{A} d\\mu = -d\\gamma\n\\]\nNow, dividing both sides by \\(d\\mu\\), we get:\n\\[\n\\frac{\\Gamma}{A} = -\\left(\\frac{\\partial \\gamma}{\\partial \\mu}\\right)_T\n\\]\nThis is equation (2.1.10).\n\nAnd I need to ask about equations 2.1.11 to 2.1.13, as I cannot find reference [40].\n\n\nQ2. Resolved\n\n\n\nOn page 40, Chapter 2.3\n\n\nI wasn’t sure what the one-particle density is or its use, so I asked ChatGPT:\n\n\nClick to reveal the answer!\n\nThe one-particle density function, often denoted as \\(\\rho^{(1)}(\\mathbf{r})\\), represents the probability density of finding a particle at a specific position \\(\\mathbf{r}\\) in a system of particles. It describes how particles are distributed in space on average and is a key observable in statistical mechanics and the study of fluids.\nThe equation given is:\n\\[\n\\hat{\\rho}^{(1)}(\\mathbf{r}) = \\sum_{j=1}^{N} \\delta(\\mathbf{r} - \\mathbf{r}_j)\n\\]\nThis is the one-particle density operator, and it has the following meaning:\n\nDirac Delta Function: \\(\\delta(\\mathbf{r} - \\mathbf{r}_j)\\) is a Dirac delta function that ‘picks out’ the position of the \\(j\\)-th particle. It is equal to zero everywhere except at \\(\\mathbf{r}_j\\), where it is infinite, ensuring that it integrates to 1. Thus, it counts the presence of a particle at \\(\\mathbf{r}_j\\).\nSummation over All Particles: The sum runs over all particles \\(j\\) from 1 to \\(N\\), so this operator adds up the contributions from all particles in the system, effectively counting the number of particles at position \\(\\mathbf{r}\\).\n\nIn essence, the one-particle density operator \\(\\hat{\\rho}^{(1)}(\\mathbf{r})\\) gives the total number of particles at position \\(\\mathbf{r}\\) in the system. Since it is an operator, the actual one-particle density \\(\\langle \\hat{\\rho}^{(1)}(\\mathbf{r}) \\rangle\\), as defined in equation (2.3.2), is obtained by averaging over all possible configurations of the system in the given ensemble (such as the grand canonical ensemble).\nWhy Does It Look Like This?\nThe form of equation (2.3.1) stems from how we represent the microscopic distribution of particles in space. The Dirac delta function ensures that for each particle, its position contributes to the overall density only at the exact location where that particle exists. This is a standard way to construct density operators in statistical mechanics because:\n\nThe delta function precisely pinpoints the position of a particle.\nSumming over all particles ensures that the density operator accounts for the total number of particles present in the system.\n\nThis representation allows us to describe spatially inhomogeneous systems where the density may vary across different regions (e.g., near a surface or an external potential). For homogeneous fluids, where particles are evenly distributed, this density would reduce to a constant value (the bulk density \\(\\rho_b\\)).\nSummary\nThe one-particle density operator, \\(\\hat{\\rho}^{(1)}(\\mathbf{r})\\), reflects how particles are distributed in space at a microscopic level. The delta functions track the positions of individual particles, and summing over all particles gives the total density at position \\(\\mathbf{r}\\). The actual one-particle density function \\(\\rho^{(1)}(\\mathbf{r})\\) is then the ensemble average of this operator over all configurations of the system.\n\n\n\nQ3. Resolved\n\n\n\nOn page 43, Chapter 2.3\n\n\nI wasn’t sure why each variable is specifically chosen to form those equations. For example, in section 2.4.3, I do not understand why λ can be factored out of g on the right-hand side when λ has two different exponents, a and b. So, I asked ChatGPT:\n\n\nClick to reveal the answer!\n\nStarting with Equation (2.4.3):\n\\[\ng(\\lambda^a u, \\lambda^b v) = \\lambda g(u, v)\n\\]\nThis equation states that the function \\(g(u, v)\\) is a generalised homogeneous function. To understand why it takes this form and why \\(\\lambda\\) appears outside the function on the right-hand side, let’s break this down:\n\nHomogeneous Functions: A function is called homogeneous of degree \\(\\alpha\\) if, when all its arguments are scaled by a factor \\(\\lambda\\), the function itself scales by \\(\\lambda^\\alpha\\). Mathematically, for a homogeneous function \\(f(x, y)\\), we would have: \\[\nf(\\lambda x, \\lambda y) = \\lambda^\\alpha f(x, y)\n\\] This property is common in critical phenomena where we are dealing with power-law behaviour near critical points.\nGeneralised Homogeneous Functions: The function \\(g(u, v)\\) is a generalised homogeneous function because it allows different exponents for different variables. In this case, the variables \\(u\\) and \\(v\\) scale with different exponents \\(a\\) and \\(b\\) under the same scaling factor \\(\\lambda\\).\n\n\\(\\lambda^a u\\): Here, \\(u\\) is scaled by \\(\\lambda^a\\). This implies that \\(u\\) behaves differently under scaling compared to \\(v\\).\n\\(\\lambda^b v\\): Similarly, \\(v\\) is scaled by \\(\\lambda^b\\).\n\nWhy \\(\\lambda\\) can be factored out: The key to understanding why \\(\\lambda\\) can be factored out on the right-hand side despite having different exponents for \\(u\\) and \\(v\\) lies in the definition of a generalised homogeneous function. By construction, we impose that when the variables are scaled in a particular way (with exponents \\(a\\) and \\(b\\)), the function scales by \\(\\lambda\\) itself. This is a property we assign to \\(g(u, v)\\). It doesn’t mean that \\(\\lambda\\) is constant; rather, the entire function \\(g(u, v)\\) is designed to respond to the scaling in this specific manner.\nThe exponents \\(a\\) and \\(b\\) control how \\(u\\) and \\(v\\) individually respond to scaling, but the overall response of the function is to scale as \\(\\lambda\\). The scaling of the function on the right-hand side doesn’t contradict the fact that \\(\\lambda\\) has different exponents inside the arguments. This is simply a mathematical constraint on how the function \\(g(u, v)\\) behaves under scaling.\nIn other words, the form: \\[\ng(\\lambda^a u, \\lambda^b v)\n\\] ensures that the function scales uniformly by \\(\\lambda\\), despite the individual variables scaling differently.\n\nEquation (2.4.4):\nNow, let’s explain how this scaling idea applies to the singular part of the free energy in equation (2.4.4):\n\\[\n\\omega_{\\text{sing}}(\\delta \\mu, t) = \\lambda^{-d} \\omega_{\\text{sing}}(\\lambda^a \\delta \\mu, \\lambda^b t)\n\\]\n\nVariables:\n\n\\(\\delta \\mu = \\mu - \\mu_{\\text{co}}\\) is the difference between the chemical potential \\(\\mu\\) and the critical chemical potential \\(\\mu_{\\text{co}}\\).\n\\(t\\) is the reduced temperature: \\(t = \\frac{T - T_c}{T_c}\\).\n\\(d\\) is the dimensionality of the system (important for scaling relations).\n\nApplying the Scaling Relation: Here, the function \\(\\omega_{\\text{sing}}\\), which represents the singular part of the free energy, is assumed to be a generalised homogeneous function. This means it should follow a scaling law similar to the one introduced in equation (2.4.3).\nThe variables \\(\\delta \\mu\\) and \\(t\\) are scaled by \\(\\lambda^a\\) and \\(\\lambda^b\\), respectively, where \\(a\\) and \\(b\\) are scaling exponents that determine how \\(\\delta \\mu\\) and \\(t\\) change when scaled by \\(\\lambda\\).\nWhy \\(\\lambda^{-d}\\)?\n\nThe factor \\(\\lambda^{-d}\\) appears because we are dealing with free energy, which is an extensive quantity. In thermodynamics, extensive quantities scale with the volume of the system, and in \\(d\\)-dimensional space, the volume scales as \\(\\lambda^d\\). Since we are dealing with the singular part of the free energy, the scaling reflects the dimensionality of the system. The minus sign arises because, under scaling, the free energy decreases as the system grows larger.\n\nThe Free Choice of \\(\\lambda\\): The scaling factor \\(\\lambda\\) is arbitrary, meaning you can choose its value to simplify the equation. In this case, we choose \\(\\lambda^b = t^{-1}\\), which simplifies the second argument (the temperature dependence) of \\(\\omega_{\\text{sing}}\\). By doing this, we reduce the complexity of the equation and make the temperature scaling explicit.\n\nEquation (2.4.5):\nThis equation shows the final form after choosing \\(\\lambda^b = t^{-1}\\):\n\\[\n\\omega_{\\text{sing}}(\\delta \\mu, t) = t^{d/b} \\omega_{\\text{sing}}(t^{-a/b} \\delta \\mu, 1)\n\\]\n\nWhy this form? By setting \\(\\lambda^b = t^{-1}\\), we have scaled out the temperature dependence in the second argument of \\(\\omega_{\\text{sing}}\\). This isolates the temperature dependence into the prefactor \\(t^{d/b}\\), while leaving a scaling function that depends only on the ratio \\(\\delta \\mu / t^{a/b}\\), which is now the only scaling variable.\n\n\\(t^{d/b}\\): This represents how the singular part of the free energy depends on temperature near the critical point. The exponent \\(d/b\\) dictates how strongly the free energy diverges as \\(t \\to 0\\) (the critical point).\n\\(\\Sigma\\left( \\frac{\\delta \\mu}{t^{a/b}} \\right)\\): The function \\(\\Sigma\\) is the scaling function, which encodes the remaining dependence on the scaled chemical potential \\(\\delta \\mu / t^{a/b}\\).\n\n\nIn Summary: - In equation (2.4.3), the exponents \\(a\\) and \\(b\\) control how the variables \\(u\\) and \\(v\\) scale. Despite their different scaling behaviours, the overall function scales by \\(\\lambda\\) because it is designed as a generalised homogeneous function. - In equation (2.4.4), the singular part of the free energy near the critical point follows a similar scaling law, with the dimensionality \\(d\\) appearing as a factor to reflect the extensive nature of free energy. - In equation (2.4.5), we choose \\(\\lambda^b = t^{-1}\\) to simplify the scaling form, making the dependence on temperature explicit and reducing the complexity of the equation. The resulting expression separates the prefactor \\(t^{d/b}\\) and leaves a scaling function that depends only on the ratio \\(\\delta \\mu / t^{a/b}\\), which captures the critical behaviour.\n\n\n\nFluctuations? One correlated entity?\n\nIn the context of phase transitions, particularly near a critical point, the term “fluctuations” refers to fluctuations in the local density of the system. Let’s explain in detail what this means, especially in the context of critical opalescence and long-range correlations.\n\nDensity Fluctuations: When we talk about “density” in this context, we mean the number density of particles in a given volume. In simple terms, it’s how densely packed the particles are in a particular region. The density can fluctuate due to thermal motion, and these fluctuations are typically small and localised in non-critical conditions.\n\nHowever, as a system approaches the critical point of a phase transition (e.g., a liquid-vapour transition), the magnitude of these fluctuations grows dramatically.\n\nWhat happens near the critical point?\n\nCorrelation Length (\\(\\xi\\)): The correlation length \\(\\xi\\) represents the distance over which particles in the system are correlated or “communicate” with each other. Normally, this length is finite, and beyond this distance, the properties of the system in one region do not affect another region.\nNear the critical point, however, \\(\\xi \\to \\infty\\). This means that particles separated by very large distances (even macroscopic scales) are correlated with each other. The entire system essentially becomes one correlated entity.\nDiverging Fluctuations: As the correlation length grows, the fluctuations in the local density (i.e., how many particles are in a given region at any time) also grow larger. This happens because, when the correlation length is large, particles across large distances influence each other. As a result, fluctuations occur over all length scales, from small to very large.\nDensity Fluctuations: Specifically, these are fluctuations in the number of particles in a given volume. Imagine regions in a fluid where the particle density becomes temporarily higher or lower than the average. At the critical point, these fluctuations become very large and occur over very large distances, leading to the system appearing “cloudy” or “milky” — this is the phenomenon of critical opalescence.\n\n\nWhat exactly is fluctuating? At the critical point, density is the key fluctuating quantity, but more generally, the term “fluctuations” can refer to:\n\n\nOrder Parameter Fluctuations: Near a critical point, there is typically an order parameter that characterises the phase transition. For example, in the liquid-vapour transition, this order parameter could be the difference in density between the liquid and vapour phases. As the system approaches the critical point, the fluctuations in the order parameter become large.\nFor example, in a fluid near the liquid-vapour critical point:\n\nIn the liquid phase, particles are densely packed.\nIn the vapour phase, particles are more spread out.\n\nNear the critical point, there is no sharp distinction between liquid and vapour phases. Instead, the system fluctuates between states that resemble both phases. You might see a region that has the density of a liquid next to a region that has the density of a vapour, and these regions appear and disappear rapidly.\n\n\nWhy do these fluctuations occur? At the critical point, the system is in a delicate balance between two phases (e.g., liquid and vapour). The free energy landscape becomes very flat near the critical point, meaning that the system can fluctuate easily between different configurations with very little energy cost. These fluctuations span all length scales, leading to the phenomenon where large regions of the system can fluctuate in density.\nCritical Opalescence: The critical opalescence mentioned in the text is a direct consequence of these large fluctuations in density. Light passing through the fluid is scattered by the density fluctuations. Since the fluctuations occur over a wide range of length scales (from very small to very large), they scatter light across all wavelengths, leading to the “milky” appearance of the fluid near the critical point.\n\n\nWhy does light scatter? Light is scattered by regions of varying density because the refractive index of the fluid depends on its density. When the density fluctuates significantly over large distances, light passing through the fluid encounters regions of different densities, leading to scattering in all directions.\n\nSummary:\n\nWhat is fluctuating near the critical point?: Primarily, the number density (i.e., the number of particles per unit volume) fluctuates near the critical point.\nWhy do these fluctuations grow?: As the correlation length \\(\\xi\\) becomes large, regions of the system separated by large distances become correlated, allowing large-scale fluctuations in density.\nWhat’s the physical manifestation?: These fluctuations lead to observable phenomena such as critical opalescence, where light is scattered by the large fluctuations in density, causing the fluid to appear milky or cloudy.\n\nIn short, the “fluctuations” near the critical point refer to density fluctuations, which are tied to the divergence of the correlation length, and these fluctuations grow larger and affect larger regions of the system as the critical point is approached.\n\n\nFrom chapter 2.4.3, I decided to skim through this document as it became too technical to understand the entire thesis when it began with perturbation theory. Additionally, many concepts here are primarily from equilibrium statistical physics.\n\n\n\n\n\n\nNote\n\n\n\nChapter 2.4.5: Wetting and Drying Phase Transitions is very well written and full of useful knowledge. For the next project meeting, let’s prepare a slide to go over this.\n\n\nWetting and Drying Phase Transitions\nI’ve been reading up to Views of Hydrophobicity, but I realised that I’m not sure how much of the stuff I learn from this thesis is related to active systems. So, I decided to review all articles given by my supervisors.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Weekly Notes",
    "section": "",
    "text": "Busy weeks due to starting three teaching modules: Weekly update of my TSR (Teaching Support Roles). Looking back, I wish I could have completed more reading, as there isn’t much to talk about in terms of active matter research for the next project meeting on Monday.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#week-2",
    "href": "notes.html#week-2",
    "title": "Weekly Notes",
    "section": "",
    "text": "Busy weeks due to starting three teaching modules: Weekly update of my TSR (Teaching Support Roles). Looking back, I wish I could have completed more reading, as there isn’t much to talk about in terms of active matter research for the next project meeting on Monday.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#week-3",
    "href": "notes.html#week-3",
    "title": "Weekly Notes",
    "section": "Week 3",
    "text": "Week 3\n\n2024/9/30 Weekly project meeting\n\nThings to discuss\n\nMeeting with Max on Friday at noon (4th of Oct), thinking of going to Budapest Café\n‘Setting Expectations’ document\nTwo conferences to join:\n• The Dao of Complexity workshop\n• The Statistical Physics of Cognition\nSo, a trip to London, how to sort out things with Clarity, and other arrangements.\nSet up RDSF data storage (Though I checked, OneDrive for Business offers 2TB of storage: Overview of OneDrive for Business)\nBrief plan discussion: reading Mary Coe’s thesis, then Understanding Molecular Simulation book\nAre there MSci students working on this project? \n\n\n\n\n2024/10/2 Why are the clouds at the same height when I look at the cloudy sky?\n\n\n\nView from Brandon Hill\n\n\nWhen we observe clouds appearing at the same height, it’s often due to a phenomenon where a particular layer of the atmosphere has the right conditions for cloud formation. In the troposphere (the lowest layer of the atmosphere), clouds form when the air cools to its dew point, causing water vapour to condense into droplets or ice crystals. This typically happens at specific altitudes where temperature and pressure conditions are ideal for condensation.\nClouds that seem to form at the same height are likely part of the same atmospheric layer, known as a cloud base. The cloud base marks the altitude at which rising air reaches its dew point. If the conditions across the sky are uniform, we’ll see many clouds forming at roughly the same altitude, giving the illusion of a flat layer of clouds.\nCloud formation is fundamentally a non-equilibrium process. It results from dynamic atmospheric conditions like rising air currents, changes in temperature and pressure, and the continuous exchange of energy. These factors drive processes such as condensation and evaporation, which are inherently out of equilibrium. This aligns with the behaviour of active matter systems, where each particle consumes energy to move, keeping the system perpetually out of equilibrium.\nTo create an effective repulsion in our ABP simulations, we consider modifying the wall-fluid interactions to induce a torque that reorients particles away from the surface. One approach might be to introduce an anisotropic interaction potential near the wall. When an ABP approaches the surface, this potential could apply a torque that turns the particle’s propulsion direction away from the wall, effectively reducing its tendency to accumulate there.\n\n\n\n2024/10/5 An idea of beginning the introduction\nThis thesis is ultimately about phase transitions. The central question is: what happens during a transition, for example, from water to ice, from a fluid to a superfluid, or from a paramagnet to a ferromagnet? The physics of such transitions is rich and complex. There are a number of different theoretical tools available to understand them. The study begins by examining thermodynamics and extends to the application of statistical mechanics to these transitions. Additionally, statistical field theory, a fully-fledged field theory similar to what is seen in particle physics or general relativity, will be applied. The goal is to use these approaches, along with concepts of symmetry and other properties, to understand different phases of matter.\nFor a long time, understanding phase transitions was a mystery, especially what’s known as a critical point. It’s an interesting situation: we have something as familiar as water, and it was not until the mid-20th century that significant progress was made. The phase behaviour of water can be examined using a pressure-temperature diagram:\n\n\n\nPhase diagram of water (Cmglee n.d.)\n\nCmglee. n.d. “Phase Diagram of Water Simplified.” https://commons.wikimedia.org/wiki/File:Phase_diagram_of_water_simplified.svg.\n\n\n\nOn the horizontal axis is temperature, and on the vertical axis is pressure.\nThe phase diagram for water features three regions: steam, water, and ice.\nThere’s a point called the triple point, where all three phases coexist, and a critical point, which marks the end of the liquid-gas boundary.\n\nWater is essential, as most of us are mostly water, and yet the physics of what happens at this critical point wasn’t understood for the longest time. It’s a system that is incredibly close to us, yet to understand it requires some of the deepest ideas in physics, particularly the renormalisation group.\nIn this sense, phase transitions form a uniquely positioned subject. They are simultaneously very close to our everyday experience and yet require extremely deep ideas to understand properly. Most other areas of physics are typically far removed from daily life. Quantum mechanics operates on microscopic scales, and astrophysics on cosmological scales, both far from what can be intuitively grasped. But here, some really deep, beautiful puzzles are found in the world around us.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#week-4",
    "href": "notes.html#week-4",
    "title": "Weekly Notes",
    "section": "Week 4",
    "text": "Week 4\n\n2024/10/7 Weekly project meeting\n\nThings to discuss\n\nOverleaf template for the thesis, as I will start writing what I learn for either the introduction or theory sections. Found one: Link\n\n\n\n\n2024/10/8 Analytical Mechanics\nI’ve been watching Tom’s lectures on Analytical Mechanics once more. I’ve just finished part 2. The topics are:\n\nCalculus of variations\nLagrangian mechanics\nHamiltonian mechanics\n\nWhy did I start watching it?\nWell, during the last project meeting, I had a naïve question about whether we could formulate a Lagrangian for the drying scenario. For example, in my supervisor’s papers, they couldn’t manage to observe the drying transition in their setup. So, I asked if, when the trajectory of all particles is away from the surface, we could possibly derive an analytical equation of forces between the particles and the wall to induce torque. Just a pure naïve idea.\nNigel mentioned that the principle of least action makes sense in equilibrium settings, so I had to revise that concept. The Lagrangian is the kinetic energy minus the potential energy, whatever this quantity is. In non-equilibrium systems, energy keeps changing since active agents continuously consume energy. Also, the effective potential depends on the properties of the wall. So, the Lagrangian would become time-dependent, which now makes sense based on what Nigel said.\nThe beautiful theorem was revisited from his lectures: Noether’s theorem.\n\nIf the laws of physics don’t depend on time, energy is conserved.\nSimilarly, if the system has translational or rotational invariance, momentum or angular momentum is conserved, respectively.\n\nThis is beautiful: conserved quantities derive from the symmetries of the system. It also implies that since energy is not conserved for the system I’m interested in, the equations of motion would be time-dependent. I think it would be useful to pay attention to how the equations of motion for active Brownian particles are formulated once I revise their papers.\nAs for his lectures, I’ll continue with part 3 since I’ve already watched the first two. That said, I remember his lecture notes being better than the video recordings, although his video lectures on phase transitions were much more inspiring. So, a note for my future self: when revising the content of the analytical formulation of mechanics, read his lecture notes.\n\n\nPrograms I made\nBefore doing that, I made a Python program that:\n\ndownloads all lectures from Mediasite from the unit I select,\ndetects and removes sections of silence in both the video and audio tracks, applying a buffer of 0.5 seconds before and after each silence to make the transitions smoother,\nsyncs the video and audio together seamlessly to create an mp4 file (the original files were mov, which were almost 10x larger in size),\nkeeps the system active throughout the process by using caffeinate to prevent the computer from going to sleep,\ntracks and reports the download status, ensuring that any interrupted downloads are handled smoothly by automatically updating cookies for authentication,\nensures frame rate consistency to avoid any sync issues that might arise due to different frame rates,\nautomatically manages temporary files created during the silence detection process, keeping the system clean after the task is done.\n\nI’m not sure if I’m allowed to share this code on GitHub, but it was a fun project.\n\n\n\nDownloaded lectures\n\n\nWhy did I make it? Because…\n\nI’ve always felt lazy logging into Blackboard (the university website) every time I wanted to watch videos during my undergraduate years.\nI personally wanted to collect the lectures before they disappeared from the website.\nThere are many pauses in the lectures where the lecturer doesn’t speak while writing equations or words. Removing these pauses also saves total viewing time. Observe the difference:\n\nTotal time to watch all videos of AM (Analytical Mechanics):\n\nAM: 9 hours, 18 minutes, 34 seconds\nProcessed_AM: 8 hours, 23 minutes, 50 seconds\nProcessed_AM (no buffer of 0.5s): 7 hours, 23 minutes, 28 seconds\n\nTotal time to watch all videos of PT (Physics of Phase Transitions):\n\nPT: 11 hours, 42 minutes, 19 seconds\nProcessed_PT: 10 hours, 49 minutes, 18 seconds\nProcessed_PT (no buffer of 0.5s): 9 hours, 45 minutes, 29 seconds\n\nThe buffer is necessary as it makes the transitions feel more natural by introducing a brief pause rather than cutting the silences completely. Watch an example below (duration change: 18:09 (before processing) → 15:29 (after processing with buffers)):\n\n\nVideo\nAM 2.5a Noether’s Theorem\n\n\n\nA beautiful quote from Tom:\nFrom AM 3.4, classical mechanics is the geometry of phase space. All of the geometries (symplectic form, canonical transformations, etc.) describe the phenomenology of classical mechanics.\n\n\n\n2024/10/12 Making a presentation\n241016 Overleaf Presentation for Project Meeting",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project Description",
    "section": "",
    "text": "In terms of the project on ‘Active Drying’, drying is an equilibrium surface phase transition that occurs when a fluid at liquid-vapour coexistence is in contact with an infinite planar substrate that is sufficiently weakly attractive to the fluid particles, such that the vapour phase prefers to be at the substrate. Surface phase transitions can be first-order or critical, and previous results (see [1, 2]) show that drying is generally a critical surface phase transition.\nPrevious work on ABPs has shown that they can exhibit a first-order wetting transition (see [3]). This effect has been investigated by looking at liquid-vapour ‘slabs’ and droplets, showing consistent results. However, the mechanism by which the wetting transition is traversed does not seem to allow for the possibility of a drying transition. To have a chance of observing this, we need to understand the mechanism by which ABPs interact with a surface.\nThe aggregation effect at a hard wall arises because ABPs become trapped at the wall and cannot diffuse away quickly enough. The project will attempt to induce a drying transition by coupling the ABP velocity vector to the wall-fluid interaction in such a way as to produce a torque that creates an effective wall-fluid repulsion, possibly enabling a drying transition. Our task will be to modify a standard MD simulation package to introduce this torque and then proceed similarly to the Turci and Wilding paper in PRL 2021 (see [4]), to try to observe a drying transition. If evidence for this is seen, then we will try to quantify the character of the transition and analyse its critical properties.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#general",
    "href": "index.html#general",
    "title": "Project Description",
    "section": "General",
    "text": "General\nWeekly update of my TSR (Teaching Support Roles)\nDocument search\nUpcoming events by Nonlinear and Complex Physics Group\nCourses to audit\nMPAGS Training Modules\nPhysics PGR SharePoint site: These pages should provide you with a ‘one-stop-shop’ for all of your PGR needs, including information about progress procedures, forms and examinations.\nMyERP",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#it",
    "href": "index.html#it",
    "title": "Project Description",
    "section": "IT",
    "text": "IT\nACRC HPC Documentation and User Guides\nBuying and borrowing IT\nIntroducing HOOMD-blue: HOOMD-blue is a Python package that performs Molecular Dynamics and hard particle Monte Carlo simulations. HOOMD-blue is general and can be used to model nanoparticles, bead-spring polymers, active matter, and many other types of systems.\nData Science and Research Software Engineering Training\nBDDS Python workshops\nQuarto Tutorial",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Purpose of this Website",
    "section": "",
    "text": "Purpose of this Website\nAs I ended week 2 on the 27th of September, I thought it would be useful to keep track of the following from next week (week 3):\n\nWhat I did every day (mostly bookmarking or a summary)\nA brief summary of what was discussed in project meetings\nThings I hope to discuss or ask in the next project meeting\nUseful resources as links, since I often don’t keep track of what I bookmarked in Safari\n\nAuthor: Sohyun Park\nSupervisors: Prof. Nigel Wilding, Dr. Francesco Turci",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "tutorial-document.html",
    "href": "tutorial-document.html",
    "title": "Quarto Document",
    "section": "",
    "text": "Quarto Document",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#colors",
    "href": "tutorial-document.html#colors",
    "title": "Quarto Document",
    "section": "1.1 Colors",
    "text": "1.1 Colors\n\nRed\nGreen\nBlue",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#shapes",
    "href": "tutorial-document.html#shapes",
    "title": "Quarto Document",
    "section": "1.2 Shapes",
    "text": "1.2 Shapes\n\nSquare\nCircle\nTriangle",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#textures",
    "href": "tutorial-document.html#textures",
    "title": "Quarto Document",
    "section": "1.3 Textures",
    "text": "1.3 Textures\n\nSmooth\nBumpy\nFuzzy",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#equations",
    "href": "tutorial-document.html#equations",
    "title": "Quarto Document",
    "section": "1.4 Equations",
    "text": "1.4 Equations\nEinstein’s theory of special relatively that expresses the equivalence of mass and energy:\nE = mc^{2}",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#sec-plot",
    "href": "tutorial-document.html#sec-plot",
    "title": "Quarto Document",
    "section": "3.1 Plot",
    "text": "3.1 Plot\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Simple Plot",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-document.html#sec-equation",
    "href": "tutorial-document.html#sec-equation",
    "title": "Quarto Document",
    "section": "3.2 Equation",
    "text": "3.2 Equation\n\ns = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\overline{x})^2}\n\\tag{1}",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Document"
    ]
  },
  {
    "objectID": "tutorial-basics.html",
    "href": "tutorial-basics.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "Quarto Basics\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "tutorial-computations.html",
    "href": "tutorial-computations.html",
    "title": "Quarto Computations",
    "section": "",
    "text": "Quarto Computations",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Computations"
    ]
  },
  {
    "objectID": "tutorial-computations.html#numpy",
    "href": "tutorial-computations.html#numpy",
    "title": "Quarto Computations",
    "section": "NumPy",
    "text": "NumPy\n\n\nCode\nimport numpy as np\na = np.arange(15).reshape(3, 5)\na\n\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Computations"
    ]
  },
  {
    "objectID": "tutorial-computations.html#matplotlib",
    "href": "tutorial-computations.html#matplotlib",
    "title": "Quarto Computations",
    "section": "Matplotlib",
    "text": "Matplotlib\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\n# fig.set_size_inches(12, 7)\nx = np.arange(10)\ny = 2.5 * np.sin(x / 20 * np.pi)\nyerr = np.linspace(0.05, 0.2, 10)\n\nplt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')\nplt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')\nplt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,\n             label='uplims=True, lolims=True')\n\nupperlimits = [True, False] * 5\nlowerlimits = [False, True] * 5\nplt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,\n             label='subsets of uplims and lolims')\n\nplt.legend(loc='lower right')\nplt.show(fig)\n\n\n\n\n\n\n\n\nFigure 1: Errorbar limit selector",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Computations"
    ]
  },
  {
    "objectID": "tutorial-computations.html#plotly",
    "href": "tutorial-computations.html#plotly",
    "title": "Quarto Computations",
    "section": "Plotly",
    "text": "Plotly\n\nCode\n\"\"\"\nThis allows our figure display to span out beyond the normal body text column.\n#| column: page\n\"\"\"\n\nimport plotly.express as px\nimport plotly.io as pio\ngapminder = px.data.gapminder()\ndef gapminder_plot(year):\n    gapminderYear = gapminder.query(\"year == \" + \n                                    str(year))\n    fig = px.scatter(gapminderYear, \n                     x=\"gdpPercap\", y=\"lifeExp\",\n                     size=\"pop\", size_max=60,\n                     hover_name=\"country\")\n    fig.show()\n    \ngapminder_plot(1957)\ngapminder_plot(2007)\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) Gapminder: 1957\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) Gapminder: 2007\n\n\n\n\n\n\n\nFigure 2: Life Expectancy and GDP",
    "crumbs": [
      "Website Design",
      "Quarto Tutorial",
      "Quarto Computations"
    ]
  },
  {
    "objectID": "theme.html",
    "href": "theme.html",
    "title": "Theme Showdown",
    "section": "",
    "text": "From a pool of 25 themes, the chosen ones are:\n• default\n• flatly\n• minty\n• sandstone\n• united\n• yeti\nCue the dramatic music…\n\n\n\nRound 1: flatly vs default\nWinner: flatly!\nRound 2: minty vs sandstone\nResult: A tie!\nRound 3: united vs yeti\nWinner: united!\n\n\n\nMatch 1: flatly vs minty\nWinner: flatly (Minty was fresh, but flatly had the edge!)\nMatch 2: united vs sandstone\nWinner: united!\n\n\n\n\nflatly vs united\nWinner: flatly! More professional, less adorable. But honestly, united put up a cute fight.\nFor now, flatly reigns supreme… but united is right there, ready for a rematch.",
    "crumbs": [
      "Website Design",
      "Theme Showdown"
    ]
  },
  {
    "objectID": "theme.html#semi-finals",
    "href": "theme.html#semi-finals",
    "title": "Theme Showdown",
    "section": "",
    "text": "Match 1: flatly vs minty\nWinner: flatly (Minty was fresh, but flatly had the edge!)\nMatch 2: united vs sandstone\nWinner: united!",
    "crumbs": [
      "Website Design",
      "Theme Showdown"
    ]
  },
  {
    "objectID": "theme.html#final-round",
    "href": "theme.html#final-round",
    "title": "Theme Showdown",
    "section": "",
    "text": "flatly vs united\nWinner: flatly! More professional, less adorable. But honestly, united put up a cute fight.\nFor now, flatly reigns supreme… but united is right there, ready for a rematch.",
    "crumbs": [
      "Website Design",
      "Theme Showdown"
    ]
  },
  {
    "objectID": "notes.html#introduction",
    "href": "notes.html#introduction",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Introduction",
    "text": "Introduction\nIn this presentation, we explore the theoretical underpinnings of hydrophobicity by examining wetting and drying phase transitions. The phenomena are driven by surface tension and critical temperature scaling, providing insights into surface phase transitions across different substrates.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#youngs-equation",
    "href": "notes.html#youngs-equation",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Young’s Equation",
    "text": "Young’s Equation\nConsider a droplet of liquid on a surface. The contact angle, denoted as θ, quantifies the affinity between the liquid and the surface. This angle is determined by resolving the surface tensions of the interfaces involved. The relationship is given by Young’s equation:\n\\[\n\\cos \\theta = \\frac{\\gamma_{sv} - \\gamma_{sl}}{\\gamma_{lv}} \\tag{1}\n\\]\nWhere: - \\(\\gamma_{sv}\\) is the surface-vapour tension - \\(\\gamma_{sl}\\) is the surface-liquid tension - \\(\\gamma_{lv}\\) is the liquid-vapour tension\n\nHydrophobic vs Hydrophilic\n\nHydrophobic surfaces: \\(90^\\circ &lt; \\theta &lt; 180^\\circ\\)\nHydrophilic surfaces: \\(0^\\circ &lt; \\theta &lt; 90^\\circ\\)\n\nAt two extremes: - \\(\\theta = 0^\\circ\\): Full wetting. - \\(\\theta = 180^\\circ\\): Perfect drying.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#surface-phase-transitions",
    "href": "notes.html#surface-phase-transitions",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Surface Phase Transitions",
    "text": "Surface Phase Transitions\nThe approach to the two limiting cases represents surface phase transitions—wetting for \\(\\theta = 0^\\circ\\) and drying for \\(\\theta = 180^\\circ\\).\n\nCahn’s Scaling Law\nCahn’s theory explains the scaling behaviour of surface tensions as the critical temperature \\(T_c\\) is approached. The surface tension vanishes following the power law:\n\\[\n\\gamma_{lv} \\sim (-t)^\\mu \\quad \\text{with} \\quad \\mu = (d-1)\\nu\n\\]\nFor a fluid with \\(d = 3\\), the critical exponent is \\(\\nu \\approx 0.63\\), yielding \\(\\mu \\approx 1.26\\).\n\n\nContact Angle Near \\(T_c\\)\nAs \\(T_c\\) is approached, the contact angle vanishes:\n\\[\n\\cos \\theta \\sim t^{\\beta - \\mu} \\quad \\text{with} \\quad \\beta \\approx 0.33\n\\]\nThus, \\(\\cos \\theta\\) diverges as \\(t^{-0.93}\\), but is constrained to \\(|\\cos \\theta| \\leq 1\\).",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#wetting-and-drying-transitions",
    "href": "notes.html#wetting-and-drying-transitions",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Wetting and Drying Transitions",
    "text": "Wetting and Drying Transitions\nThe phase transition occurs at a specific temperature: - Wetting occurs at \\(T_W\\), where \\(\\cos \\theta = 1\\). - Drying occurs at \\(T_D\\), where \\(\\cos \\theta = -1\\).\nAt these temperatures, a macroscopic layer of liquid or vapour forms at the surface.\n\nCritical vs First-Order Transitions\nThe nature of these transitions can be: - Critical: The equilibrium film width \\(\\ell_{eq}\\) grows continuously as \\(T_D\\) is approached. - First-order: The film width jumps discontinuously.\nBelow are illustrations of critical and first-order transitions for drying:\n\n# Insert visual figure using code or links to diagrams",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#predrying-transitions",
    "href": "notes.html#predrying-transitions",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Predrying Transitions",
    "text": "Predrying Transitions\nIn systems undergoing first-order drying, predrying can occur at temperatures \\(T &gt; T_D\\). In this case, the vapour film transitions from a thin to a thick layer before the full drying transition at \\(T_D\\).\n\nScaling Behaviour\nThe equilibrium film width \\(\\ell_{eq}\\) exhibits different behaviours depending on the order of the transition:\n\\[\n\\ell_{eq} = \\frac{-\\Gamma}{A_{sv} \\Delta \\rho}\n\\]\nWhere: - \\(\\Gamma\\) is the adsorption - \\(A_{sv}\\) is the surface area of the substrate-vapour interface - \\(\\Delta \\rho = \\rho_l - \\rho_v\\) is the difference between the liquid and vapour densities",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#conclusion",
    "href": "notes.html#conclusion",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "Conclusion",
    "text": "Conclusion\nWetting and drying phase transitions provide critical insights into the behaviour of hydrophobic and hydrophilic surfaces. These transitions are influenced by surface tension, temperature, and the nature of interactions between fluid and substrate. Theoretical models such as Cahn’s scaling law and simulation studies guide our understanding of these phenomena.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#references",
    "href": "notes.html#references",
    "title": "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions",
    "section": "",
    "text": "A beautiful quote from Tom:\nFrom AM 3.4, classical mechanics is the geometry of phase space. All of the geometries (symplectic form, canonical transformations, etc.) describe the phenomenology of classical mechanics.\n2024/10/12 Making a presentation\nIntroduction\nIn this presentation, I’m going to explain the theoretical foundations of hydrophobicity by examining wetting and drying phase transitions. The phenomena are driven by surface tension and critical temperature scaling.\nYoung’s Equation\nConsider a droplet of liquid on a surface. The contact angle, denoted as θ, quantifies the affinity between the liquid and the surface. This angle is determined by resolving the surface tensions of the interfaces involved. The relationship is given by Young’s equation:\n\\[\n\\cos \\theta = \\frac{\\gamma_{sv} - \\gamma_{sl}}{\\gamma_{lv}} \\tag{1}\n\\]\nWhere: - \\(\\gamma_{sv}\\) is the surface-vapour tension - \\(\\gamma_{sl}\\) is the surface-liquid tension - \\(\\gamma_{lv}\\) is the liquid-vapour tension\nHydrophobic vs Hydrophilic\n\nHydrophobic surfaces: \\(90^\\circ &lt; \\theta &lt; 180^\\circ\\)\nHydrophilic surfaces: \\(0^\\circ &lt; \\theta &lt; 90^\\circ\\)\n\nAt two extremes:\n\n\\(\\theta = 0^\\circ\\): Full wetting.\n\\(\\theta = 180^\\circ\\): Perfect drying.\n\nSurface Phase Transitions\nThe approach to the two limiting cases represents surface phase transitions—wetting for \\(\\theta = 0^\\circ\\) and drying for \\(\\theta = 180^\\circ\\).\nCahn’s Scaling Law\nCahn’s theory explains the scaling behaviour of surface tensions as the critical temperature \\(T_c\\) is approached (see: (Cahn 1977)). The surface tension vanishes following the power law:\n\\[\n\\gamma_{lv} \\sim (-t)^\\mu \\quad \\text{with} \\quad \\mu = (d-1)\\nu\n\\]\nFor a fluid with \\(d = 3\\), the critical exponent is \\(\\nu \\approx 0.63\\), yielding \\(\\mu \\approx 1.26\\).\nContact Angle Near \\(T_c\\)\nAs \\(T_c\\) is approached, the contact angle vanishes:\n\\[\n\\cos \\theta \\sim t^{\\beta - \\mu} \\quad \\text{with} \\quad \\beta \\approx 0.33\n\\]\nThus, \\(\\cos \\theta\\) diverges as \\(t^{-0.93}\\), but is constrained to \\(|\\cos \\theta| \\leq 1\\).\nWhy \\(\\beta - \\mu\\)?\nNear the critical temperature, both \\(\\gamma_{sv}\\) and \\(\\gamma_{sl}\\) vanish. As \\(T\\) approaches \\(T_c\\), \\(\\gamma_{lv}\\) tends to zero as \\((-t)^\\mu\\). The numerator \\(\\gamma_{sv} - \\gamma_{sl}\\) scales with the order parameter difference \\(\\Delta \\rho = \\rho_l - \\rho_v\\), because the distinction between liquid and vapour phases diminishes. This gives:\n\\[\n\\gamma_{sv} - \\gamma_{sl} \\sim (-t)^\\beta\n\\]\nBy substituting the scaling behaviours of \\(\\gamma_{lv}\\) and \\(\\gamma_{sv} - \\gamma_{sl}\\) into Young’s equation, we get:\n\\[\n\\cos \\theta \\sim \\frac{(-t)^\\beta}{(-t)^\\mu} = t^{\\beta - \\mu}\n\\]\nWetting and Drying Transitions\nThe phase transition occurs at a specific temperature:\n\nWetting occurs at \\(T_W\\), where \\(\\cos \\theta = 1\\).\nDrying occurs at \\(T_D\\), where \\(\\cos \\theta = -1\\).\n\nAt these temperatures, a macroscopic layer of liquid or vapour forms at the surface.\nCritical vs First-Order Transitions\nThe nature of these transitions can be:\n\nCritical: The equilibrium film width \\(\\ell_{eq}\\) grows continuously as \\(T_D\\) is approached.\nFirst-order: The film width jumps discontinuously.\n\nBelow are illustrations of critical and first-order transitions for drying:\n\nPredrying Transitions\nIn systems undergoing first-order drying, predrying can occur at temperatures \\(T &gt; T_D\\). In this case, the vapour film transitions from a thin to a thick layer before the full drying transition at \\(T_D\\).\nScaling Behaviour\nThe equilibrium film width \\(\\ell_{eq}\\) exhibits different behaviours depending on the order of the transition:\n\\[\n\\ell_{eq} = \\frac{-\\Gamma}{A_{sv} \\Delta \\rho}\n\\]\nWhere:\n\n\\(\\Gamma = \\int \\mathrm{d}r \\, [\\rho(r) - \\rho_b]\\) is the adsorption, which describes the excess of fluid near the surface relative to the bulk.\n\\(A_{sv}\\) is the surface area of the substrate-vapour interface\n\\(\\Delta \\rho = \\rho_l - \\rho_v\\) is the difference between the liquid and vapour densities\n\nFor \\(\\rho(r)\\), we know from the statistical mechanics of fluids that the one-particle density \\(\\rho^{(1)}(r)\\) can be written in the form of an ensemble average.:\n\\[\n\\langle \\hat{\\rho}^{(1)}(r) \\rangle = \\frac{1}{\\Xi} \\sum_{j=1}^\\infty \\frac{e^{\\beta \\mu N}}{\\Lambda^{3N}(N-1)!} \\int \\mathrm{d}r^{N-1} e^{-\\beta[\\Phi(r^N) + V_{\\text{ext}}(r^N)]}\n\\]\nThis defines the fluid’s spatially varying density at a microscopic level.\n\n\n\n\n\n\n\n\nCahn, John W. 1977. “Critical Point Wetting.” The Journal of Chemical Physics 66 (8): 3667–72.\n\n\nCmglee. n.d. “Phase Diagram of Water Simplified.” https://commons.wikimedia.org/wiki/File:Phase_diagram_of_water_simplified.svg.",
    "crumbs": [
      "Project Notes",
      "Hydrophobicity Across Length Scales: Wetting and Drying Phase Transitions"
    ]
  },
  {
    "objectID": "reading.html#colloquium-inclusions-boundaries-and-disorder-in-scalar-active-matter",
    "href": "reading.html#colloquium-inclusions-boundaries-and-disorder-in-scalar-active-matter",
    "title": "List of My Reading",
    "section": "2024/10/14 - Colloquium: Inclusions, boundaries, and disorder in scalar active matter",
    "text": "2024/10/14 - Colloquium: Inclusions, boundaries, and disorder in scalar active matter\nThree different models of active matter are discussed here: ABP (active Brownian particles), RTP (run-and-tumble particles), and AOUP (active Ornstein-Uhlenbeck particles). ABP is a model that I’m going to investigate initially because it’s the simplest particle model, but I want to be aware that there are other models. From page 3, what is interesting is this, since I saw somewhere that detailed balance doesn’t hold for non-equilibrium systems:\n\n\n\n\n\n\nNote\n\n\n\nOn the contrary, in a harmonic potential AOUPs always have a steady state given by a centered Gaussian distribution (Szamel, 2014) and, somewhat surprisingly, their dynamics obey detailed balance (Fodor et al., 2016).\n\n\nI wanted to revisit the detailed balance algorithm more deeply, so I started reading Understanding Molecular Simulation.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "notes.html#week-5",
    "href": "notes.html#week-5",
    "title": "Weekly Notes",
    "section": "Week 5",
    "text": "Week 5\n\n2024/10/17 Summary of project meetings so far\n\n\nPre-PhD discussions with Nigel\n\nSelf-propelled particles are a big topic at the moment. Many researchers are interested in active matter, which is different from passive matter, the subject we mostly learn about in our undergraduate studies of statistical mechanics. Statistical mechanics is a beautiful way of describing systems with a vast number of components. When we have an Avogadro number of components, we cannot expect to write down the equations of motion for each one and solve them because there are simply too many. Instead, we have to step back and ask, “What can we say about the averages and the fluctuations?” People like Boltzmann and Gibbs gave us a great theory for understanding equilibrium statistical mechanics. Now, we want to move beyond that to study non-equilibrium systems.\nPhysicists want to have simple models that capture the essential elements of the real systems they are studying. We start with basic models, like self-propelled particles. Francesco and Nigel have been studying these, and they are fascinating. For example, purely repulsive particles in an equilibrium system would not exhibit much behaviour. They do not have a liquid-vapour transition because attraction is required for condensation from vapour into liquid. Purely repulsive particles can crystallise, but they do not exhibit a liquid-vapour transition.\nHowever, if we take those same purely repulsive particles and make them self-propelling, a liquid-vapour transition suddenly emerges. This is an example of how interactions between many simply interacting bodies can lead to large-scale emergent properties. These active Brownian particles, which are self-propelled and repulsive, show this phase transition into a liquid-vapour phase. This is not just about bulk phase transitions but also about what happens at surfaces. How do they interact with a surface? Can phase transitions occur at a surface, such as wetting or drying transitions?\nThese are very new areas of research. Francesco and Nigel have been exploring some of these questions, and their recent papers reflect that. They realise there are many interesting directions this research could take. One of the main tools here, since we do not have advanced theories like those for equilibrium systems, is computer simulation. We are fortunate to have great high-performance computing resources in Bristol. Some of these problems need high performance, as they require large systems with many particles.\nAnother area Nigel is interested in is hydrophobic interactions. We have particles that are either entirely hydrophobic or amphiphilic, meaning they have both hydrophobic and hydrophilic parts. These particles can self-assemble into large-scale structures, but there are still aspects of that process that are not well understood. For instance, it is known experimentally that heating amphiphiles promotes self-assembly. Normally, heating increases disorder, so why is it different here? What is happening with hydrophobic or amphiphilic interactions that leads to this interesting temperature dependence? This is not well understood at the moment, and it is an open question that he hopes to address soon using computer simulation.\nHe thinks there may be other factors at play here, relating to surface phase transitions. They have made some progress in understanding how water behaves at a planar surface, like a hydrophobic planar surface. That behaviour is essentially the physics of a drying transition. All these things are connected. Applying our understanding of what happens at a planar surface to hydrophobic entities like solute particles, molecules, amphiphiles, or proteins brings basic physics closer to biological systems and non-equilibrium systems. This is an exciting area because it links traditional equilibrium phase transition concepts to how nature exploits them for function.\nProfessor Paul Bartlett in the chemistry department also works in this area, conducting colloidal experiments. Colloids are used in a range of industrial products, such as paints, shampoos, personal care products, and foodstuffs. They are important in the industry, but making new products with new functionality is challenging because these products are often mixtures of many different components. The space of possible combinations is enormous, making it difficult to determine the right mix to achieve a desired property.\nThe question is whether simulation, something like digital twins, can help us find useful mixtures more efficiently and avoid trial-and-error. Much of what happens now is trial and error. If we could create a digital twin or simulation based on real data, it could guide us. AI and neural networks could be used to incorporate real data into simulations, teaching the system the interactions between the components to reproduce real behaviour. Then, we could explore new directions, new types of mixtures, or new areas of phase space more quickly and at lower cost.\nWe are still some way from that goal, but he believes we are ready to take the first steps in that direction. Simulations with an AI or neural network overlay, which learn how to reproduce experimental data, would be the first step.\nRegarding the specific project, how do active Brownian particles interact with a surface? What if that surface is flexible, like a lipid membrane, or penetrable so that the particles can pass through it like a thin membrane? We want to study these systems through simulation and use our knowledge of statistical physics to understand them.\nIt is not biology, but it is not far removed. There are many exciting aspects to explore, but the focus will be on a simulation approach. One attractive feature of statistical physics is the abundance of problems to choose from, allowing researchers to make real progress, often with just one or two people. In particle physics or astrophysics, researchers are often part of large collaborations and do not have the freedom to determine the direction of the research. Success in such fields is shared among many, whereas in our field, a small number of people can have a significant impact, giving more freedom and satisfaction. Some of the work may be patented, but most of it is published in the open literature, with your name at the top, not among a thousand others. You really feel that your contribution is valued.\nFor active Brownian particles, we use a method called Brownian Dynamics. It is similar to Molecular Dynamics, but for passive particles in equilibrium, Molecular Dynamics is quite straightforward. We calculate all the forces; every particle interacts via some potential, and we determine the force on each particle due to the others it interacts with. Then, we integrate the equations of motion, Newton’s equations, through time.\nEssentially, we evolve the particle positions through time in a stepwise manner. We increase time from T to T+ΔT and determine where all the particles will be, then iterate and repeat this process. The particles move around the system according to Newton’s equations of motion, making it physically realistic.\nFor active Brownian particles, in addition to the passive interactions between the particles, each particle experiences a driving force along a specific direction, which we represent as an arrow. In the case of Janus particles, this driving force may be due to a chemical reaction, such as with peroxide, which creates movement along a particular vector. Because of interactions with other particles, this vector diffuses over time, and we treat this stochastically using random numbers. Like in Molecular Dynamics, we advance the system in time steps, but we also add a random displacement to the driving force. At each time step, the arrow may point in a slightly different direction, and the force acts along that direction. The dynamics of the system result from both the passive interaction forces between the particles and the driving force, which can diffuse over time. This is the algorithm we use, known as Active Brownian Dynamics.\nThe term “Brownian” refers to the diffusion of the force vector for each particle. The programs for these simulations are quite simple. We do not always need to write our own from scratch, as some major Molecular Dynamics packages can handle this for us. In some cases, we can use pre-existing, optimised community codes that are efficient, fast, and parallelised. However, we often do specialised work that requires writing our own code, so there is plenty of opportunity to develop custom code as well.\nActive Brownian Dynamics is distinct from Monte Carlo methods, which are mainly used for equilibrium systems. Unlike Molecular Dynamics, which evolves the system according to Newton’s equations of motion, Monte Carlo does not produce realistic dynamics. In Monte Carlo simulations, the transition from one configuration of particles to the next is not physically realistic, but the generated configurations are correctly distributed according to their Boltzmann weights in terms of free energy. Monte Carlo is used to generate equilibrium-distributed configurations from which we can measure various properties, but it does not provide dynamical information.\nMonte Carlo methods are very efficient at generating different, uncorrelated configurations because they allow for large jumps between configurations, exploiting fictitious dynamics. In contrast, Molecular Dynamics or Active Brownian Dynamics involve gradual changes, making it more like a shuffle that takes longer to reach a completely different configuration. Monte Carlo methods enable much larger leaps, making them more efficient at collecting uncorrelated statistics.\nMonte Carlo is better at exploring phase space. Nigel has spent much of his career developing algorithms to explore new areas in configuration space. For example, to study the phase transition from a liquid state to a crystal state, where particles arrange on a lattice, it is difficult to determine the parameters at which the transition occurs using Molecular Dynamics because of effects like hysteresis. One of the innovations he developed for Monte Carlo was a method that, instead of gradually nucleating a crystal and letting it grow, jumps completely from a fluid configuration to a crystal configuration in a single move. This approach, called “Phase Switch Monte Carlo”, allows the system to explore each state and then jump back, enabling the calculation of free energy differences and the precise determination of coexistence points.\nThis is the sort of technique that makes Monte Carlo powerful, something Molecular Dynamics cannot easily do. Different phenomena require different techniques. If we are interested in dynamics, we need one technique; if we are interested in phase transitions between very different structures, we need another. The lack of symmetry is what makes these problems challenging. By exploiting fictitious dynamics in Monte Carlo, we can map a liquid configuration onto a solid configuration, even though this cannot physically occur in reality. In practice, a small region would nucleate and grow into a crystal, eventually filling the entire simulation box. If we are only interested in the coexistence conditions, then Monte Carlo is far more powerful.\nI have enjoyed Tom’s phase transitions course, which introduced many important ideas. It covered models of phase transitions, Landau theory, and aspects of the renormalisation group. It also discussed the different ensembles, the relationship between the system and its surroundings, and the various free energies relevant to each case.\n\nSupervisor: The other thing that may drive you—okay, this is different. This is morphogenesis. This concept dates back to the 19th century. It’s related to mathematical physics. The question is, how does shape emerge? At that time, it typically boiled down to geometries. But then, it’s not always just geometry. What I want to say is, if you are interested in biological motivation or the connection between physics and biology, that’s another possibility—taking the angle of active matter as a model for biological systems.\nWe’re not direct experts in this, but we have connections that we could explore if you’re interested in linking the theoretical approach to some kind of experimental test. That’s always possible. The last angle is linking artificial systems, like robots. In Bristol, there’s a big robotics centre—actually, the largest nationwide—the Bristol Robotics Laboratory. It’s a collaboration between us and the University of the West of England (UWE), but it’s not here on this campus; it’s at the UWE campus.\nThere are researchers here who do this as well. The most famous, at least relevant to my interests, is Sabine Hauert. You will see she has a series of TED Talks. If you watch them, you’ll get a sense of what she does. She’s interested in swarm robotics. There’s a very small step between what she does and what we’re discussing. I don’t know if all her talks are available online—there should be plenty of material. Watching them might give you a sense of the kinds of things they do.\nSwarm robotics, in many ways, aligns with what we’re doing in active matter. It’s just at a different scale. They study behaviours from the very small—the microscopic level—all the way to larger scales. For example, they have projects involving drug delivery. You can say that this would be a kind of morphogenesis. These are actual videos of “kilobots,” which are two-dimensional systems made up of many small robots. They have limited sensing but can communicate.\nIn the systems we study, there’s no direct communication like that; there’s not even inherent activity in the same way. However, with swarms, they study how collective behaviour and communication can lead to emergent properties. Even in engineering, they’re already thinking in terms that we consider in physics. Our role is to support them with a somewhat broader picture.\nWhat we do now is provide model systems, classes of problems, and minimal models. That’s what we can contribute from the physics side. We try to reduce the complexity and say, “If you perturb these few ingredients, you have this space of parameters that opens into different phases—maybe phase transitions or some kind of controlled behaviour.” That’s what we can do as physicists.\nTypically, engineers aren’t interested in that level of abstraction. They say, “I have all these parameters; I perturb them, and this is what happens.” We try to provide a coherent picture. However, there is the possibility of collaborating. We can take some generic problems that have the potential to interface with biology, engineering, and other aspects of physics.\nIt also depends on whether you want to go for a more theoretical or more applied angle. We can start with the theoretical basics to get the foundation covered, and then we can move in different directions later. It’s really a matter of seeing what your driving interest is. If you’re not sure, we’ll provide you with an initial exploratory project to work on. This will give you the opportunity to identify what aspect you find the most interesting and would like to pursue for the next couple of years.\nIf you wanted to, there would be the opportunity to engage with this. For instance, there’s no issue in asking Sabine, “We have this good PhD student who is interested in testing her results in the field.” I managed to set up the kilobots, saw them moving around. It wasn’t very exciting, but it was doable. If I can do it, you can do it too.\nI’ve already chatted with Sabine because I’ll have a PhD student myself next year or the following year, beyond you. The idea is to set up multiple projects in that direction. She’s interested, so if swarms are a topic that interests you, we can have an interdisciplinary project in that direction. That wouldn’t be hard to arrange.\nSupervisor: So, entropy is not just disorder, okay? This notion of entropy as disorder is a popular message that simplifies a lot of the storytelling around statistical mechanics. The classical example that we often give in this department is that if you take only hard spheres—a purely repulsive system—there is no energy, right, apart from infinite energy when they overlap. So, the energy is always zero because all the acceptable configurations are at zero energy. Then, the free energy is only entropic, just entropy.\nYou might think, “Okay, with only entropy, you will just have disorder.” That would be the natural assumption. In this simple system, there’s no kind of interaction, just hard spheres. However, what you actually observe is that if you increase the density of such systems, you spontaneously observe a transition from a fluid to a crystal.\nYou: Really?\nSupervisor: Yes, you spontaneously observe this transition. The crystal that forms is essentially FCC-like (face-centred cubic). So somehow, even though this system is maximising entropy—because there is only entropy involved—it results in an ordered state at these very high densities. This occurs because the crystal packing is better, so every single sphere has a little bit of extra wiggle room, more local configurations. The effective volume accessible to each sphere is larger, even though they are in an ordered configuration.\nSo, you can obtain crystalline order by maximising entropy. It’s quite counterintuitive. You would think that maximising entropy means just getting disorder and taking the system very far away from any kind of ordered pattern. But in fact, no. If you take this simplest system, it goes into the most obvious ordered state—a crystal—by maximising entropy.\nThis idea that entropy equals disorder is just a story we tell that simplifies things. Entropy can take you in many different directions; it’s not just disorder. It means the choice of possible states is broader than what you may think initially. Entropic interactions do not necessarily lead to disorder; on the contrary, they can lead to packing, to crystallisation, and so on.\nYou: Interesting. How does this relate to active systems?\nSupervisor: In the case of active systems, this construction becomes even further apart. Energy is there, but it is not the only relevant force. You have these intrinsic forces in such systems, so they cannot be written directly in the same form. The notion of accessible states, the statistical nature of entropy, can still come back if you have at least steady states. The free energy becomes more about the log of the probability distribution, as you’ve seen in your project.\nYour project was all equilibrium, but not really. The financial data was some kind of steady state; you assumed some steady state. All this is to say that, for example, self-assembly in biological systems—biological molecules or macromolecules—yes, it is driven by free energies, at least in our picture, because you have some solvent, and they tend to minimise the free energy. But the energy landscape is very complicated due to this competition between many different states that entropy actually can favour.\nThese states are not necessarily disordered; they are just different from the ones that energy alone would pull toward. The main picture here is that not all of the rest is disorder. It’s essentially about simple basins versus complex basins in free energy. That’s the problem, for example, in protein folding. The free energy landscape is very structured, not just because of the energy, but also because of all the possibilities that entropy adds to it. It’s not just about misfolding—all the different configurations represent different structures.\nYou: I’m curious about how life emerges. That’s the question I want to answer.\nSupervisor: If you want to narrow it down to something that’s researchable, maybe you want to clarify—emerges from what, under what conditions? Maybe imagine a particular setup. This kind of problem, for example, has been tackled here in chemistry by taking the approach of constructing proto-cellular structures from purely synthetic components. It’s sort of a bottom-up approach, asking, “If I add this, this, and this ingredient, how far can I go?”\nThis is the group of Professor Stephen Mann. They are quite successful and have managed to produce very plausible membrane-like structures that assemble similarly to how natural membranes do. From our point of view in physics, it means many different things. The active matter angle is more about how, for example, morphogenesis occurs—not how life itself emerges, but how patterns in biological structures emerge. This is a related question but leans more towards mathematical and physical understanding.\nFor instance, you could try to cast it into a set of partial differential equations. I was trying to reproduce a paper on this topic recently. It’s fascinating because this isn’t active matter at all; it’s simply a set of differential equations. The study focuses on diatoms—not diatomic molecules, but little organisms that produce intricate shells. The paper attempts to create a minimal model that can manifest similar patterns.\nWhat they do is take the obvious route: they write down a partial differential equation, essentially a reaction-diffusion model. Turing demonstrated that with suitable parameters, these models break into bifurcation-like phenomena, leading to pattern formation. They explore their model by selecting certain initial conditions and letting the system evolve, showing how various patterns emerge.\nYou: Is this computationally intensive?\nSupervisor: Not necessarily. It’s just about evolving a partial differential equation on paper. It’s fairly simple—around four equations in total. I should be able to reproduce it; I’m just fighting with the package I’m trying to use. This is an example of a minimal model. In the spirit of this, you’re assuming some nonlinearity in the relationships between suitable fields. This is doable in physics.\nThere is always an element of relaxation towards equilibrium in these models. Active matter, typically, is more concerned with steady states far from equilibrium, where you have strong intrinsic forces driving the system, pushing it away from equilibrium. However, you may still observe phases that resemble equilibrium phases, like a crystalline or liquid phase with characteristic density fluctuations.\nYou can’t always address where the driving force is coming from. In these models, even in the partial differential equations, we often put some ingredients in by hand. The central questions, like “How does life emerge?” are not fully addressed in physics; they might belong more to chemistry because they involve molecular-level processes. Life involves some kind of chemical cycle that transforms into a stable mechanism.\nYou: So, the emergence of life is more of a chemical question?\nSupervisor: Yes, at its core, it’s more of a chemical question. It’s about looking at chemical networks and how they transform into stable mechanisms. We’re not really experts in chemical networks, but there’s a lot of work on understanding non-equilibrium chemical networks and their structures. You’re asking an enormous question—how does life emerge? One way to approach this is by narrowing it down to a more specific problem that can be explored from a particular perspective.\nYou need to accept a set of assumptions and focus on a particular aspect, whether it’s morphogenesis, collective behaviour in bio-inspired systems, or understanding how chemical networks are affected by non-equilibrium forces, or what is the best mathematical description that allows you to categorise different chemical networks. This is a very broad question, and it’s huge in scope. We need to be inspired by that motivation but narrow it down to something achievable in the span of four years. It sounds like a lot of time, but it’s not that much.\nI think we’ve preempted a conversation that we’ll probably have with you in the next few months. This is just food for thought for now. If you need input, you can drop by. Let’s say you find an article and want to know more about its background or how it links to what you’ve done or plan to do—we can help with that.\nYou: Can I ask you one question? All these physics fields—or I would probably call them games because they are like tools to play with to a large extent—what are the things that you’re most curious about?\nSupervisor: Personally, I’m interested in going towards the biophysics angle. I’ve been working on abstract models of liquids for a long time. For example, I’ve simulated liquids in four dimensions on a curved three-dimensional surface. No one can actually sample those configurations; they don’t exist in reality. So, I would like to connect back to more concrete problems inspired by biophysics.\nMorphogenesis is one of the directions I would like to explore—using our understanding of active systems to make sense of the emergence of patterns in biological systems. But you can’t just jump into this field directly; you need to take a particular angle. In my case, it would be coupled to things like network formation or slow dynamics—problems where I already have some expertise.\nAt the same time, we are currently working on a basic but significant question: what drives the hydrophobic effect? When I give talks on biophysics, I can’t properly explain the hydrophobic effect—the one I mentioned earlier. If you talk to different people in this department or in chemistry, each person will tell you a different story about what they think causes the hydrophobic effect.\nSome will say, and you’ll find this on Wikipedia, that around a hydrophobic solute, water forms an ice-like structure, creating some sort of clathrate shape. It’s a story people tell themselves, but there’s no real evidence for it. The idea comes from an interpretation of experimental results. The experiments do show that there is some enthalpic and possibly entropic contribution, but it’s not clear that there is a structural change like that occurring. It’s more like a heuristic story we tell ourselves.\nYou: That makes sense. So, it’s more of a model or hypothesis?\nSupervisor: Yes, exactly. For example, if you look at surface phase transitions in liquids, these are actual phase transitions. You have a well-defined surface in contact with the liquid, and everything is more straightforward. The wall is infinite, so integrals converge, and you get proper correlation lengths. However, these transitions are challenging to sample numerically because you need to simulate both the bulk and the surface, ensuring that the surface is large enough to capture all the fluctuations.\nNow, when you introduce a solute in a fluid, everything is rounded by definition—there’s no thermodynamic limit in the same way. The solute has a finite surface, and the interactions are what they are. This solute might be roughly the same size as the fluid molecules, so there has to be a crossover regime where you can’t really coarse-grain. The question is, how much of this physics of surface phase transitions still controls those effects?\nHere, I’m not even talking about activity. We have looked at wetting and drying in active systems, but for hydrophobicity and related phenomena, the question is, can we relate the strength of the effect? Let’s say we take a protein. In this picture, a protein is just a blob, and it’s more complicated than that, but you could say it’s a blob with areas that are hydrophobic and others that are hydrophilic. What does that truly mean?\nStatistically, you see that it tends to stay in contact with other proteins like itself and away from water. That’s actually all you know—it tends to come together and not expose itself to water. For example, if you put it on a membrane, it sticks in one way and not the other. There are many other ideas linked to being hydrophobic, but it’s not a mechanistic explanation. We are just saying, “That’s what we observe.” It’s more of an effective, statistical statement.\nYou: So, it’s more about the properties of the solvent around the molecule?\nSupervisor: Yes, exactly. We want to measure the properties of the solvent around it. In this case, the solvent is water, but it could be something else. Quantities that seem to be interesting include some kind of measure of local compressibility. What does it mean to be “local”? What is the compressibility when you only have a few molecules? Can you still define it?\nIt seems like we can define something. Correlating the existing metrics of hydrophobicity with whatever we have as a metric for the behaviour of the fluid around a molecule is something we’re exploring at the moment. Before jumping straight into proteins, we look at simpler cases that we can control.\nFor example, we have a solute, a rigid molecule shaped like a lock—a sphere with a concavity. The curvature of this object changes, which is relevant because proteins have complex shapes with pockets and holes. The solvent here is water, and we observe effects that are not captured by true dynamics. We use Molecular Dynamics with some bias. If you let it go without bias, the solute just sticks there and never gets out. Even though the particles are all repulsive apart from the water, the solvent pushes the solute into the cavity, creating a large effective potential that keeps it there.\nThat’s the hydrophobic force—an effective potential of mean force, which can be seen as the logarithm of a probability. So, we’re trying to calculate the logarithm of this probability.\nYou: That makes a lot of sense. This is very enlightening.\nSupervisor: Yes, it’s a fascinating topic. There’s a lot more to explore, especially in how we understand the interactions between solutes and solvents. In the end, it’s about connecting abstract models back to real, observable phenomena and seeing how far we can push those models to explain complex behaviour in biological systems.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#hydrophobic-vs-hydrophilic",
    "href": "notes.html#hydrophobic-vs-hydrophilic",
    "title": "Weekly Notes",
    "section": "Hydrophobic vs Hydrophilic",
    "text": "Hydrophobic vs Hydrophilic\n\nHydrophobic surfaces: \\(90^\\circ &lt; \\theta &lt; 180^\\circ\\)\nHydrophilic surfaces: \\(0^\\circ &lt; \\theta &lt; 90^\\circ\\)\n\nAt two extremes:\n\n\\(\\theta = 0^\\circ\\): Full wetting.\n\\(\\theta = 180^\\circ\\): Perfect drying.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#cahns-scaling-law",
    "href": "notes.html#cahns-scaling-law",
    "title": "Weekly Notes",
    "section": "Cahn’s Scaling Law",
    "text": "Cahn’s Scaling Law\nCahn’s theory explains the scaling behaviour of surface tensions as the critical temperature \\(T_c\\) is approached (see: (Cahn 1977)). The surface tension vanishes following the power law:\n\nCahn, John W. 1977. “Critical Point Wetting.” The Journal of Chemical Physics 66 (8): 3667–72.\n\\[\n\\gamma_{lv} \\sim (-t)^\\mu \\quad \\text{with} \\quad \\mu = (d-1)\\nu\n\\]\nFor a fluid with \\(d = 3\\), the critical exponent is \\(\\nu \\approx 0.63\\), yielding \\(\\mu \\approx 1.26\\).",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#contact-angle-near-t_c",
    "href": "notes.html#contact-angle-near-t_c",
    "title": "Weekly Notes",
    "section": "Contact Angle Near \\(T_c\\)",
    "text": "Contact Angle Near \\(T_c\\)\nAs \\(T_c\\) is approached, the contact angle vanishes:\n\\[\n\\cos \\theta \\sim t^{\\beta - \\mu} \\quad \\text{with} \\quad \\beta \\approx 0.33\n\\]\nThus, \\(\\cos \\theta\\) diverges as \\(t^{-0.93}\\), but is constrained to \\(|\\cos \\theta| \\leq 1\\).\n\nWhy \\(\\beta - \\mu\\)?\nNear the critical temperature, both \\(\\gamma_{sv}\\) and \\(\\gamma_{sl}\\) vanish. As \\(T\\) approaches \\(T_c\\), \\(\\gamma_{lv}\\) tends to zero as \\((-t)^\\mu\\). The numerator \\(\\gamma_{sv} - \\gamma_{sl}\\) scales with the order parameter difference \\(\\Delta \\rho = \\rho_l - \\rho_v\\), because the distinction between liquid and vapour phases diminishes. This gives:\n\\[\n\\gamma_{sv} - \\gamma_{sl} \\sim (-t)^\\beta\n\\]\nBy substituting the scaling behaviours of \\(\\gamma_{lv}\\) and \\(\\gamma_{sv} - \\gamma_{sl}\\) into Young’s equation, we get:\n\\[\n\\cos \\theta \\sim \\frac{(-t)^\\beta}{(-t)^\\mu} = t^{\\beta - \\mu}\n\\]",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#critical-vs-first-order-transitions",
    "href": "notes.html#critical-vs-first-order-transitions",
    "title": "Weekly Notes",
    "section": "Critical vs First-Order Transitions",
    "text": "Critical vs First-Order Transitions\nThe nature of these transitions can be:\n\nCritical: The equilibrium film width \\(\\ell_{eq}\\) grows continuously as \\(T_D\\) is approached.\nFirst-order: The film width jumps discontinuously.\n\nBelow are illustrations of critical and first-order transitions for drying:",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "notes.html#scaling-behaviour",
    "href": "notes.html#scaling-behaviour",
    "title": "Weekly Notes",
    "section": "Scaling Behaviour",
    "text": "Scaling Behaviour\nThe equilibrium film width \\(\\ell_{eq}\\) exhibits different behaviours depending on the order of the transition:\n\\[\n\\ell_{eq} = \\frac{-\\Gamma}{A_{sv} \\Delta \\rho}\n\\]\nWhere:\n\n\\(\\Gamma = \\int \\mathrm{d}r \\, [\\rho(r) - \\rho_b]\\) is the adsorption, which describes the excess of fluid near the surface relative to the bulk.\n\\(A_{sv}\\) is the surface area of the substrate-vapour interface\n\\(\\Delta \\rho = \\rho_l - \\rho_v\\) is the difference between the liquid and vapour densities\n\nFor \\(\\rho(r)\\), we know from the statistical mechanics of fluids that the one-particle density \\(\\rho^{(1)}(r)\\) can be written in the form of an ensemble average.:\n\\[\n\\langle \\hat{\\rho}^{(1)}(r) \\rangle = \\frac{1}{\\Xi} \\sum_{j=1}^\\infty \\frac{e^{\\beta \\mu N}}{\\Lambda^{3N}(N-1)!} \\int \\mathrm{d}r^{N-1} e^{-\\beta[\\Phi(r^N) + V_{\\text{ext}}(r^N)]}\n\\]\nThis defines the fluid’s spatially varying density at a microscopic level.",
    "crumbs": [
      "Project Notes",
      "Weekly Notes"
    ]
  },
  {
    "objectID": "reading.html#what-is-a-complex-system-completed",
    "href": "reading.html#what-is-a-complex-system-completed",
    "title": "List of My Reading",
    "section": "",
    "text": "By James Ladyman and Karoline Wiesner\nMy final-year MSci project involved the study of a complex system, foreign exchange markets. Each individual currency was treated as a spin in the Ising model, and by applying E.T. Jaynes’s principle of maximum entropy, we discovered the structure of interactions between these entities. Remarkably, this simple model of magnetism can also describe the phase transitions of liquid-gas systems, as they fall into the same universality class. This model has even been extended to study how critical our brain is, giving rise to the field of the critical brain hypothesis. Therefore, many ideas during my research project were derived from neuroscience papers. The financial market and the brain: both are complex systems.\nAbout a year ago, I read a paper published by James Ladyman, a professor of philosophy of science at Bristol, titled ‘What is a Complex System?’ (Ladyman, Lambert, and Wiesner 2013). As I started my PhD at Bristol, my second supervisor, Francesco, mentioned that he also published a book about complexity. It became clear to me that I should read this book.\n\nLadyman, James, James Lambert, and Karoline Wiesner. 2013. “What Is a Complex System?” European Journal for Philosophy of Science 3: 33–67.\nChapter 1 presents The Truisms of Complexity Science as follows:\n1. More is different.\n2. Nonliving systems can generate order.\n3. Complexity can come from simplicity.\n4. Coordinated behaviour does not require an overall controller.\n5. Complex systems are often modelled as networks or information processing systems.\n6. There are various kinds of invariance and forms of universal behaviour in complex systems.\n7. Complexity science is computational and probabilistic.\n8. Complexity science involves multiple disciplines.\n9. There is a difference between the order that complex systems produce and the order of the complex systems themselves.\nFeatures that are necessary and sufficient for which kinds of complexity and complex system are as follows:\n1. Numerosity: complex systems involve many interactions among many components.\n2. Disorder and diversity: the interactions in a complex system are not coordinated or controlled centrally, and the components may differ.\n3. Feedback: the interactions in complex systems are iterated so that there is feedback from previous interactions on a time scale relevant to the system’s emergent dynamics.\n4. Non-equilibrium: complex systems are open to the environment and are often driven by something external.\n5. Spontaneous order and self-organisation: complex systems exhibit structure and order that arises out of the interactions among their parts.\n6. Nonlinearity: complex systems exhibit nonlinear dependence on parameters or external drivers.\n7. Robustness: the structure and function of complex systems is stable under relevant perturbations.\n8. Nested structure and modularity: there may be multiple scales of structure, clustering and specialisation of function in complex systems.\n9. History and memory: complex systems often require a very long history to exist and often store information about history.\n10. Adaptive behaviour: complex systems are often able to modify their behaviour depending on the state of the environment and the predictions they make about it.\n\n2024/9/27\nReading What is a Complex System? by James Ladyman and Karoline Wiesner. The purpose of reading is to understand how systems out of thermodynamic equilibrium (a feature of complex systems) relate to a wide range of concepts associated with complexity.\n• There was a video that helped clarify what is meant by a Markov chain and a stochastic process being stationary: Markov Chains Clearly Explained! Part - 1. His other videos are helpful as well. For example, when I saw \\(P_{ij}^{(n)} = A_{ij}^n\\) from the n-Step Transition Matrix video (Part 3), it was quite surprising. However, the Part 5 video on hidden Markov models wasn’t very helpful, but reading the appendix in the book was sufficient.\n• What I’ve felt from reading this book (currently at p.90) is that it presents a lot of different ideas and has interesting discussions on the history of science, but I only seem to get excited when it starts mentioning the brain. I’m not sure if I’m necessarily interested in quantifying complexity. From the Apple notes on 28/9/2024:\nAs I’m reading through What is a Complex System?, it seems that the description of a complex system arises as we have more representations at different scales of analysis. The concept of ‘complexity’ was destined to arise as the scientific era evolved.\nNon-equilibrium systems arise because we define what systems are in thermodynamic equilibrium, which are idealisations.\nThings that fail to be idealised fall into this dualistic concept: non-equilibrium.\n\n2024/10/3\nIt took about two weeks to finish this book. It was an accessible read until I reached chapter 4, which was Measuring Features of Complex Systems. In the end, I’m glad that I read it, even though I somehow felt rushed to finish as there were other responsibilities to manage continuously, such as setting up clusters for my university Mac, teaching, office setup, etc. Most importantly, from now on, I am going to focus on reading material that is directly related to my project since it may give the impression to my supervisors that I’m not working hard. Reading a book like this takes a long time, as there are many new terms I had not encountered before.\nAnyway, it was a good read. Would I buy a physical copy for myself? Maybe not, but it was a good one.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  },
  {
    "objectID": "reading.html#on-the-motion-of-small-particles-suspended-in-liquids-at-rest-required-by-the-molecular-kinetic-theory-of-heat-completed",
    "href": "reading.html#on-the-motion-of-small-particles-suspended-in-liquids-at-rest-required-by-the-molecular-kinetic-theory-of-heat-completed",
    "title": "List of My Reading",
    "section": "2024/10/18 - 10/19 On the Motion of Small Particles Suspended in Liquids at Rest Required by the Molecular-Kinetic Theory of Heat (Completed)",
    "text": "2024/10/18 - 10/19 On the Motion of Small Particles Suspended in Liquids at Rest Required by the Molecular-Kinetic Theory of Heat (Completed)\nBy Albert Einstein\nSince I am studying a model called active Brownian particles, a thought occurred to me: I should actually read where this understanding of Brownian motion came from! This was done by the very man: Albert Einstein in 1905 (Einstein et al. 1905). I normally leave my iPad in my office so whenever I needed derivations, I used a pen and note, quite a classical way while reading a classical paper:\n\nEinstein, Albert et al. 1905. “On the Motion of Small Particles Suspended in Liquids at Rest Required by the Molecular-Kinetic Theory of Heat.” Annalen Der Physik 17 (549-560): 208.\nMy handwritten notes\nI’m so glad that I read this. It reminded me of how much I loved studying thermodynamics during my undergraduate studies. It was kind of like studying geometry (or topology). I consider a system subject to constraints and investigate how the particle behaves given the state variables. The paper wasn’t straightforward to read, as I had to remind myself of several concepts such as the continuous form of entropy, perturbation theory, and Fourier transforms to find a solution for diffusion equations.\nI really liked the way he displayed his logic. He considers a system: a pure solvent and suspended particles. These particles are confined by a semi-permeable wall, which only allows solvent to flow, not the solute. Then he thinks about the existing concept of osmotic pressure, which inhibits the solvent from moving to equalise concentrations. He then makes several reasonable assumptions, such as:\n\nSuspended particles have slow and irregular motions due to the solvent’s molecules.\nThe motions of those individual particles are independent of each other.\n\nHe then links the diffusion coefficient to two different contexts. The first is through the theory of diffusion, involving concepts of entropy and free energy. The second is through the theory of thermal molecular motion, where he considers the rate of change of density. By uniting these two theories, he arrives at the root-mean-square displacement of the particle. Wow!\nI’m so happy to have chosen a theoretical physics PhD. Thinking about how theories interconnect reveals a universal understanding. I’m looking forward to reading more of his articles.",
    "crumbs": [
      "Project Notes",
      "List of My Reading"
    ]
  }
]