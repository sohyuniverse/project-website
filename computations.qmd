---
title: "Computations"
toc: true
number-sections: true
format: 
  html: 
    code-fold: true
bibliography: 
  - references/From 14:08:24.bib
  - references/From 16:09:24.bib
  - references/quarto.bib
reference-location: margin
citation-location: margin
jupyter: python3
---

# Localised external potential {#sec-localised-external-potential}

The localised external potential added to the system [@turci_wetting_2021]:  

$$
V_{\text{ext}}(x) = \varepsilon_w \left[ \cos\left(\frac{\pi x}{d}\right) + 1 \right] \times H(d - x) H(x + d),
$$ {#eq-extpot}

where $H(x)$ is the Heaviside function, $d = \sigma$, and $\varepsilon_w$ represents the repulsive barrier strength.

```{python}
#| label: fig-eps
#| fig-cap: "External potential $V_{\\text{ext}}(x)$ for different $\\epsilon_w$"
import numpy as np
import matplotlib.pyplot as plt

# Parameters
sigma = 1.0  # Particle diameter
d = sigma    # Barrier width
ew_values = np.arange(20, 55, 5)  # Barrier strength values from 20 to 50 incrementing by 5

# Define the potential function
def V_ext(x, epsilon_w, d):
    H = lambda x: np.heaviside(x, 0.5)  # Heaviside step function
    return epsilon_w * (np.cos(np.pi * x / d) + 1) * H(d - x) * H(x + d)

# x range
x = np.linspace(-2 * d, 2 * d, 500)

# Plot
plt.figure(figsize=(10, 6))
for ew in ew_values:
    V = V_ext(x, ew, d)
    plt.plot(x, V, label=f"$\\epsilon_w = {ew}$")

plt.axvline(-d, color='gray', linestyle='--', label=r"$x = -d$", linewidth=0.8)
plt.axvline(d, color='gray', linestyle='--', label=r"$x = d$", linewidth=0.8)
plt.axhline(0, color='black', linewidth=0.5)
plt.xlabel("$x$", fontsize=12)
plt.ylabel("$V_{\\text{ext}}(x)$", fontsize=12)
plt.legend(fontsize=10, loc='upper right')
plt.grid(alpha=0.5)
plt.show()
```

::: {.callout-note}
Intermediate values can be thought of as representing a thin porous membrane with nonzero crossing probability.
:::

<details>
<summary>Callout tip</summary>
::: {.callout-tip}
Note that there are five types of callouts, including:
`note`, `tip`, `warning`, `caution`, and `important`.
:::
</details>

# Asymmetry order parameter

The degree of asymmetry of the instantaneous density profile $\rho(x, t)$ with respect to the barrier location is quantified as:  

$$
\mathcal{A}(t) = \left| \frac{\int_{0}^{L_x/2} \rho(x, t) \, dx - \int_{-L_x/2}^{0} \rho(x, t) \, dx}{(\rho - \rho_{\text{LD}}) L_x} \right|.
$$  

In the steady state, the average $\overline{\mathcal{A}(t)}$ (over time and distinct initial conditions) provides a measure of the typical asymmetry of the liquid region with respect to $x = 0$.

```{python}
#| label: fig-aop
#| fig-cap: "Asymmetry order parameter $\\mathcal{A}(t)$ over time for $\\epsilon_w$ = 20"
import pandas as pd
import numpy as np
import re
import os
import matplotlib.pyplot as plt

def load_profile(filename):
    """
    Load LAMMPS profile dump file into a pandas DataFrame.
    Reads multiple timesteps and returns a DataFrame containing data for all timesteps.
    """
    data = []
    with open(filename, 'r') as f:
        lines = f.readlines()

    i = 0
    while i < len(lines):
        line = lines[i].strip()
        # Skip comment lines and empty lines
        if line.startswith('#') or line == '':
            i += 1
            continue
        tokens = line.split()
        if len(tokens) == 3:
            # Try to parse the line as Timestep and Number-of-chunks
            try:
                timestep = int(tokens[0])
                chunk_count = int(tokens[1])
                # Total-count is ignored
            except ValueError:
                # If parsing fails, skip this line
                i += 1
                continue
            # Read the data for this timestep
            data_start = i + 1
            data_end = data_start + chunk_count
            for j in range(data_start, data_end):
                if j >= len(lines):
                    break  # Prevent index out of range
                dl = lines[j].strip()
                if dl == '':
                    continue
                dl_tokens = dl.split()
                if len(dl_tokens) != 4:
                    continue
                try:
                    chunk = int(dl_tokens[0])
                    coord1 = float(dl_tokens[1])
                    ncount = int(float(dl_tokens[2]))  # Convert to float first to handle possible decimals
                    density = float(dl_tokens[3])
                    data.append({
                        'Timestep': timestep,
                        'Chunk': chunk,
                        'Coord1': coord1,
                        'Ncount': ncount,
                        'density': density
                    })
                except ValueError:
                    continue  # Skip lines with invalid data
            i = data_end
        else:
            i += 1

    df = pd.DataFrame(data)
    return df  # Return the DataFrame with all timesteps

def calculate_asymmetry(data, lx, rho_ld, rho):
    """
    Compute the asymmetry order parameter A(t) with proper normalization.
    """
    x_bins = data['Coord1']  # Bin center positions along x-axis
    densities = data['density']  # Density per bin

    # Ensure that x_bins and densities are sorted according to x_bins
    sorted_indices = np.argsort(x_bins)
    x_bins = x_bins.iloc[sorted_indices].reset_index(drop=True)
    densities = densities.iloc[sorted_indices].reset_index(drop=True)

    bin_width = abs(x_bins.iloc[1] - x_bins.iloc[0])  # Calculate bin width

    # Split the bins into left and right of the barrier (x = 0)
    left_mask = x_bins < 0
    right_mask = x_bins > 0

    left_bins = densities[left_mask]
    right_bins = densities[right_mask]

    # Integrate density over the left and right regions
    left_integral = (left_bins * bin_width).sum()
    right_integral = (right_bins * bin_width).sum()

    # Numerator for A(t)
    numerator = abs(right_integral - left_integral)

    # Denominator normalisation
    denominator = (rho - rho_ld) * lx

    # Asymmetry order parameter
    asymmetry = numerator / denominator if denominator != 0 else 0

    return asymmetry

def plot_asymmetry_vs_time(filename, lx, rho_ld, rho, taur):
    """
    Compute and plot the asymmetry order parameter A(t) as a function of t/taur for a given dump file.
    Estimate the steady-state relaxation time tau_c based on the rate of change of A(t).
    """
    data = load_profile(filename)
    # Compute asymmetry at each timestep
    asymmetry_list = []
    timesteps = []
    grouped = data.groupby('Timestep')
    for timestep, df_timestep in grouped:
        asymmetry = calculate_asymmetry(df_timestep, lx=lx, rho_ld=rho_ld, rho=rho)
        asymmetry_list.append(asymmetry)
        timesteps.append(timestep)

    # Convert to numpy arrays
    asymmetry_array = np.array(asymmetry_list)
    timestep_array = np.array(timesteps)

    # Sort by timestep
    sorted_indices = np.argsort(timestep_array)
    timestep_array = timestep_array[sorted_indices]
    asymmetry_array = asymmetry_array[sorted_indices]

    # Calculate t / taur
    dt = 0.00004 * taur
    t_over_taur = timestep_array * dt / taur  # Simplifies to timestep_array * 0.00004

    # Extract epsilon_w from filename
    # filename format: 'wet.<timestamp>.eps.<epsilon_w>.ly.<ly>.dump'
    pattern = r'wet\..*\.eps\.(\d+)\.ly\.(\d+)\.dump'
    match = re.match(pattern, os.path.basename(filename))
    if match:
        epsilon_w = match.group(1)
        ly = match.group(2)
    else:
        epsilon_w = 'Unknown'
        ly = 'Unknown'

    # Plot A(t) vs t / taur
    plt.figure(figsize=(10, 6))
    plt.plot(t_over_taur, asymmetry_array, linestyle='-', label=r'$\mathcal{A}(t)$')
    plt.xlabel(r'$t / \tau_r$', fontsize=14)
    plt.ylabel(r'$\mathcal{A}(t)$', fontsize=14)
    plt.text(0.05, 0.95, f'$\\epsilon_w$ = {epsilon_w}, $L_y$ = {ly}',
            transform=plt.gca().transAxes,
            fontsize=12,
            verticalalignment='top',
            bbox=dict(facecolor='white', edgecolor='black', boxstyle='round', pad=0.5))
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return t_over_taur, asymmetry_array

def compute_taur():
    """
    Compute the persistence time tau_r based on the LAMMPS input script variables.
    """
    sigma = 1.0
    wcaepsilon = 1.0    
    friction = 50.0
    Pe = 50.0
    activity = 24 * wcaepsilon / (sigma * friction)  # activity = 24*epsilon/(sigma*friction)
    T = activity * friction * sigma / (3 * Pe)  # temperature
    Dr = 3 * T / (friction * sigma**2)  # rotational diffusion
    taur = 1 / Dr  # persistence time
    return taur

# Main script
if __name__ == "__main__":
    folder = "dumps"  # Folder containing dump files
    lx = 240.0  # Total system size in x-direction
    rho_ld = 0.15  # Low-density MIPS value
    rho = 0.5  # Total system density
    # Compute taur
    taur = compute_taur()
    print(f"Computed tau_r (persistence time): {taur}")

    # Plot asymmetry vs time for multiple dump files
    for eps in range(20, 51, 5):  # Iterate over eps values: 20, 25, 30, ..., 50
        filename = os.path.join(folder, f'wet.241206.0213.eps.{eps}.ly.120.dump')
        if os.path.exists(filename):
            t_over_taur, asymmetry_array = plot_asymmetry_vs_time(filename, lx=lx, rho_ld=rho_ld, rho=rho, taur=taur)
        # else:
            # print(f"File {filename} does not exist.")
```

# Slurm Jobs  

- **slurm-10797760**: It took 20 minutes to compute 2,500,000 steps.  
  → This means I need about 27 hours to compute 4 × 50,000,000 steps for four epsilons.  

- **slurm-10802470**: Lost atoms in all epsilons.  

- **slurm-10814916**: Cancelled, since I realised what was happening was:  
  ABPs interacted to form clusters before the wall was even there.  
  So, I commented out the following lines:  
  ```  
  # variable relaxation equal ceil(10*${taur}/${dt})  
  # thermo_style custom step time v_tscaled pe density press  
  # thermo ${snapshot} # output thermo data every snapshot steps  
  # run ${relaxation}  
  ```  

- **slurm-10815371**: Lost atoms again in `eps=20` even when `dt = 0.00002*${taur}`.  

- **slurm-10815451**: Lost atoms again for all epsilons even when `dt = 0.00001*${taur}`...  
  I realised I didn’t include `timestep ${dt}`! Now it works perfectly.  

- **slurm-10824222**: All of them were extremely successful! It took about 8 hours to compute 4 different potentials for 4 × 25,000,000 steps.  
  Removed OMPI error by adding `export OMPI_MCA_mca_base_component_show_load_errors=0`.  

- **slurm-10846320**: As soon as I moved to 3D, I experienced, for the first time, an issue where not enough atoms were created:
  ```  
  Created orthogonal box = (-50 -12 -12) to (50 12 12)
    16 by 2 by 2 MPI processor grid
  WARNING: Only inserted 37118 particles out of 43200 (../create_atoms.cpp:894)
  Created 37118 atoms
    using lattice units in orthogonal box = (-50 -12 -12) to (50 12 12)
    create_atoms CPU = 334.523 seconds
  ```  

  Message to Francesco: 

  > I fixed it! I realised the random insertion was basically the Monte Carlo placement of atoms in the box, so I relaxed the constraint for the overlap parameter to be 0.8 rather than ${sigma} (which is 1), then added a short soft-potential relaxation before switching to WCA parameters, allowing atoms to move apart. Then I ran a short FIRE minimisation before proceeding with ABP dynamics, and boom! It all works out perfectly! It took about 6 hours of reading LAMMPS documentation to solve this issue, but it worked, so I was so happy to share!

  **Before**:
  ```bash
  # Define simulation box and atoms
  region box block -${lx} ${lx} -${ly} ${ly} -${lz} ${lz}
  create_box 1 box # allocate one atom type within this region
  create_atoms 1 random ${npart} ${seed} box overlap ${sigma} # create 'npart' particles dispersed at random in the box
  
  # Pair potential
  pair_style lj/cut 3.0 # LJ potential truncated at rc
  pair_coeff * * ${wcaepsilon} ${sigma} ${rc} # set WCA parameters for ALL particles (indicated by the two asterisks)
  pair_modify shift yes # shift potential to zero at rc
  mass 1 1 # set particle mass to 1 in LJ units
  
  # Neighbor list settings of every = 1 and delay = 0 are required for the minimisation, read 'minimize' document
  neighbor 0.3 bin
  neigh_modify every 1 delay 0 check yes

  # ABP dynamics
  fix 1 all abp2d ${T} ${friction} ${activity} ${seed} # implements fix_abp
  ```

  **After**:
  ```bash
  # Define simulation box and atoms
  region box block -${lx} ${lx} -${ly} ${ly} -${lz} ${lz}
  create_box 1 box # allocate one atom type within this region
  
  # ---- Modified Section for Atom Creation and Pre-Relaxation ----
  
  # Attempt to create all atoms with a slightly relaxed overlap constraint
  # Setting 'overlap 0.8' means atoms can be placed if their pairwise distance
  # is at least 0.8*diameter (instead of full 1.0*diameter), making insertion easier.
  create_atoms 1 random ${npart} ${seed} box overlap 0.8 # create 'npart' particles dispersed at random in the box
  
  # Immediately after insertion, a soft-potential relaxation to push particles apart.
  # Switch to a soft pair style:
  pair_style soft 1.122462  # The cutoff isn't crucial here; just needs to be >= sigma
  pair_coeff * * 10.0       # Initial softness amplitude; 10.0 is fairly large repulsion
  mass 1 1 # set particle mass to 1 in LJ units
  
  # A short run to resolve overlaps:
  fix relax all nve
  thermo_style custom step pe press density
  thermo 1000
  
  # Run for some steps to separate particles:
  run 10000
  unfix relax
  
  # Pair potential
  pair_style lj/cut 3.0 # LJ potential truncated at rc
  pair_coeff * * ${wcaepsilon} ${sigma} ${rc} # set WCA parameters for ALL particles (indicated by the two asterisks)
  pair_modify shift yes # shift potential to zero at rc
  
  # Neighbor list settings of every = 1 and delay = 0 are required for the minimisation, read 'minimize' document
  neighbor 0.3 bin
  neigh_modify every 1 delay 0 check yes
  
  # Run a short minimisation before starting ABP:
  min_style fire
  minimize 1.0e-9 1.0e-9 1000 1000
  reset_timestep 0
  
  # ABP dynamics
  fix 1 all abp ${T} ${friction} ${activity} ${seed} # implements fix_abp
  ```

- **slurm-10848549, slurm-10852370, slurm-10864052**: Three jobs were submitted to produce trajectories for 3D simulations based on the following submission script:

    ```bash
    # Set fixed variables
    SEED=1
    RHO=0.75
    LY=12 # half of Ly = 24σ (Ly = 16, 24, 32, 44σ), chosen to compare with Fig. 3(a)

    # Create timestamp variables for date and time
    DATE=$(date +%y%m%d)
    TIME=$(date +%H%M)

    # Run the LAMMPS job for EPS values incrementing by 2 from 2 to 38
    #for EPS in {2..38..4}; do # slurm-10848549
    #for EPS in {4..36..4}; do # slurm-10852370
    for EPS in {28..36..4}; do # slurm-10864052
    #for EPS in {28..36..4}; do
    FILENAME="${DATE}.${TIME}.eps.${EPS}.ly.$((2*LY))"
    echo "Running with EPS=$EPS"
    mpirun --bind-to none -n 64 $HOME/mylammps/src/lmp_mpi -in abp3d-wet.lmp \
        -var dumpfolder dumps \
        -var dumpfile $FILENAME \
        -var seed $SEED \
        -var rho $RHO \
        -var eps $EPS \
        -var ly $LY
    done
    ```

At some point, I had to stop the job associated with `slurm-10852370`, as it critically slowed down the computation when $\text{EPS} = 28$. This job, allocated to `NodeList=bp1-compute194`, was already slower than the job from `slurm-10848549`, which was allocated to `bp1-compute158`. The comparison is as follows:

- **From `bp1-compute158` (average total wall time)**:  
  $$
  \frac{\text{(5:37:42 + 5:34:02 + 5:39:12 + 5:34:59 + 5:34:57 + 5:35:16 + 5:40:10 + 5:38:25 + 5:40:58 + 5:36:07)}}{10} = \text{5:37:10.8}
  $$

- **From `bp1-compute194` (average total wall time)**:  
  $$
  \frac{\text{(7:23:53 + 7:49:35 + 14:03:35 + 6:54:52 + 6:22:22 + 7:52:00)}}{6} = \text{8:24:22.8}
  $$

- **From `bp1-compute147` (average total wall time)** (running while writing the SLURM script for `slurm-10864052`):  
  $$
  \frac{\text{(5:35:53 + 5:36:36)}}{2} = \text{5:36:14.5}
  $$

If I compare the last two jobs, I can only spot one difference: the slow job used the `Backfill` scheduler, which prioritises filling gaps in resource allocation. This may affect performance compared to the `Main` scheduler used for the fast job.

---

:::: {.columns}
::: {.column width="50%"}
```
scontrol show job 10852370
JobId=10852370 JobName=active_drying
   UserId=sp13328(275000) GroupId=phys(1364) MCS_label=N/A
   Priority=10489 Nice=0 Account=phys030385 QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=2-10:43:03 TimeLimit=14-00:00:00 TimeMin=N/A
   SubmitTime=2024-12-10T15:41:44 EligibleTime=2024-12-10T15:41:44
   AccrueTime=2024-12-10T15:41:44
   StartTime=2024-12-10T16:04:36 EndTime=2024-12-24T16:04:36 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2024-12-10T16:04:36 Scheduler=Backfill
   Partition=compute AllocNode:Sid=bp1-login01:511021
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=bp1-compute194
   BatchHost=bp1-compute194
   NumNodes=1 NumCPUs=64 NumTasks=64 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=64,mem=6400M,node=1,billing=64
   AllocTRES=cpu=64,mem=6400M,node=1,billing=64
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=1 MinMemoryCPU=100M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/user/work/sp13328/active-drying/sub3d.sh
   WorkDir=/user/work/sp13328/active-drying
   StdErr=/user/work/sp13328/active-drying/slurms/slurm-10852370.out
   StdIn=/dev/null
   StdOut=/user/work/sp13328/active-drying/slurms/slurm-10852370.out
   Power=
   TresPerTask=cpu:1
```
:::

::: {.column width="50%"}
```
scontrol show job 10864052
JobId=10864052 JobName=active_drying
   UserId=sp13328(275000) GroupId=phys(1364) MCS_label=N/A
   Priority=10325 Nice=0 Account=phys030385 QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=16:01:00 TimeLimit=14-00:00:00 TimeMin=N/A
   SubmitTime=2024-12-13T03:02:37 EligibleTime=2024-12-13T03:02:37
   AccrueTime=2024-12-13T03:02:37
   StartTime=2024-12-13T03:02:38 EndTime=2024-12-27T03:02:38 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2024-12-13T03:02:38 Scheduler=Main
   Partition=compute AllocNode:Sid=bp1-login01:1994760
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=bp1-compute147
   BatchHost=bp1-compute147
   NumNodes=1 NumCPUs=64 NumTasks=64 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=64,mem=6400M,node=1,billing=64
   AllocTRES=cpu=64,mem=6400M,node=1,billing=64
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=1 MinMemoryCPU=100M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/user/work/sp13328/active-drying/sub3d.sh
   WorkDir=/user/work/sp13328/active-drying
   StdErr=/user/work/sp13328/active-drying/slurms/slurm-10864052.out
   StdIn=/dev/null
   StdOut=/user/work/sp13328/active-drying/slurms/slurm-10864052.out
   Power=
   TresPerTask=cpu:1
```
:::
::::

---

Next time, I should check `scontrol show node bp1-compute<NODE-ID>` when this happens again...

# OVITO

## Coordination Analysis

To visualise a meaningful local density, I used the **Coordination Number** (the number of neighbours within a defined radius) as a property for each particle. The **Coordination Analysis** modifier in OVITO provides an approximate measure of local density by counting the neighbours within a specified cutoff radius. In my case, the neighbour cutoff radius, derived from the LAMMPS simulation setup, is $r_{\text{eff}} = 2^{1/6} \cdot \sigma + 0.3 = 1.422462$, where $0.3$ is the buffer added to the neighbour list with `neighbor 0.3 bin`.

The local density is calculated as:
$$
\rho_{\text{local}} = \frac{\text{Coordination Number}}{\frac{4}{3} \pi r_{\text{eff}}^3},
$$
where $r_{\text{eff}} = 1.422462$ is the effective cutoff radius. The denominator represents the volume of a sphere with radius $r_{\text{eff}}$, corresponding to the local volume used for density estimation. In the **Compute Property** modifier, I used the following expression to calculate the density:
```plaintext
Coordination / (4/3 * pi * 1.422462^3)
```
This output property was named **Density**, as it was subsequently used for colour coding in the visualisation.

![Modifcations](images/computations/Screenshot 2024-12-11 at 18.21.27-side.png)

To visualise the density, I applied a **Color Coding** modifier to map the density property to a colour scale. Regions with high coordination numbers (dense regions) are highlighted in brighter colours, while sparse regions appear dimmer. 

The number of histogram bins in the Coordination Analysis was carefully chosen to balance resolution and computational efficiency. The number of bins is given by:
$$
\text{Number of bins} = \frac{\text{Effective cutoff radius}}{\text{Bin width}} = \frac{1.422462}{0.002} \approx 711.
$$
The choice of a bin width of $0.002$ LJ units is grounded in standard practices for molecular dynamics simulations. While the specific value is not prescribed in the literature, texts like Frenkel and Smit's *Understanding Molecular Simulation, 3rd Edition, Chapter 5.1* emphasises the importance of sufficient resolution to capture sharp features in dense systems.

A bin width of $0.002$ represents 0.2% of the particle size, which is sufficient to resolve narrow features, such as the first peak in the RDF. This strikes a balance between resolution and computational cost. A smaller bin width, such as $0.001$, would provide even higher resolution but require significantly more memory and computational resources (unless I'm studying very sharp features, such as crystalline structures). Conversely, a bin width of $0.005$ would reduce the number of bins to approximately 284, which I found empirically oversmooths the RDF in dense systems like mine ($\rho = 0.75$).