---
title: Weekly Notes
toc: true
bibliography: references/quarto.bib
reference-location: margin
citation-location: margin
---

## Week 3

### 2024/10/2 Why are the clouds at the same height when I look at the cloudy sky?

![View from Brandon Hill](images/notes/View%20from%20Brandon%20Hill.jpg)

When we observe clouds appearing at the same height, it's often due to a phenomenon where a particular layer of the atmosphere has the right conditions for cloud formation. In the troposphere (the lowest layer of the atmosphere), clouds form when the air cools to its dew point, causing water vapour to condense into droplets or ice crystals. This typically happens at specific altitudes where temperature and pressure conditions are ideal for condensation.

Clouds that seem to form at the same height are likely part of the same atmospheric layer, known as a *cloud base*. The cloud base marks the altitude at which rising air reaches its dew point. If the conditions across the sky are uniform, we’ll see many clouds forming at roughly the same altitude, giving the illusion of a flat layer of clouds.

Cloud formation is fundamentally a non-equilibrium process. It results from dynamic atmospheric conditions like rising air currents, changes in temperature and pressure, and the continuous exchange of energy. These factors drive processes such as condensation and evaporation, which are inherently out of equilibrium. This aligns with the behaviour of active matter systems, where each particle consumes energy to move, keeping the system perpetually out of equilibrium.

To create an effective repulsion in our ABP simulations, we consider modifying the wall-fluid interactions to induce a torque that reorients particles away from the surface. One approach might be to introduce an anisotropic interaction potential near the wall. When an ABP approaches the surface, this potential could apply a torque that turns the particle's propulsion direction away from the wall, effectively reducing its tendency to accumulate there. 

### 2024/10/5 Thesis writing (from LaTeX document)

**An idea of beginning the introduction of phase transitions**

This thesis is ultimately about phase transitions. The central question is: what happens during a transition, for example, from water to ice, from a fluid to a superfluid, or from a paramagnet to a ferromagnet? The physics of such transitions is rich and complex. There are a number of different theoretical tools available to understand them. The study begins by examining thermodynamics and extends to the application of statistical mechanics to these transitions. Additionally, statistical field theory, a fully-fledged field theory similar to what is seen in particle physics or general relativity, will be applied. The goal is to use these approaches, along with concepts of symmetry and other properties, to understand different phases of matter.

For a long time, understanding phase transitions was a mystery, especially what's known as a critical point. It's an interesting situation: we have something as familiar as water, and it was not until the mid-20th century that significant progress was made. The phase behaviour of water can be examined using a pressure-temperature diagram:

![Phase diagram of water [@cmglee_phase_diagram]](images/notes/Phase_diagram_of_water_simplified.svg)

- On the horizontal axis is temperature, and on the vertical axis is pressure.
- The phase diagram for water features three regions: steam, water, and ice.
- There's a point called the triple point, where all three phases coexist, and a critical point, which marks the end of the liquid-gas boundary.

Water is essential, as most of us are mostly water, and yet the physics of what happens at this critical point wasn't understood for the longest time. It's a system that is incredibly close to us, yet to understand it requires some of the deepest ideas in physics, particularly the renormalisation group.

In this sense, phase transitions form a uniquely positioned subject. They are simultaneously very close to our everyday experience and yet require extremely deep ideas to understand properly. Most other areas of physics are typically far removed from daily life. Quantum mechanics operates on microscopic scales, and astrophysics on cosmological scales, both far from what can be intuitively grasped. But here, some really deep, beautiful puzzles are found in the world around us.

**An idea of beginning the introduction of renormalisation group**

So we're finally at the point now where we can look at the renormalisation group, or RG. We have gathered a bunch of pieces of evidence. Here's our phase diagram; we have a critical point here. What have we seen near a critical point? We've seen that the correlation length goes to infinity; length scales diverge.

We've also seen the idea of universality, where different systems (for example, the liquid-gas phase transition and the Ising model) are the same. So we've observed that the correlation length $\xi$ goes to infinity, and many systems become similar near the critical point.

The renormalisation group is an idea that arises from these two observations. What's really happening is that at the critical point, the system becomes scale-invariant. Suppose I have a physical system, and these are sort of blobs, say, of up spins or some kind of physical structure. If the correlation length is infinite, there are structures at all length scales. If I zoom out, if I coarse-grain the system, it should look similar. Because if I'm rescaling my system, and if the correlation length is finite, by zooming out, the correlation length of my new system becomes smaller. In my rescaled system, after I've zoomed out, things that were on long scales become short scales. As a result, the correlation length, which was on long scales, is now on short scales; the correlations become smaller.

But if the correlation length was infinite, when I zoomed out and rescaled my system, it would still look infinite. So what we see is that when I zoom out and rescale my system, I should see the same thing if the correlation length goes to infinity. And what this means immediately is that universality emerges. Because if I do that—if I have a system where I zoom out, rescale, and it looks the same, zoom out, rescale, and it looks the same—near the critical point *only*, then all the microscopic details of my system just fall away. What I'm left with is some kind of *universal theory*, a universal theory of physics which doesn't depend on the details of my model. So the fact that it was a liquid-gas transition or a magnet doesn't make any difference to me. The only things that matter are the basic ideas of dimensionality and symmetry that underlie the transition, and everything else is irrelevant at the critical point *because* the correlation length diverges.

The renormalisation group is a way to implement this mathematically or in an actual model. So the whole idea is to take your system near criticality, rescale it, and study how it changes under these rescalings. What we hope to find are fixed points of the rescaling—so where you zoom out, and the system looks the same—and these fixed points become critical points of the system. The fixed points of the RG transformations correspond to critical points of the physical system. From this, you can derive the critical exponents from the sort of scaling near the critical points.

So this is the idea of the renormalisation group, and you can see how it emerges naturally and explains basically everything. There's this idea that fluctuations become very important, and the correlation length diverges, and therefore we can look at this rescaling transformation. In this sense, the theory gives us the non-mean-field critical exponents, and we also see this universality emerge.

So the renormalisation group is really an idea that ties a lot of puzzles together, a lot of systems together, and it's sufficiently important that it was awarded a Nobel Prize for pure condensed matter theory, which is a rare achievement for Ken Wilson in the 1970s.

That's the basic idea of the renormalisation group. The problem ultimately with RG is that to implement it in anything other than the simplest of systems is a technical nightmare. It's extremely difficult, takes a long time, and the results are very hard to obtain. However, people have done that, and it's correct—it works; it's just very hard. So that's the idea, the philosophy of the RG, and it's one of the most beautiful ideas in theoretical physics.

## Week 4

### 2024/10/8 Analytical Mechanics

I've been watching Tom's lectures on Analytical Mechanics once more. I’ve just finished part 2. The topics are:

1. Calculus of variations
2. Lagrangian mechanics
3. Hamiltonian mechanics

Why did I start watching it?

Well, during the last project meeting, I had a naïve question about whether we could formulate a Lagrangian for the drying scenario. For example, in my supervisor's papers, they couldn't manage to observe the drying transition in their setup. So, I asked if, when the trajectory of all particles is away from the surface, we could possibly derive an analytical equation of forces between the particles and the wall to induce torque. Just a pure naïve idea.

Nigel mentioned that the principle of least action makes sense in equilibrium settings, so I had to revise that concept. The Lagrangian is the kinetic energy minus the potential energy, whatever this quantity is. In non-equilibrium systems, energy keeps changing since active agents continuously consume energy. Also, the effective potential depends on the properties of the wall. So, the Lagrangian would become time-dependent, which now makes sense based on what Nigel said.

The beautiful theorem was revisited from his lectures: **Noether's theorem**.

- If the laws of physics don't depend on time, energy is conserved.
- Similarly, if the system has translational or rotational invariance, momentum or angular momentum is conserved, respectively.

This is beautiful: conserved quantities derive from the symmetries of the system. It also implies that since energy is *not* conserved for the system I'm interested in, the equations of motion would be time-dependent. I think it would be useful to pay attention to how the equations of motion for active Brownian particles are formulated once I revise their papers.

As for his lectures, I’ll continue with part 3 since I've already watched the first two. That said, I remember his lecture notes being better than the video recordings, although his video lectures on phase transitions were much more inspiring. So, a note for my future self: when revising the content of the analytical formulation of mechanics, read his lecture notes.

---

#### Programs I made

Before doing that, I made a Python program that:

1. downloads all lectures from Mediasite from the unit I select,
2. detects and removes sections of silence in both the video and audio tracks, applying a buffer of 0.5 seconds before and after each silence to make the transitions smoother,
3. syncs the video and audio together seamlessly to create an mp4 file (the original files were mov, which were almost 10x larger in size),
4. keeps the system active throughout the process by using `caffeinate` to prevent the computer from going to sleep,
5. tracks and reports the download status, ensuring that any interrupted downloads are handled smoothly by automatically updating cookies for authentication,
6. ensures frame rate consistency to avoid any sync issues that might arise due to different frame rates,
7. automatically manages temporary files created during the silence detection process, keeping the system clean after the task is done.

I'm not sure if I'm allowed to share this code on GitHub, but it was a fun project.

![Downloaded lectures](images/notes/Screenshot%202024-10-08%20at%2017.47.53.png)

Why did I make it? Because...

- I’ve always felt lazy logging into Blackboard (the university website) every time I wanted to watch videos during my undergraduate years.
- I personally wanted to collect the lectures before they disappeared from the website.
- There are many pauses in the lectures where the lecturer doesn't speak while writing equations or words. Removing these pauses also saves total viewing time. Observe the difference:

**Total time to watch all videos of AM (Analytical Mechanics):**

- AM: 9 hours, 18 minutes, 34 seconds
- Processed_AM: 8 hours, 23 minutes, 50 seconds
- Processed_AM (no buffer of 0.5s): 7 hours, 23 minutes, 28 seconds

**Total time to watch all videos of PT (Physics of Phase Transitions):**

- PT: 11 hours, 42 minutes, 19 seconds
- Processed_PT: 10 hours, 49 minutes, 18 seconds
- Processed_PT (no buffer of 0.5s): 9 hours, 45 minutes, 29 seconds

The buffer is necessary as it makes the transitions feel more natural by introducing a brief pause rather than cutting the silences completely. Watch an example below (duration change: 18:09 (before processing) → 15:29 (after processing with buffers)):

![AM 2.5a Noether's Theorem](videos/notes/processed_AM 2.5a Noether's Theorem.mp4)

---

A beautiful quote from Tom: 

*From AM 3.4, classical mechanics is the geometry of phase space. All of the geometries (symplectic form, canonical transformations, etc.) describe the phenomenology of classical mechanics.*

</details>

## Week 8

### 2024/11/8 Active Brownian Particles (ABPs)

I now understand why ABPs drive the system out of equilibrium. I love deriving everything mathematically from first principles.

Things I have understood:

- The equations of motion for ABPs are described by overdamped Langevin equations.
- Persistent fluxes of particles prevent the detailed balance condition; therefore, there is no Boltzmann distribution.

I'm still uncertain about the concept of entropy production. When I finish reading *Physics and Beyond*, I'll start reading *Understanding Molecular Simulation* again, as it has a chapter that discusses this concept.

These are the PDF notes that I wrote: [My handwritten notes](documents/241108 Active Brownian Particles.pdf)

## Week 9

### 2024/11/13 Mathpix and Stochastic Processes

Francesco introduced a tool called [Mathpix](https://mathpix.com), which looks incredibly useful for converting handwritten notes from Goodnotes into formal documents!

He also shared his draft notes on random walks and stochastic processes: [Handbook of Particle-Based Simulation of Fluids](https://atooms.frama.io/handbook/html/contents/rw.html), which I can download as `.ipynb` files to experiment with the code.

I also received a very warm, thoughtful email from Nigel, my supervisor. Since I’d like to reread it often, I want to preserve it here with password protection:

```{=html}
<div id="protected-content" style="display:none; white-space: pre-wrap; font-family: 'Lato', Courier, monospace; padding: 20px; border: 1px solid #ccc; background-color: #fdf7e4; border-radius: 5px;">
  <p>
Hi Sohyun,

Thanks for your messages. (I thought I’d move things to email as it is easier for longer conversations).

Happy to talk about your worries at the next meeting. I would just say at the start that I am not currently concerned about your progress. It is normal at the start of a PhD to spend quite a lot of time “getting the lay of the land”. We appreciate that with such a lot of new ideas and concepts that things can sometimes feel overwhelming, and it can be difficult to separate the wood from the trees. Please be reassured that this is normal.

I would also say that it is very positive that you are engaged with a good number of complex concepts and trying to understand them in your own terms. The students who don’t make progress are usually the ones who are not engaged, so again I think you can be reassured.

When it comes to reading papers/books that we have suggested, I just wanted to comment that the way most scientists absorb information is usually not by sitting down, reading a paper from cover to cover and not putting it down until they understand every detail. If we did that, we wouldn’t get anything done. Rather we try to first get a feel for roughly what the message of the paper is talking about, the main ideas if you like, and very often that is as far as it goes. If there is something that we find particularly interesting or relevant we might go into more detail, but I would say that that is the exception. So, if we put papers on Zotero and suggest you look at them, we don’t mean that you need to read and understand everything in great detail. In most cases you might just want to get a rough idea of what a paper is about (AI can help here) and build up some familiarity with the types of approaches and language that are used in the field.

Another comment I would make is that it is not always necessary as a PhD student to be able to derive from first principles the methods and results that underpin your approach. Of course, we need to feel reasonably confident that we are using tried and tested techniques for which the strengths and weakness are known. But often the methods themselves might be based on decades of work drawn from vast fields of literature. If one were not to try to make progress until all the foundations had been thoroughly understood, then there would be little time left to measure things. I think it is quite right and proper that you want to understand where the Langevin or ABP equations of motion comes from and how this translates into a time stepping algorithm for evolving the system. But I don’t think one necessarily needs to go all the way back to Einstein’s original papers to do this.  You told me (I think in the context of your project last year) that you felt the need to understand all the foundations before making progress on the project. I would just comment that often we learn ‘on the job’, so to say, ie we might start calculating or measuring without understanding completely what we are doing or why, but the process of ‘doing’ builds that understanding.

Anyway, I hope these comments help and we can certainly discuss further next week.

Best wishes,

Nigel
  </p>
</div>

<div id="password-prompt">
  <label for="password">Type password to reveal the email:</label>
  <input type="password" id="password">
  <button onclick="checkPassword()">Submit</button>
</div>

<script>
  function checkPassword() {
    const inputPassword = document.getElementById('password').value;
    const correctPassword = "7942";  // Set your password here

    if (inputPassword === correctPassword) {
      document.getElementById('protected-content').style.display = "block";
      document.getElementById('password-prompt').style.display = "none";
    } else {
      alert("Incorrect password. Try again.");
    }
  }
</script>
```

### 2024/11/14 Reading Francesco's notes

![On One dimensional motion and its statistics, Random Walks](images/Particle-Based Simulation of Fluids/Screenshot 2024-11-14 at 19.30.16.png)

Derivations: [My handwritten notes](documents/241116 Random Walks 1.pdf)

Still not sure how $l^2$ came in the equations...

### 2024/11/16 Watching Biophysics Lectures {#sec-biophysics-lectures}

![Welcome page](images/notes/Biophysics Lectures/Screenshot 2024-11-16 at 12.32.43.png)
![Welcome page - Schedule](images/notes/Biophysics Lectures/Screenshot 2024-11-16 at 12.31.33.png)

I decided to watch lectures from weeks 1, 2, 4, and 5.

---

![Lecture, Monday October 3rd](images/notes/Biophysics Lectures/Screenshot 2024-11-16 at 13.13.22 (2).png)

This was quite interesting, as it implies that the duality of the self and the world subsists in the possibility of having the same copy of DNA (which is also unique to myself compared to others, even though the similarity in DNA is striking across humans: approximately 99.9% of the DNA sequence is the same between any two individuals). For example, the clothes I am wearing now do not share the same DNA.

However, after finishing week 1, it wasn’t very exciting. I would like to work with (idealised) mathematical objects that have relevance to empirical reality, not specific details about how DNA is formed, which chemicals and bonds are involved, or what functions it performs. When I was learning astrophysics, I remember only liking cosmology out of all the astrophysics fields because the other fields felt like the biology of stars, planets, or galaxies. It seems I’m not very interested in these phenomenological descriptions.

---

![Lecture, Monday October 10th](images/notes/Biophysics Lectures/Screenshot 2024-11-16 at 17.02.44 (2).png)

This is quite interesting: the protein coat (capsid) enclosing the nucleic acid of a virus is always a multiple of 60, such as 60, 120, or 180. Why this number? Why does it exhibit icosahedral symmetry?

![Lecture, Monday October 10th](images/notes/Biophysics Lectures/Screenshot 2024-11-16 at 17.12.54 (2).png)

Also, this is fascinating. Nature can achieve self-assembly, but humans cannot, no matter how much we know about the individual ingredients that make up these structures (even though the lecturer mentioned that simple structures like helices, such as the tobacco mosaic virus, can be made in the lab). Most of the lecture content here was quite engaging as it covered protein folding. I remembered writing an essay on AlphaFold for coursework while attending these lectures two years ago.

But no, as soon as the next lecture started covering the chemistry of protein folding and the formation of primary and secondary structures, it completely lost me. It took a few hours to complete one lecture because it was just not interesting.

---

![Lecture, October 19th](images/notes/Biophysics Lectures/Screenshot 2024-11-16 at 22.54.01 (2).png)

I guess that’s why I’m more interested in fungal networks than other life forms, which I find mundane...? After this quiz, there were further explanations about hydrogen bonding, van der Waals attractions, hydrophobic effects, and so on, but they were all nightmares and torturous. There were mentions of entropic costs, enthalpically favourable states, and other concepts, but without equations, it all felt hollow and uninteresting. I’m not sure if this was the lecturer’s fault or the content itself, but I hated learning about all this. No wonder I gave a score of just 2 for this unit. I wanted to stop, but I know the upcoming topics will be about self-assembly or molecular motors, so I decided to persevere... What torture!

---

It really makes me reflect on what I’ve been genuinely interested in. From my third year, I started rating how much I enjoyed each unit after finishing it, based on this scale:

- **5**: I found it very interesting  
- **4**: Interesting  
- **3**: Okay  
- **2**: Not much fun  
- **1**: I hated it  

Here are the scores for my third-year and fourth-year units:

- **5**: Quantum Physics / Physics Research Project  
- **4.5**: Particle Physics, Analytical Mechanics / The Physics of Phase Transitions  
- **4**: Methods of Theoretical Physics / Foundations of Modern Physics, General Relativity and Cosmology, Quantum Information Theory  
- **3.5**: Materials Physics
- **3**: Environmental Physics  
- **2.5**: Solid State Physics / Applied Materials Physics  
- **2**: Biophysics  

Based on this, how did I end up choosing my PhD project in active matter? There were complex reasons for choosing it, as I outlined in [2024/11/15 - Proliferating Active Matter](notes.qmd#sec-proliferating-active-matter), but honestly, I should have thought it through more carefully (even though, at the time, I believed I was being very thorough).

So, what am I really interested in? It would be the foundations of modern physics: fairly mathematical work, possibly with computational approaches. However, I’m also deeply interested in neuroscience. I thought I would find a connection by pursuing active matter projects, as studying the brain is a subset of biology. Now, I’m not so sure...

So, I looked at the programme for the conference *Statistical Physics of Cognition* (happening in 10 days) to see if there is any research linking active matter and neuroscience. Although I couldn’t find one specifically about active matter, I found one that connects with non-equilibrium statistical physics:

![[Statistical Physics of Cognition - Programme](https://iop.eventsair.com/spc2024/programme)](images/notes/Biophysics Lectures/Screenshot 2024-11-17 at 01.20.37.png)

It appears that the researcher has written several papers on this topic with William Bialek, who authored a seminal paper on applying the maximum entropy principle to elucidate collective behaviour in retinal neuronal networks (see @schneidman2006weak), which inspired my MSci research project.

The abstract of the first paper (@lynn2022emergence):

> Living systems are fundamentally irreversible, breaking detailed balance and establishing an arrow of time. But how does the evident arrow of time for a whole system arise from the interactions among its multiple elements? We show that the local evidence for the arrow of time, which is the entropy production for thermodynamic systems, can be decomposed. First, it can be split into two components: an independent term reflecting the dynamics of individual elements and an interaction term driven by the dependencies among elements. Adapting tools from nonequilibrium physics, we further decompose the interaction term into contributions from pairs of elements, triplets, and higher-order terms. We illustrate our methods on models of cellular sensing and logical computations, as well as on patterns of neural activity in the retina as it responds to visual inputs. We find that neural activity can define the arrow of time even when the visual inputs do not, and that the dominant contribution to this breaking of detailed balance comes from interactions among pairs of neurons.

The abstract of the second paper (@lynn2022decomposing):

> We show that the evidence for a local arrow of time, which is equivalent to the entropy production in thermodynamic systems, can be decomposed. In a system with many degrees of freedom, there is a term that arises from the irreversible dynamics of the individual variables, and then a series of non-negative terms contributed by correlations among pairs, triplets, and higher-order combinations of variables. We illustrate this decomposition on simple models of noisy logical computations, and then apply it to the analysis of patterns of neural activity in the retina as it responds to complex dynamic visual scenes. We find that neural activity breaks detailed balance even when the visual inputs do not, and that this irreversibility arises primarily from interactions between pairs of neurons.

## Week 10

### 2024/11/19 Continuing to Watch Biophysics Lectures

![Lecture, Monday October 24th](images/notes/Biophysics Lectures/Screenshot 2024-11-19 at 10.10.06 (2).png)

Interesting!

---

![Lecture, Wednesday October 26th](images/notes/Biophysics Lectures/Screenshot 2024-11-19 at 11.01.33 (2).png)

*There is a whole branch of physics called soft matter, which investigates viscoelastic types of materials. At Harvard, there is a seminar series called Squishy Physics, which describes it quite well. And, of course, biological materials form the heart of soft matter.*

---

![Lecture, Wednesday October 26th](images/notes/Biophysics Lectures/Screenshot 2024-11-19 at 11.40.46 (2).png)

The image source: [Microtubules: the basics](https://www.nature.com/scitable/content/microtubules-the-basics-14673338/)

---

![Lecture, Wednesday October 26th](images/notes/Biophysics Lectures/Screenshot 2024-11-19 at 11.55.45 (2).png)

![Immune System Science GIF](images/notes/Biophysics Lectures/immune-system-science.gif)

This lecture (Wednesday October 26th) has been particularly interesting, as it delved into the dynamics of cells. Topics included microtubules growing and collapsing to induce cell movement, how blood cells squeeze through capillaries (this was adorable! Although I think it was mentioned in the previous lecture), and connections to soft matter physics, where I feel most aligned. There were also discussions about self-assembly, which were very interesting.

So, the topics that interest me in biophysics are cell dynamics, self-assembly (and disassembly).

### 2024/11/21 A liquid droplet

Suddenly, a strange thought occurred, which I was trying to phrase to ask my supervisor:

> I’ve been thinking about the shape of a liquid droplet on a surface, particularly how it changes with wetting states. On a superhydrophobic surface, the droplet appears almost perfectly spherical because it minimises contact with the surface. As the surface becomes less hydrophobic, transitioning to a partially wetting state, the droplet flattens into a spherical cap shape. I was wondering if this change in shape could be thought of as a conceptual process where the spherical droplet 'expands,' exposing less of its 'head' above the surface as it flattens. In the extreme limit of complete wetting, it could correspond to a large sphere overlaying the surface, with only a tiny portion visible above it. Has anyone investigated this kind of analogy or thought about the wetting transition in a similar way? 

I have derived some expressions in these [handwritten notes](documents/241121 Liquid Droplet.pdf), but I'm not sure where to go with this.

## Week 11

### 2024/11/26 What if we coarse-grain to introduce the dominance of inertial terms?

In our models of ABPs, we have a self-propulsion force that drives the motility. One thought occurred suddenly. Our models effectively ignore the inertial terms, dictated by the overdamped Langevin equation, and wet the surface. On contrary, for purely repulsive particles banging on the hard wall, it also dries the surface. Then, when we have a cluster forming due to motility-induced phase separation, there must arise an overall momentum current stemming from the collective behaviour of active particles. This collective matter can then be thought of as being in an underdamped situation, where inertial terms could emerge through coarse-graining. It's 2am, so I'm probably thinking of something silly, but in my intuition, strong inertial terms could lead to a possible drying scenario by overcoming surface trapping mechanisms. 

The overdamped assumption is deeply tied to the low Reynolds number regime typical of ABPs. Even at the collective level, the viscosity of the medium might suppress inertial effects. I'm uncertain whether the collective velocity field is sufficiently strong to overcome this dissipation...