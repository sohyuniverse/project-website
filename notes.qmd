---
title: Weekly Notes
toc: true
bibliography: references/quarto.bib
reference-location: margin
citation-location: margin
---

## Week 2

### 2024/9/27 Things I did so far

Busy weeks due to starting three teaching modules: [Weekly update of my TSR (Teaching Support Roles)](onenote:Overview.one#Links&section-id={D4B227EF-F9FF-864E-A042-9B9A7C2E06E3}&page-id={B7874627-D52D-5043-9E1A-2067E68FAD02}&object-id={537C7D03-2CEB-3142-8063-0EF93BF5A5E9}&25&base-path=https://uob-my.sharepoint.com/personal/sp13328_bristol_ac_uk/Documents/PhD%20in%20Active%20Matter). Looking back, I wish I could have completed more reading, as there isn’t much to talk about in terms of active matter research for the next project meeting on Monday.

## Week 3
### 2024/9/30 Weekly project meeting

#### Things to discuss

1. Meeting with Max on Friday at noon (4th of Oct), thinking of going to Budapest Café

2. ‘Setting Expectations’ document

3. Two conferences to join:

    • [The Dao of Complexity workshop](https://iop.eventsair.com/doc2024)

    • [The Statistical Physics of Cognition](http://complexity-physics.org/blog/2024/08/19/the-statistical-physics-of-cognition)

    So, a trip to London, how to sort out things with Clarity, and other arrangements.

4. Set up [RDSF](https://uob.sharepoint.com/sites/itservices/SitePages/filestores.aspx) data storage
(Though I checked, OneDrive for Business offers 2TB of storage: [Overview of OneDrive for Business](https://uob.sharepoint.com/sites/systemsupport/SitePages/onedrive-overview.aspx))

5. Brief plan discussion: reading Mary Coe's thesis, then *Understanding Molecular Simulation* book

6. Are there MSci students working on this project?
![MSci project proposal](images/notes/ProjectProposalsMsci2024-112.jpg)

### 2024/10/2 Why are the clouds at the same height when I look at the cloudy sky?

![View from Brandon Hill](images/notes/View%20from%20Brandon%20Hill.jpg)

When we observe clouds appearing at the same height, it's often due to a phenomenon where a particular layer of the atmosphere has the right conditions for cloud formation. In the troposphere (the lowest layer of the atmosphere), clouds form when the air cools to its dew point, causing water vapour to condense into droplets or ice crystals. This typically happens at specific altitudes where temperature and pressure conditions are ideal for condensation.

Clouds that seem to form at the same height are likely part of the same atmospheric layer, known as a *cloud base*. The cloud base marks the altitude at which rising air reaches its dew point. If the conditions across the sky are uniform, we’ll see many clouds forming at roughly the same altitude, giving the illusion of a flat layer of clouds.

Cloud formation is fundamentally a non-equilibrium process. It results from dynamic atmospheric conditions like rising air currents, changes in temperature and pressure, and the continuous exchange of energy. These factors drive processes such as condensation and evaporation, which are inherently out of equilibrium. This aligns with the behaviour of active matter systems, where each particle consumes energy to move, keeping the system perpetually out of equilibrium.

To create an effective repulsion in our ABP simulations, we consider modifying the wall-fluid interactions to induce a torque that reorients particles away from the surface. One approach might be to introduce an anisotropic interaction potential near the wall. When an ABP approaches the surface, this potential could apply a torque that turns the particle's propulsion direction away from the wall, effectively reducing its tendency to accumulate there. 

### 2024/10/5 Thesis writing

**An idea of beginning the introduction of phase transitions**

This thesis is ultimately about phase transitions. The central question is: what happens during a transition, for example, from water to ice, from a fluid to a superfluid, or from a paramagnet to a ferromagnet? The physics of such transitions is rich and complex. There are a number of different theoretical tools available to understand them. The study begins by examining thermodynamics and extends to the application of statistical mechanics to these transitions. Additionally, statistical field theory, a fully-fledged field theory similar to what is seen in particle physics or general relativity, will be applied. The goal is to use these approaches, along with concepts of symmetry and other properties, to understand different phases of matter.

For a long time, understanding phase transitions was a mystery, especially what's known as a critical point. It's an interesting situation: we have something as familiar as water, and it was not until the mid-20th century that significant progress was made. The phase behaviour of water can be examined using a pressure-temperature diagram:

![Phase diagram of water [@cmglee_phase_diagram]](images/notes/Phase_diagram_of_water_simplified.svg)

- On the horizontal axis is temperature, and on the vertical axis is pressure.
- The phase diagram for water features three regions: steam, water, and ice.
- There's a point called the triple point, where all three phases coexist, and a critical point, which marks the end of the liquid-gas boundary.

Water is essential, as most of us are mostly water, and yet the physics of what happens at this critical point wasn't understood for the longest time. It's a system that is incredibly close to us, yet to understand it requires some of the deepest ideas in physics, particularly the renormalisation group.

In this sense, phase transitions form a uniquely positioned subject. They are simultaneously very close to our everyday experience and yet require extremely deep ideas to understand properly. Most other areas of physics are typically far removed from daily life. Quantum mechanics operates on microscopic scales, and astrophysics on cosmological scales, both far from what can be intuitively grasped. But here, some really deep, beautiful puzzles are found in the world around us.

**An idea of beginning the introduction of renormalisation group**

So we're finally at the point now where we can look at the renormalisation group, or RG. We have gathered a bunch of pieces of evidence. Here's our phase diagram; we have a critical point here. What have we seen near a critical point? We've seen that the correlation length goes to infinity; length scales diverge.

We've also seen the idea of universality, where different systems—for example, the liquid-gas phase transition and the Ising model—are the same. So we've observed that the correlation length $\xi$ goes to infinity, and many systems become similar near the critical point.

The renormalisation group is an idea that arises from these two observations. What's really happening is that at the critical point, the system becomes scale-invariant. Suppose I have a physical system, and these are sort of blobs—say, of up spins or some kind of physical structure. If the correlation length is infinite, there are structures at all length scales. If I zoom out, if I coarse-grain the system, it should look similar. Because if I'm rescaling my system, and if the correlation length is finite, by zooming out, the correlation length of my new system becomes smaller. In my rescaled system, after I've zoomed out, things that were on long scales become short scales, and so the correlation length—which was on long scales—is now on short scales; the correlations become smaller.

But if the correlation length was infinite, when I zoomed out and rescaled my system, it would still look infinite. So what we see is that when I zoom out and rescale my system, I should see the same thing if the correlation length goes to infinity. And what this means immediately is that universality emerges. Because if I do that—if I have an idea where I zoom out, rescale, and it looks the same, zoom out, rescale, and it looks the same—near the critical point *only*, then all the microscopic details of my system just fall away. What I'm left with is some kind of *universal theory*, a universal theory of physics which doesn't depend on the details of my model. So the fact that it was a liquid-gas transition or a magnet doesn't make any difference to me. The only things that matter are the basic ideas of dimensionality and symmetry that underlie the transition, and everything else is irrelevant at the critical point *because* the correlation length diverges.

The renormalisation group is a way to implement this mathematically or in an actual model. So the whole idea is to take your system near criticality, rescale it, and study how it changes under these rescalings. What we hope to find are fixed points of the rescaling—so where you zoom out, and the system looks the same—and these fixed points become critical points of the system. The fixed points of the RG transformations correspond to critical points of the physical system. From this, you can derive the critical exponents from the sort of scaling near the critical points.

So this is the idea of the renormalisation group, and you can see how it emerges naturally and explains basically everything. There's this idea that fluctuations become very important, and the correlation length diverges, and therefore we can look at this rescaling transformation. In this sense, the theory gives us the non-mean-field critical exponents, and we also see this universality emerge.

So the renormalisation group is really an idea that ties a lot of puzzles together, a lot of systems together, and it's sufficiently important that it was awarded a Nobel Prize for pure condensed matter theory, which is a rare achievement for Ken Wilson in the 1970s.

That's the basic idea of the renormalisation group. The problem ultimately with RG is that to implement it in anything other than the simplest of systems is a technical nightmare. It's extremely difficult, takes a long time, and the results are very hard to obtain. However, people have done that, and it's correct—it works; it's just very hard. So that's the idea, the philosophy of the RG, and it's one of the most beautiful ideas in theoretical physics.

## Week 4
### 2024/10/7 Weekly project meeting

#### Things to discuss

1. Overleaf template for the thesis, as I will start writing what I learn for either the introduction or theory sections. Found one: [Link](https://www.overleaf.com/latex/templates/university-of-bristol-thesis-template/kzqrfvyxxcdm)

### 2024/10/8 Analytical Mechanics

I've been watching Tom's lectures on Analytical Mechanics once more. I’ve just finished part 2. The topics are:

1. Calculus of variations
2. Lagrangian mechanics
3. Hamiltonian mechanics

Why did I start watching it?

Well, during the last project meeting, I had a naïve question about whether we could formulate a Lagrangian for the drying scenario. For example, in my supervisor's papers, they couldn't manage to observe the drying transition in their setup. So, I asked if, when the trajectory of all particles is away from the surface, we could possibly derive an analytical equation of forces between the particles and the wall to induce torque. Just a pure naïve idea.

Nigel mentioned that the principle of least action makes sense in equilibrium settings, so I had to revise that concept. The Lagrangian is the kinetic energy minus the potential energy, whatever this quantity is. In non-equilibrium systems, energy keeps changing since active agents continuously consume energy. Also, the effective potential depends on the properties of the wall. So, the Lagrangian would become time-dependent, which now makes sense based on what Nigel said.

The beautiful theorem was revisited from his lectures: **Noether's theorem**.

- If the laws of physics don't depend on time, energy is conserved.
- Similarly, if the system has translational or rotational invariance, momentum or angular momentum is conserved, respectively.

This is beautiful: conserved quantities derive from the symmetries of the system. It also implies that since energy is *not* conserved for the system I'm interested in, the equations of motion would be time-dependent. I think it would be useful to pay attention to how the equations of motion for active Brownian particles are formulated once I revise their papers.

As for his lectures, I’ll continue with part 3 since I've already watched the first two. That said, I remember his lecture notes being better than the video recordings, although his video lectures on phase transitions were much more inspiring. So, a note for my future self: when revising the content of the analytical formulation of mechanics, read his lecture notes.

---

#### Programs I made

Before doing that, I made a Python program that:

1. downloads all lectures from Mediasite from the unit I select,
2. detects and removes sections of silence in both the video and audio tracks, applying a buffer of 0.5 seconds before and after each silence to make the transitions smoother,
3. syncs the video and audio together seamlessly to create an mp4 file (the original files were mov, which were almost 10x larger in size),
4. keeps the system active throughout the process by using `caffeinate` to prevent the computer from going to sleep,
5. tracks and reports the download status, ensuring that any interrupted downloads are handled smoothly by automatically updating cookies for authentication,
6. ensures frame rate consistency to avoid any sync issues that might arise due to different frame rates,
7. automatically manages temporary files created during the silence detection process, keeping the system clean after the task is done.

I'm not sure if I'm allowed to share this code on GitHub, but it was a fun project.

![Downloaded lectures](images/notes/Screenshot%202024-10-08%20at%2017.47.53.png)

Why did I make it? Because...

- I’ve always felt lazy logging into Blackboard (the university website) every time I wanted to watch videos during my undergraduate years.
- I personally wanted to collect the lectures before they disappeared from the website.
- There are many pauses in the lectures where the lecturer doesn't speak while writing equations or words. Removing these pauses also saves total viewing time. Observe the difference:

**Total time to watch all videos of AM (Analytical Mechanics):**

- AM: 9 hours, 18 minutes, 34 seconds
- Processed_AM: 8 hours, 23 minutes, 50 seconds
- Processed_AM (no buffer of 0.5s): 7 hours, 23 minutes, 28 seconds

**Total time to watch all videos of PT (Physics of Phase Transitions):**

- PT: 11 hours, 42 minutes, 19 seconds
- Processed_PT: 10 hours, 49 minutes, 18 seconds
- Processed_PT (no buffer of 0.5s): 9 hours, 45 minutes, 29 seconds

The buffer is necessary as it makes the transitions feel more natural by introducing a brief pause rather than cutting the silences completely. Watch an example below (duration change: 18:09 (before processing) → 15:29 (after processing with buffers)):

![AM 2.5a Noether's Theorem](videos/notes/processed_AM 2.5a Noether's Theorem.mp4)

---

A beautiful quote from Tom: 

*From AM 3.4, classical mechanics is the geometry of phase space. All of the geometries (symplectic form, canonical transformations, etc.) describe the phenomenology of classical mechanics.*

### 2024/10/12 Making a presentation

[241016 Overleaf Presentation for Project Meeting](https://www.overleaf.com/project/670f722528944d7d9b10009d)

</details>

## Week 6

### 2024/10/23 Molsim-2025 School Application

[MolSim-2025](https://www.cecam.org/workshop-details/molsim-2025-1335) is 2-week school that offers a program focusing on basic and advanced molecular simulation methods. It targets PhD candidates and postdoctoral researchers who want to understand and apply these methods to study topics in physics, chemistry or biology. The program has an integrated and balanced structure of lectures and hands-on exercises. It offers a recap of the basic theory relevant for molecular simulation, followed by a thorough covering of basic and advanced simulation techniques. All techniques will be accompanied by illustrative examples of present day research.

#### My application (submitted on 241023)

I am a PhD student in theoretical and computational physics at the University of Bristol, specialising in the simulation of surface phase transitions with a focus on active matter using high-performance computing. My supervisors are Nigel Wilding, who has strongly recommended this winter school and has done substantial work on developing efficient algorithms for MC simulations [1-4], and Francesco Turci, who specialises in disordered systems. My initial project involves investigating drying (surface) phase transitions using Monte Carlo and molecular dynamics techniques, introducing an additional parameter to how they approached the problem in PRL 2021 [5].

I am highly motivated to join this winter school since it is my first year of PhD studies and also my first time learning how to conduct molecular simulations. I am currently reading the book *Understanding Molecular Simulation* to grasp the methodologies, and I was very excited by the fact that the authors of this book will be present for the lectures. Even though this book is enjoyable to read, many concepts within it are quite new to me, so it will be a great opportunity to deepen my knowledge in this area and meet colleagues with similar interests. The latter reason is also very important since I am the only PhD student learning models of active particles for surface phase transitions in my department.

Please let me know if you would like me to send my CV. For now, I have attached my personal statement, which I submitted for my PhD application, showing my dynamic and multifaceted experience. I hope this wonderful opportunity will materialise.

- [1] N.B. Wilding, A.D. Bruce, Density fluctuations and field mixing in the critical fluid, J. Phys. Condens. Matter 4 (12) (1992) 3087–3108
- [2] N.B. Wilding, Critical point and coexistence curve properties of the Lennard-Jones fluid: a finite-size scaling study, Phys. Rev. E 52 (1) (1995) 602–611
- [3] A.D. Bruce, N.B. Wilding, G.J. Ackland, Free energy of crystalline solids: a lattice-switch Monte Carlo method, Phys. Rev. Lett. 79 (16) (1997) 3002–3005
- [4] S. Consta, N.B. Wilding, D. Frenkel, Z. Alexandrowicz, Recoil growth: an efficient simulation method for multi-polymer systems, J. Chem. Phys. 110 (6) (1999) 3220–3228
- [5] F. Turci, N.B. Wilding, Phase separation and multibody effects in three-dimensional active Brownian particles, Phys. Rev. Lett. 126.3 (2021) 038002.

## Week 7

### 2024/10/28 Weekly project meeting

#### Things to discuss

1. Ask about page 51 on *Molecular Dynamics Simulation* showing notes written on Zotero.

## Week 8

### 2024/11/4 Weekly project meeting

#### Things to discuss (written on 11/2)

**The meaning of our project**

I'm afraid to say I'm not exactly certain about the importance of our initial project. To me, a naive student who has just finished my undergraduate in physics, physics operates by explaining experimental results in mathematical language. As a few examples,

- In the early 20th century, we realised that classical physics could not explain the ultraviolet catastrophe. So Planck suggested that light exists in quanta to explain the spectra we observe, which later led to Bohr's theory of electron orbits. Furthermore, when classical electrodynamics dictated that an orbiting electron around the nucleus should radiate energy and spiral inward, the new language of quantum mechanics was formulated to explain the stability of atoms. This process of experimental results leading to theoretical explanations, which further lead to contradictions with existing theories, gives birth to new frameworks that unite our understanding.

- The Michelson–Morley experiment investigating the existence of the ether disproved their hypothesis, supporting the constancy of the speed of light, $c$. This result led Einstein to create a theory which completely changed the very meanings of our familiar, intuitive concepts: space and time. Another example of how experimental results led to the unification of our understanding; for example, magnetism can be understood as electricity in special relativity.

- Even in statistical physics, the phase diagram of water has been known for a long time, but the critical point wasn't understood until the mid to late 20th century. It was not until Ken Wilson formulated the renormalisation group to explain this by rescaling physical theories (which I find one of the most beautiful results in theoretical physics).

I have chosen the active matter project because it's a framework that makes the most sense to me, due to my keen interest in philosophy throughout the years, which has made me already familiar with the active matter paradigm shift. If I describe briefly,

- The reason that Newtonian mechanics completely altered how scientific disciplines have progressed is because I believe it gave us what it means by our 'understanding'. Before Newton, Ptolemy's circles and epicycles could fully explain the orbits of planetary bodies. However, we don't consider this as 'understanding' the underlying mechanism because what Ptolemy did was to focus only on motions. What Newton did, on the other hand, was to identify the 'cause' of motion, which is force. By introducing that concept, he was able to derive his simple equation of universal gravity, which applies to many different things, not just planetary bodies, but also projectiles, pendulums, etc. So by his theory, we were able to unite many phenomena that we observe into one unified framework.

- However, he still couldn't explain how these forces arise, so he devised what are called 'active principles'. I could elaborate further, but I shall stop here. The important thing about this new revelation was that, from the Newtonian way of thinking, we conceived matter as passive and forces as active. We thought of matter as inert, dead, inanimate. No wonder the physicalist view in biology had failed by the end of the 19th century, which led to the 'century of the gene' in the 20th century, explaining everything in terms of gene expression. However, thinking only about genes couldn't explain many things in biology such as: what is the difference between animate and inanimate things? Why are there many phenotypes from the same genotype? So what Thomas Huxley hoped for at the end of the 19th century, the physical language to explain biological matter, has resurfaced at the beginning of the 21st century with the emergence of active matter, where soft matter physicists could contribute. All in all, at Oxford University, there is the phrase: 'Biology is soft matter come alive'. I would interpret it as 'active', though.

The significance of this new way of thinking is apparent to me. However, what is not clear is how observing a drying transition by active particles contributes to our understanding of drying phenomena. The reason I kept asking about real life examples is because I wasn't sure if our results (if we observe drying) would explain the experimental results that need to be explained. If there are such cases, this would be recognised as our correct understanding of drying phenomena, which leads us to think that moving out of equilibrium statistical physics is a more reasonable direction. Since we are dealing with such limited, specific systems where liquid and vapour coexist, which is already quite rare in nature, for this project meeting, I hoped to gain more understanding of the significance of our project.

### 2024/11/4 Diary

I was quite ill last week, taking antibiotics from Monday to Saturday. While having more rest at home than in other weeks, I was thinking about a few things.

1. At first, I started thinking about why I began to study physics. It was driven by mysteries and curiosities. At that time, I didn't know that what I was going to learn were representations of empirical observations; rather, I naively thought it would be the objective truth. As I was learning physics in my undergraduate studies, I realised there are different mathematical formalisms to describe the same physical phenomena, such as Newtonian, Lagrangian, and Hamiltonian mechanics in classical physics, or matrix mechanics by Heisenberg and wave mechanics by Schrödinger in quantum mechanics. Then my approach to physics became more like, 'Which model makes the most sense to me for understanding the observed?' or 'Which models lead to more universal descriptions of all things?' Then I encountered the concept of universality: many of the microscopic details that go into the theory are irrelevant under successive renormalisation group transformations; the theory that describes the critical point is *universal*. I thought this idea was stunningly beautiful, and I thought that if I were to start my PhD, I would investigate how these patterns that interlace between different systems or physical theories emerge.
2. Then, as I was ill and thinking about many things, I reflected on what has captivated me in my undergraduate studies. The list follows like this:

- Year 1, The breakdown of the concept, simultaneity: As I was learning special relativity, I realised that the notion I thought was objective was actually subjective. The only way to know what time is is to look at the clock. This idea was profound for me, emphasising that time is not an absolute entity. Einstein's theory was so elegant in the way that it started with only two assumptions: first, the speed of light is constant based on empirical observations; second, the laws of physics are the same in all inertial frames of reference. The consequences of the theory based on these assumptions were just mind-blowing.
- Year 2, first term, quantum tunnelling: I was always fascinated by the implications of quantum mechanics even before starting my physics degree. Once I was able to explain the phenomenon of the wavefunction trapped in a finite potential well, which allows for the probability of finding a particle on the other side of the barrier, it was really mind-blowing that I clearly understood the mathematics but had no idea how to comprehend it, even though this phenomenon is the foundation of all technologies I use now.
- Year 2, second term, Maxwell's equations: The universal descriptions that entail all electromagnetic phenomena in such simple mathematical forms using vector calculus. I was struck by the beauty of these equations that I set them as a screensaver since my second year. Also, when I went to Edinburgh and saw a statue of Maxwell with his equations, I couldn't leave that place for a long time.
- Year 3, second term, the principle of least action: This whole Lagrangian formulation of classical mechanics was the clearest and the deepest representation I had encountered. The path is chosen to extremise the action, which was such a profound perspective that changed all my ways of thinking. I could go on and on about this, but in short, I loved the encounter with this concept so much that I used it as a nickname on several websites.
- Year 4, first term, the whole ideas from general relativity: This subject was of tremendous importance to me. When I decided to study physics, this was the subject I intended to study and also the reason I chose the MSci, as it was a unit for fourth-year students. I would say this is the subject which made me come to university. The idea that if we also include a time axis overlaid onto three spatial axes, an object at rest in a given reference frame moves at the speed of light along the time axis. Then, if an object moves, since the magnitude of speed is bound by the speed of light, the component that lies on the time axis decreases as it increases along the spatial axes. It was such a beautiful way of describing how time and space were interlinked. Also, the calculus of variations, which led to Lagrangian mechanics that I loved, started from considering the straight line in a global perspective. Einstein's theory started with thinking about the straight line (or rather more precisely, a vector) from a local perspective with parallel transport. The implication of this led to Einstein's field equations, which imply matter tells spacetime how to curve and spacetime tells matter how to move. These whole ideas related very closely to some of my psychedelic experiences, and I thought it was THE subject I would want to learn much more deeply in the coming years, before I met the language of the physics of phase transitions.
- Year 4, second term, the whole ideas from the physics of phase transitions: The correlation length diverging at criticality, universality classes, critical exponents telling how to rescale a physical theory—all these were so fascinating that it almost felt like meta-foundational physics, like physics of physical theory. Also, a lot of concepts extended even beyond the realm of physics, such as the critical brain hypothesis in neuroscience. This perspective of considering the structure of physical theory was like the final lesson of my undergraduate studies and the one that I wanted to pursue for the rest of my physics career. At that time, I met Nigel, my supervisor, who told me that in statistical physics, a small number of people make a great contribution to the field, unlike astrophysicists or particle physicists who are in large groups and don't get to decide the direction. That was the moment I felt that I chose the field of statistical physics, even though I have loved concepts from astrophysics (general relativity) and particle physics (quantum field theory).

3. Now I'm in week 8 of my PhD, but I can't say that I have been really engaged with my research. The biggest fault is down to me: I haven't read references thoroughly so that I can appreciate the field in general yet. I'm the person who likes to read one book over and over again (and it seems that the only professor who likes to do this is Sandu Popescu), but I am kind of overwhelmed by how many references were uploaded in Zotero and the time it takes for me to read *understanding molecular simulation*. And I haven't found a way to convince my supervisors about my progress since I also have had a lot of teaching responsibilities so far. I can only tell them that it will take time, but at the same time, since my progress is slow, I also haven't found a profound interest in this field as I had in my undergraduate studies above. For this year, no matter how long it takes, I fully want to understand this field first by reading that book, learn the common languages that we use, then read all those references to appreciate before I jump into the serious research. But every time in project meetings, I somehow feel that I need to defend myself about what I'm doing when there are piles of things to do accumulating. In short, I feel overwhelmed.

4. That was the reason that my rest has been quite different and sidetracked this time in week 7. I started reading *Physics and Beyond* written by Werner Heisenberg (see [2024/11/2 - *Physics and Beyond*](@sec-physics-and-beyond)). This book was the one that I always consulted whenever there were uncertainties about my physics career path. I am on page 108 now out of 268. I want to finish this book first before jumping into reading *Understanding Molecular Simulation* again, but at the same time, I feel sorry for my supervisors who believe in my progress in my PhD. It's all difficult now in terms of my situation.

### 2024/11/5 Being a Good Lecturer

After attending [Hermes](https://www.bristol.ac.uk/people/person/Hermes-Gadelha-d024bd20-e1b7-4c53-af70-94acac5c6425/)' lecture, I realised there are certain qualities that a very good lecturer shares. To describe him:

He was smiling, making jokes, honest, sharing life stories, and, most importantly, engaging with students at their level.

Often, when I talk with certain lecturers or fellow physics PGRs, they ramble without checking if the person listening is following. Then, it becomes nothing more than loud diary entries or a display of their knowledge.

I also felt that the language barrier was irrelevant. Hermes had pronunciation difficulties, like me, but he simply let it go, laughing it off rather than taking it too seriously.

I was quite surprised by how many qualities he actually shared with another of my favourite lecturers, Sandu Popescu. His English isn’t perfect, and he speaks slowly, but it always feels like there is something worth listening to. Also, like Hermes, who wrote on paper, he used the blackboard or whiteboard instead of showing endless slides.

Hermes made plenty of eye contact and made excellent use of pauses during his explanations, allowing students to form images in their minds for better understanding.

There was much for me to learn from his approach to lecturing. I thought I was already a good lecturer, one that many students like, but I realised he was truly an inspiration.

### 2024/11/8 Active Brownian Particles (ABPs)

I now understand why ABPs drive the system out of equilibrium. I love deriving everything mathematically from first principles.

Things I have understood:

- The equations of motion for ABPs are described by overdamped Langevin equations.
- Persistent fluxes of particles prevent the detailed balance condition; therefore, there is no Boltzmann distribution.

I'm still uncertain about the concept of entropy production. When I finish reading *Physics and Beyond*, I'll start reading *Understanding Molecular Simulation* again, as it has a chapter that discusses this concept.

These are the PDF notes that I wrote: [My handwritten notes](documents/241108 Active Brownian Particles.pdf)

## Week 9

### 2024/11/13 Mathpix and Stochastic Processes

Francesco introduced a tool called [Mathpix](https://mathpix.com), which looks incredibly useful for converting handwritten notes from Goodnotes into formal documents!

He also shared his draft notes on random walks and stochastic processes: [Handbook of Particle-Based Simulation of Fluids](https://atooms.frama.io/handbook/html/contents/rw.html), which I can download as `.ipynb` files to experiment with the code.

I also received a very warm, thoughtful email from Nigel, my supervisor. Since I’d like to reread it often, I want to preserve it here with password protection:

```{=html}
<div id="protected-content" style="display:none; white-space: pre-wrap; font-family: 'Courier New', Courier, monospace; padding: 20px; border: 1px solid #ccc; background-color: #fdf7e4; border-radius: 5px;">
  <p>
Hi Sohyun,

Thanks for your messages. (I thought I’d move things to email as it is easier for longer conversations).

Happy to talk about your worries at the next meeting. I would just say at the start that I am not currently concerned about your progress. It is normal at the start of a PhD to spend quite a lot of time “getting the lay of the land”. We appreciate that with such a lot of new ideas and concepts that things can sometimes feel overwhelming, and it can be difficult to separate the wood from the trees. Please be reassured that this is normal.

I would also say that it is very positive that you are engaged with a good number of complex concepts and trying to understand them in your own terms. The students who don’t make progress are usually the ones who are not engaged, so again I think you can be reassured.

When it comes to reading papers/books that we have suggested, I just wanted to comment that the way most scientists absorb information is usually not by sitting down, reading a paper from cover to cover and not putting it down until they understand every detail. If we did that, we wouldn’t get anything done. Rather we try to first get a feel for roughly what the message of the paper is talking about, the main ideas if you like, and very often that is as far as it goes. If there is something that we find particularly interesting or relevant we might go into more detail, but I would say that that is the exception. So, if we put papers on Zotero and suggest you look at them, we don’t mean that you need to read and understand everything in great detail. In most cases you might just want to get a rough idea of what a paper is about (AI can help here) and build up some familiarity with the types of approaches and language that are used in the field.

Another comment I would make is that it is not always necessary as a PhD student to be able to derive from first principles the methods and results that underpin your approach. Of course, we need to feel reasonably confident that we are using tried and tested techniques for which the strengths and weakness are known. But often the methods themselves might be based on decades of work drawn from vast fields of literature. If one were not to try to make progress until all the foundations had been thoroughly understood, then there would be little time left to measure things. I think it is quite right and proper that you want to understand where the Langevin or ABP equations of motion comes from and how this translates into a time stepping algorithm for evolving the system. But I don’t think one necessarily needs to go all the way back to Einstein’s original papers to do this.  You told me (I think in the context of your project last year) that you felt the need to understand all the foundations before making progress on the project. I would just comment that often we learn ‘on the job’, so to say, ie we might start calculating or measuring without understanding completely what we are doing or why, but the process of ‘doing’ builds that understanding.

Anyway, I hope these comments help and we can certainly discuss further next week.

Best wishes,

Nigel
  </p>
</div>

<div id="password-prompt">
  <label for="password">Type password to reveal the email:</label>
  <input type="password" id="password">
  <button onclick="checkPassword()">Submit</button>
</div>

<script>
  function checkPassword() {
    const inputPassword = document.getElementById('password').value;
    const correctPassword = "7942";  // Set your password here

    if (inputPassword === correctPassword) {
      document.getElementById('protected-content').style.display = "block";
      document.getElementById('password-prompt').style.display = "none";
    } else {
      alert("Incorrect password. Try again.");
    }
  }
</script>
```

### 2024/11/14 Reading Francesco's notes

I asked a couple of questions to ChatGPT while reading it. Their answers were:

![On One dimensional motion and its statistics, Random Walks](images/Particle-Based Simulation of Fluids/Screenshot 2024-11-14 at 19.30.16.png)

<details>
<summary>Click to reveal the answer!</summary>

In this context, the **factor 2** accounts for a restriction in the random walk model related to how steps accumulate over time.

For a simple one-dimensional random walk on a lattice (with lattice spacing $a$), the particle moves by one lattice unit per time step, either to the left or right. This discrete nature of the walk leads to the following pattern:

1. **Even and Odd Positions:** 
   - If the walk starts at the origin, then after an even number of steps $t$, the particle can only be at even lattice positions (multiples of $2a$), because it has made an even number of moves.
   - Similarly, after an odd number of steps $t$, the particle will only be at odd lattice positions (odd multiples of $a$).

2. **Even and Odd Times:**
   - For **even $t$**, the probability distribution $P(x, t)$ will only have non-zero values at **even lattice positions**.
   - For **odd $t$**, $P(x, t)$ will only have non-zero values at **odd lattice positions**.

This constraint implies that half of the possible bins (positions) are unfilled for a given time $t$ because only positions matching the parity of $t$ are reachable. To represent this correctly in the continuous approximation of $P(x, t)$, the **factor 2** compensates for this limitation by doubling the probability density to account for the missing (unreachable) positions. 

In essence, the factor 2 ensures that the probability distribution $P(x, t)$ normalises correctly even though only half of the possible positions are accessible at any given time due to the parity constraint.

---

Here are answers to your questions based on the proof and equation provided:

1. Why is the combination written as $\frac{N!}{n_1! n_2!}$?

This combination term represents the **number of distinct ways** to arrange a sequence with $n_1$ right steps and $n_2$ left steps out of $N$ total steps. In combinatorics, when we have a sequence with repetitions (here, right and left steps), the number of unique arrangements is given by dividing the factorial of the total number of steps, $N!$, by the factorials of the number of steps of each type, $n_1!$ and $n_2!$. This accounts for the fact that swapping identical steps (all right steps, for example) doesn’t create a new unique path.

2. Why is this multiplied by $p^{\frac{1}{2}(N+m)} q^{\frac{1}{2}(N-m)}$ to give $P(m, N)$?

The probability of a specific path occurring (with exactly $n_1$ steps to the right and $n_2$ steps to the left) is the product of the probability of each step. Since each right step has probability $p$ and each left step has probability $q = 1 - p$, the probability of any particular path with $n_1 = \frac{1}{2}(N + m)$ right steps and $n_2 = \frac{1}{2}(N - m)$ left steps is:

$$
p^{n_1} q^{n_2} = p^{\frac{1}{2}(N + m)} q^{\frac{1}{2}(N - m)}
$$

Multiplying this by the combinatorial factor $\frac{N!}{n_1! n_2!}$ gives the total probability $P(m, N)$, as it accounts for all possible distinct paths that result in the final position $m$.

3. What does it mean by the ‘first’ and ‘second’ central moments?

The **first central moment** of a distribution is the **mean** (or expected value) of the distribution, represented as $\langle k \rangle$ for the binomial distribution in this proof. It indicates the average or most likely position after $N$ steps.

The **second central moment** is the **variance**, which is the expected value of the squared deviation from the mean. For the binomial distribution here, the variance is calculated as $\sigma^2 = \langle k^2 \rangle - \langle k \rangle^2$. The variance measures the spread of the distribution around the mean. Together, the first and second moments describe the central tendency and spread of the distribution.

4. How can we use the final expression in the proof to arrive at equation (2)?

To arrive at equation (2), follow these steps based on the large $N$ limit:

1. **Convergence to Normal Distribution**: For large $N$, the binomial distribution $P(k, N)$ converges to a normal distribution (Central Limit Theorem) with mean $\langle k \rangle = N/2$ and variance $\text{Var}[k] = N/4$. The form becomes:

   $$
   P(k, N) \approx \frac{1}{\sqrt{2\pi N/4}} \exp\left(-\frac{(k - N/2)^2}{2N/4}\right)
   $$

2. **Rewrite in Terms of $m$**: Recall that $m = 2k - N$. This substitution transforms $P(k, N)$ to $P(m, N)$. Expressing $k$ in terms of $m$ gives $k = \frac{N + m}{2}$. Substitute $k$ and rewrite the exponent in terms of $m$.

3. **Simplify the Result**: After substitution and simplification, you obtain:

   $$
   P(m, N) = \frac{2}{\sqrt{2\pi N}} \exp\left(-\frac{m^2}{2N}\right)
   $$

4. **Transition to Continuous Variables**: In the continuous limit, let $t = N \Delta t$ (where $\Delta t$ is the time step). Then, replacing $N$ with $t/\Delta t$ in the expression, you arrive at the form of equation (2):

   $$
   P(x, t) \approx \frac{2}{\sqrt{2\pi l^2 t / \Delta t}} \exp\left(-\frac{x^2 \Delta t}{2 l^2 t}\right)
   $$

This final expression is the continuous probability density function for displacement $x$ at time $t$, matching equation (2) in the problem.

---

Thank you for clarifying. Here’s a deeper look into each part:

1. Why is the combination written as $\frac{N!}{n_1! n_2!}$?

The combination $\frac{N!}{n_1! n_2!}$ arises from a fundamental combinatorial principle when considering paths of independent steps. Here’s why this form is specifically chosen:

   - **Fundamental Counting Principle**: For $N$ total steps, if each step could either go right or left, you could naively think that each unique ordering of right and left steps forms a distinct path. However, some steps are identical (right steps and left steps).
   - **Correcting for Identical Arrangements**: Since a particular sequence with $n_1$ right steps and $n_2$ left steps will appear repeatedly due to the indistinguishability of identical steps, we divide by the factorials of these counts ($n_1!$ and $n_2!$) to avoid overcounting.
   - **Why Factorials Specifically?**: Factorials count the number of ways to arrange $n_1$ right steps among themselves or $n_2$ left steps among themselves. By dividing $N!$ (the total permutations) by $n_1! n_2!$, we’re specifically removing duplicate configurations of right and left steps, leaving only the unique paths. Thus, $\frac{N!}{n_1! n_2!}$ counts the number of distinct ways to take $n_1$ right and $n_2$ left steps out of $N$, preserving only unique configurations.

2. Why is the total probability the product of the combinatorial factor and the probability of a specific path occurring?

To understand this, we need to consider the role of independence and probability aggregation:

   - **Independence of Steps**: Each step is independent and has its own probability of being right ($p$) or left ($q = 1 - p$). The probability of one particular sequence occurring is therefore $p^{n_1} q^{n_2}$.
   - **Probability of Each Unique Path**: Since each unique path represents one way of achieving a final position $m$ (which corresponds to $n_1$ right steps and $n_2$ left steps), the probability of ending at $m$ is the sum of probabilities for all such unique paths.
   - **Combinatorial Factor Accounts for All Unique Paths**: Instead of explicitly summing over each arrangement (which would be infeasible for large $N$), the combinatorial factor $\frac{N!}{n_1! n_2!}$ tells us how many such paths exist. By multiplying this count by the probability of any specific path configuration $p^{n_1} q^{n_2}$, we obtain the **total probability** of reaching position $m$. This is because each of the $\frac{N!}{n_1! n_2!}$ paths has the same probability $p^{n_1} q^{n_2}$, and hence their total contribution to $P(m, N)$ is the product.

3. What does ‘first’ and ‘second central moments’ mean, and why are they ‘central’?

Moments are measures of the shape of a distribution relative to a reference point:

   - **First Moment**: The first moment relative to zero (mean) is the average, which gives the central tendency.
   - **Second Moment**: The second moment relative to the mean is the variance, which describes the spread around the mean.
   - **Why ‘Central’?**: These moments are measured relative to the mean (a central point of the distribution), rather than from zero. This is crucial in probability and statistics because measuring variance relative to the mean provides a clearer picture of the dispersion around the most typical value rather than from a fixed point. For example, the second central moment is the **variance**, which quantifies how data points typically deviate from the mean, thus centralising the spread description.

4. Why does $l$ appear in the final expression, and how is it derived?

The appearance of $l$ in the final expression relates to the **scale of random steps** in the walk. Here’s why $l$ enters the equation in that specific form:

   - **Role of $l$**: $l$ represents the step size or spatial scale for each step in the random walk. When scaling up to $N$ steps, the typical displacement (mean squared displacement) should depend on both the number of steps and the step size. Intuitively, larger steps (higher $l$) mean the walker covers more distance over the same number of steps.
   - **Variance Contribution**: For each step, $\xi$ has unit variance, but since each step is scaled by $l$, the effective variance of each step is $l^2$. Consequently, after $N$ steps, the total variance in displacement (mean squared displacement) is $N \times l^2$.
   - **In the Limit to Continuous Space**: When we transition to a continuous probability distribution for displacement, we see this dependence in the term $l^2 t / \Delta t$ in the denominator of the Gaussian expression, reflecting the cumulative effect of each step’s scale.

</details>