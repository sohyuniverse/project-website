---
title: Reading Archive
toc: true
bibliography: references/quarto.bib
reference-location: margin
citation-location: margin
---

To keep track of my progress:  
- *Italicised titles* indicate works I’ve completed reading.  
- Regular (non-italicised) titles indicate works I have not finished yet.  

---

## 2024/9/21 - 10/3 *What is a Complex System?*

By James Ladyman and Karoline Wiesner

My final-year MSci project involved the study of a complex system, foreign exchange markets. Each individual currency was treated as a spin in the Ising model, and by applying E.T. Jaynes's principle of maximum entropy, we discovered the structure of interactions between these entities. Remarkably, this simple model of magnetism can also describe the phase transitions of liquid-gas systems, as they fall into the same universality class. This model has even been extended to study how critical our brain is, giving rise to the field of the critical brain hypothesis. Therefore, many ideas during my research project were derived from neuroscience papers. The financial market and the brain: both are complex systems.

About a year ago, I read a paper published by James Ladyman, a professor of philosophy of science at Bristol, titled 'What is a Complex System?' [@ladyman2013complex]. As I started my PhD at Bristol, my second supervisor, Francesco, mentioned that he also published a book about complexity. It became clear to me that I should read this book.

Chapter 1 presents The Truisms of Complexity Science as follows: 

*1. More is different.*

*2. Nonliving systems can generate order.*

*3. Complexity can come from simplicity.*

*4. Coordinated behaviour does not require an overall controller.*

*5. Complex systems are often modelled as networks or information processing systems.*

*6. There are various kinds of invariance and forms of universal behaviour in complex systems.*

*7. Complexity science is computational and probabilistic.*

*8. Complexity science involves multiple disciplines.*

*9. There is a difference between the order that complex systems produce and the order of the complex systems themselves.*

Features that are necessary and sufficient for which kinds of complexity and complex system are as follows: 

*1. Numerosity: complex systems involve many interactions among many components.*

*2. Disorder and diversity: the interactions in a complex system are not coordinated or controlled centrally, and the components may differ.*

*3. Feedback: the interactions in complex systems are iterated so that there is feedback from previous interactions on a time scale relevant to the system’s emergent dynamics.*

*4. Non-equilibrium: complex systems are open to the environment and are often driven by something external.*

*5. Spontaneous order and self-organisation: complex systems exhibit structure and order that arises out of the interactions among their parts.*

*6. Nonlinearity: complex systems exhibit nonlinear dependence on parameters or external drivers.*

*7. Robustness: the structure and function of complex systems is stable under relevant perturbations.*

*8. Nested structure and modularity: there may be multiple scales of structure, clustering and specialisation of function in complex systems.*

*9. History and memory: complex systems often require a very long history to exist and often store information about history.*

*10. Adaptive behaviour: complex systems are often able to modify their behaviour depending on the state of the environment and the predictions they make about it.*

---

2024/9/27

Reading *What is a Complex System?* by James Ladyman and Karoline Wiesner. The purpose of reading is to understand how systems out of thermodynamic equilibrium (a feature of complex systems) relate to a wide range of concepts associated with complexity.

   • There was a video that helped clarify what is meant by a Markov chain and a stochastic process being stationary: [Markov Chains Clearly Explained! Part - 1](https://www.youtube.com/watch?v=i3AkTO9HLXo&list=PLM8wYQRetTxBkdvBtz-gw8b9lcVkdXQKV). His other videos are helpful as well. For example, when I saw $P_{ij}^{(n)} = A_{ij}^n$ from the *n-Step Transition Matrix* video (Part 3), it was quite surprising. However, the Part 5 video on hidden Markov models wasn’t very helpful, but reading the appendix in the book was sufficient.

   • What I’ve felt from reading this book (currently at p.90) is that it presents a lot of different ideas and has interesting discussions on the history of science, but I only seem to get excited when it starts mentioning the brain. I'm not sure if I’m necessarily interested in quantifying complexity. From the Apple notes on 28/9/2024:

   *As I’m reading through *What is a Complex System?*, it seems that the description of a complex system arises as we have more representations at different scales of analysis. The concept of ‘complexity’ was destined to arise as the scientific era evolved.  
   Non-equilibrium systems arise because we define what systems are in thermodynamic equilibrium, which are idealisations.  
   Things that fail to be idealised fall into this dualistic concept: non-equilibrium.*

---

2024/10/3

It took about two weeks to finish this book. It was an accessible read until I reached chapter 4, which was *Measuring Features of Complex Systems*. In the end, I'm glad that I read it, even though I somehow felt rushed to finish as there were other responsibilities to manage continuously, such as setting up clusters for my university Mac, teaching, office setup, etc. Most importantly, from now on, I am going to focus on reading material that is directly related to my project since it may give the impression to my supervisors that I'm not working hard. Reading a book like this takes a long time, as there are many new terms I had not encountered before.

Anyway, it was a good read. Would I buy a physical copy for myself? Maybe not, but it was a good one.

## 2024/10/4 - Hydrophobicity Across Length Scales: The Role of Surface Criticality [@coe2021hydrophobicity]

*Cited by 5*

### Q1. *Resolved*

![On page 35, Chapter 2.1](images/Hydrophobicity%20Across%20Length%20Scales/Screenshot%202024-10-04%20at%2001.12.24.png)

I'm not entirely sure why the surface tension is defined in that way, as it's normally considered force per unit area. So, I asked ChatGPT:

<details>
<summary>Click to reveal the answer!</summary>

The definition of surface tension in this context is derived from the thermodynamic perspective, which connects surface tension to the grand potential difference between a bulk fluid and a system with an interface. The reasoning follows these steps:

1. **Grand Potential Definition**: The grand potential, $\Omega$, is given by:
   $$
   \Omega = U - TS - \mu N
   $$
   This expression is crucial for systems where particle number and volume are allowed to fluctuate, such as liquid-vapour interfaces.

2. **Surface Tension from Thermodynamics**: Surface tension, $\gamma$, represents the energy cost per unit area to create an interface. In thermodynamic terms, this corresponds to how much the grand potential increases when introducing an interface between coexisting phases (liquid and vapour).

3. **Difference between Bulk and Interface**: The bulk grand potential, $\Omega_b$, corresponds to a system without an interface (uniform bulk). When an interface is introduced, the grand potential changes, denoted as $\Omega_{\text{ex}}$, the excess grand potential:
   $$
   \Omega_{\text{ex}} = \Omega - \Omega_b
   $$

4. **Surface Tension as Excess Grand Potential per Unit Area**: The surface tension is then defined as the excess grand potential per unit area, $A$, required to form the interface:
   $$
   \gamma = \frac{\Omega_{\text{ex}}}{A} = \frac{\Omega - \Omega_b}{A}
   $$
   This formulation makes sense because surface tension describes the energetic cost to create a unit area of interface between two phases.

In summary, while surface tension is often discussed in terms of force per unit length in simpler mechanical contexts, in thermodynamics, it is expressed as the excess grand potential per unit area. This ties into how interfaces modify the system's energy in comparison to the bulk. The two definitions are equivalent but applied in different contexts: mechanical (force/length) vs. thermodynamic (energy/area).

</details>

<details>
<summary>Derivation of equation 2.1.8, 2.1.9, 2.1.10</summary>

$$
\Omega_b(\mu, V, T) = U - TS - \mu N = -pV
$$

Taking the differential:

$$
d\Omega_b = d(U - TS - \mu N)
$$

Using the first law of thermodynamics:

$$
dU = TdS - pdV + \mu dN
$$

Differentiating $TS$:

$$
d(TS) = TdS + SdT
$$

Differentiating $\mu N$:

$$
d(\mu N) = \mu dN + N d\mu
$$

Substituting these into the differential for $\Omega_b$:

$$
d\Omega_b = (TdS - pdV + \mu dN) - (TdS + SdT) - (\mu dN + N d\mu)
$$

Simplifying:

$$
d\Omega_b = -S_b dT - N_b d\mu - p dV
$$

For constant volume ($dV = 0$), *the volume is assumed to remain constant because we are focusing on surface effects rather than bulk volume changes.*:

$$
d\Omega_b = -S_b dT - N_b d\mu
$$

Now for equation (2.1.6):

$$
\Omega(\mu, V, T) = U - TS - \mu N = -pV + \gamma A
$$

Taking the differential:

$$
d\Omega = d(U - TS - \mu N + \gamma A)
$$

Substituting differentials:

$$
d\Omega = TdS - pdV + \mu dN - (TdS + SdT) - (\mu dN + N d\mu) + \gamma dA + A d\gamma
$$

Simplifying:

$$
d\Omega = -S dT - N d\mu + \gamma dA
$$

Now subtract:

$$
d\Omega - d\Omega_b = (-S dT - N d\mu + \gamma dA) - (-S_b dT - N_b d\mu)
$$

Simplifying:

$$
d\Omega - d\Omega_b = -(S - S_b)dT - (N - N_b)d\mu + \gamma dA
$$

This is equation (2.1.8).

---

From equation (2.1.7):

$$
\gamma = \frac{\Omega_{\text{ex}}}{A} = \frac{\Omega - \Omega_b}{A}
$$

Taking the differential:

$$
d\gamma = \frac{d\Omega_{\text{ex}}}{A} = \frac{d\Omega - d\Omega_b}{A}
$$

Now, use the result from equation (2.1.8):

$$
d\Omega - d\Omega_b = -(S - S_b) dT - (N - N_b) d\mu + \gamma dA
$$

So:

$$
d\Omega_{\text{ex}} = -s_{\text{ex}} A dT - \Gamma d\mu + \gamma dA
$$

where:

- $s_{\text{ex}} = \frac{S - S_b}{A}$ is the surface excess entropy per unit area,
- $\Gamma = N - N_b$ is the excess number of particles (adsorption).

Dividing through by $A$, we get:

$$
\frac{d\Omega_{\text{ex}}}{A} = -s_{\text{ex}} dT - \frac{\Gamma}{A} d\mu + \gamma \frac{dA}{A}
$$

Now, using the fact that $\gamma \frac{dA}{A}$ is small for constant area, we can ignore this term, giving:

$$
s_{\text{ex}} dT + \frac{\Gamma}{A} d\mu + d\gamma = 0
$$

This is equation (2.1.9).

---

Rearrange equation (2.1.9):

$$
\frac{\Gamma}{A} d\mu = -s_{\text{ex}} dT - d\gamma
$$

At constant temperature ($dT = 0$), this simplifies to:

$$
\frac{\Gamma}{A} d\mu = -d\gamma
$$

Now, dividing both sides by $d\mu$, we get:

$$
\frac{\Gamma}{A} = -\left(\frac{\partial \gamma}{\partial \mu}\right)_T
$$

This is equation (2.1.10).

</details>

And I need to ask about equations 2.1.11 to 2.1.13, as I cannot find reference [40].

### Q2. *Resolved*

![On page 40, Chapter 2.3](images/Hydrophobicity%20Across%20Length%20Scales/Screenshot%202024-10-04%20at%2020.33.41.png)

I wasn't sure what the one-particle density is or its use, so I asked ChatGPT:

<details>
<summary>Click to reveal the answer!</summary>

The one-particle density function, often denoted as $\rho^{(1)}(\mathbf{r})$, represents the probability density of finding a particle at a specific position $\mathbf{r}$ in a system of particles. It describes how particles are distributed in space on average and is a key observable in statistical mechanics and the study of fluids.

The equation given is:

$$
\hat{\rho}^{(1)}(\mathbf{r}) = \sum_{j=1}^{N} \delta(\mathbf{r} - \mathbf{r}_j)
$$

This is the **one-particle density operator**, and it has the following meaning:

1. **Dirac Delta Function**: $\delta(\mathbf{r} - \mathbf{r}_j)$ is a Dirac delta function that ‘picks out’ the position of the $j$-th particle. It is equal to zero everywhere except at $\mathbf{r}_j$, where it is infinite, ensuring that it integrates to 1. Thus, it counts the presence of a particle at $\mathbf{r}_j$.

2. **Summation over All Particles**: The sum runs over all particles $j$ from 1 to $N$, so this operator adds up the contributions from all particles in the system, effectively counting the number of particles at position $\mathbf{r}$.

In essence, the one-particle density operator $\hat{\rho}^{(1)}(\mathbf{r})$ gives the total number of particles at position $\mathbf{r}$ in the system. Since it is an operator, the actual **one-particle density** $\langle \hat{\rho}^{(1)}(\mathbf{r}) \rangle$, as defined in equation (2.3.2), is obtained by averaging over all possible configurations of the system in the given ensemble (such as the grand canonical ensemble).

*Why Does It Look Like This?*

The form of equation (2.3.1) stems from how we represent the microscopic distribution of particles in space. The Dirac delta function ensures that for each particle, its position contributes to the overall density only at the exact location where that particle exists. This is a standard way to construct density operators in statistical mechanics because:

- The delta function precisely pinpoints the position of a particle.
- Summing over all particles ensures that the density operator accounts for the total number of particles present in the system.

This representation allows us to describe spatially inhomogeneous systems where the density may vary across different regions (e.g., near a surface or an external potential). For homogeneous fluids, where particles are evenly distributed, this density would reduce to a constant value (the bulk density $\rho_b$).

*Summary*

The one-particle density operator, $\hat{\rho}^{(1)}(\mathbf{r})$, reflects how particles are distributed in space at a microscopic level. The delta functions track the positions of individual particles, and summing over all particles gives the total density at position $\mathbf{r}$. The actual one-particle density function $\rho^{(1)}(\mathbf{r})$ is then the ensemble average of this operator over all configurations of the system.

</details>

### Q3. *Resolved*

![On page 43, Chapter 2.3](images/Hydrophobicity%20Across%20Length%20Scales/Screenshot%202024-10-04%20at%2022.36.13.png)

I wasn't sure why each variable is specifically chosen to form those equations. For example, in section 2.4.3, I do not understand why λ can be factored out of g on the right-hand side when λ has two different exponents, a and b. So, I asked ChatGPT:

<details>
<summary>Click to reveal the answer!</summary>

Starting with Equation (2.4.3):

$$
g(\lambda^a u, \lambda^b v) = \lambda g(u, v)
$$

This equation states that the function $g(u, v)$ is a **generalised homogeneous function**. To understand why it takes this form and why $\lambda$ appears outside the function on the right-hand side, let's break this down:

1. **Homogeneous Functions**: A function is called homogeneous of degree $\alpha$ if, when all its arguments are scaled by a factor $\lambda$, the function itself scales by $\lambda^\alpha$. Mathematically, for a homogeneous function $f(x, y)$, we would have:
   $$
   f(\lambda x, \lambda y) = \lambda^\alpha f(x, y)
   $$
   This property is common in critical phenomena where we are dealing with power-law behaviour near critical points.

2. **Generalised Homogeneous Functions**: The function $g(u, v)$ is a **generalised** homogeneous function because it allows different exponents for different variables. In this case, the variables $u$ and $v$ scale with different exponents $a$ and $b$ under the same scaling factor $\lambda$.

   - $\lambda^a u$: Here, $u$ is scaled by $\lambda^a$. This implies that $u$ behaves differently under scaling compared to $v$.
   - $\lambda^b v$: Similarly, $v$ is scaled by $\lambda^b$.

3. **Why $\lambda$ can be factored out**: The key to understanding why $\lambda$ can be factored out on the right-hand side despite having different exponents for $u$ and $v$ lies in the definition of a generalised homogeneous function. By construction, we impose that when the variables are scaled in a particular way (with exponents $a$ and $b$), the function scales by $\lambda$ itself. This is a property we assign to $g(u, v)$. It doesn’t mean that $\lambda$ is constant; rather, the entire function $g(u, v)$ is designed to respond to the scaling in this specific manner.

   The exponents $a$ and $b$ control how $u$ and $v$ individually respond to scaling, but the overall response of the function is to scale as $\lambda$. The scaling of the function on the right-hand side doesn't contradict the fact that $\lambda$ has different exponents inside the arguments. This is simply a mathematical constraint on how the function $g(u, v)$ behaves under scaling.

   In other words, the form:
   $$
   g(\lambda^a u, \lambda^b v)
   $$
   ensures that the function scales uniformly by $\lambda$, despite the individual variables scaling differently.

Equation (2.4.4): 

Now, let's explain how this scaling idea applies to the singular part of the free energy in equation (2.4.4):

$$
\omega_{\text{sing}}(\delta \mu, t) = \lambda^{-d} \omega_{\text{sing}}(\lambda^a \delta \mu, \lambda^b t)
$$

1. **Variables**:
   - $\delta \mu = \mu - \mu_{\text{co}}$ is the difference between the chemical potential $\mu$ and the critical chemical potential $\mu_{\text{co}}$.
   - $t$ is the reduced temperature: $t = \frac{T - T_c}{T_c}$.
   - $d$ is the dimensionality of the system (important for scaling relations).

2. **Applying the Scaling Relation**:
   Here, the function $\omega_{\text{sing}}$, which represents the singular part of the free energy, is assumed to be a generalised homogeneous function. This means it should follow a scaling law similar to the one introduced in equation (2.4.3).

   The variables $\delta \mu$ and $t$ are scaled by $\lambda^a$ and $\lambda^b$, respectively, where $a$ and $b$ are scaling exponents that determine how $\delta \mu$ and $t$ change when scaled by $\lambda$.

3. **Why $\lambda^{-d}$?**
   - The factor $\lambda^{-d}$ appears because we are dealing with free energy, which is an extensive quantity. In thermodynamics, extensive quantities scale with the volume of the system, and in $d$-dimensional space, the volume scales as $\lambda^d$. Since we are dealing with the **singular** part of the free energy, the scaling reflects the dimensionality of the system. The minus sign arises because, under scaling, the free energy decreases as the system grows larger.

4. **The Free Choice of $\lambda$**:
   The scaling factor $\lambda$ is arbitrary, meaning you can choose its value to simplify the equation. In this case, we choose $\lambda^b = t^{-1}$, which simplifies the second argument (the temperature dependence) of $\omega_{\text{sing}}$. By doing this, we reduce the complexity of the equation and make the temperature scaling explicit.

Equation (2.4.5): 

This equation shows the final form after choosing $\lambda^b = t^{-1}$:

$$
\omega_{\text{sing}}(\delta \mu, t) = t^{d/b} \omega_{\text{sing}}(t^{-a/b} \delta \mu, 1)
$$

- **Why this form?**
  By setting $\lambda^b = t^{-1}$, we have scaled out the temperature dependence in the second argument of $\omega_{\text{sing}}$. This isolates the temperature dependence into the prefactor $t^{d/b}$, while leaving a scaling function that depends only on the ratio $\delta \mu / t^{a/b}$, which is now the only scaling variable.

  - $t^{d/b}$: This represents how the singular part of the free energy depends on temperature near the critical point. The exponent $d/b$ dictates how strongly the free energy diverges as $t \to 0$ (the critical point).
  - $\Sigma\left( \frac{\delta \mu}{t^{a/b}} \right)$: The function $\Sigma$ is the scaling function, which encodes the remaining dependence on the scaled chemical potential $\delta \mu / t^{a/b}$.

In Summary:
- In equation (2.4.3), the exponents $a$ and $b$ control how the variables $u$ and $v$ scale. Despite their different scaling behaviours, the overall function scales by $\lambda$ because it is designed as a generalised homogeneous function.
- In equation (2.4.4), the singular part of the free energy near the critical point follows a similar scaling law, with the dimensionality $d$ appearing as a factor to reflect the extensive nature of free energy.
- In equation (2.4.5), we choose $\lambda^b = t^{-1}$ to simplify the scaling form, making the dependence on temperature explicit and reducing the complexity of the equation. The resulting expression separates the prefactor $t^{d/b}$ and leaves a scaling function that depends only on the ratio $\delta \mu / t^{a/b}$, which captures the critical behaviour.

</details>

<details>
<summary>Fluctuations? *One correlated entity*?</summary>

In the context of phase transitions, particularly near a **critical point**, the term "fluctuations" refers to **fluctuations in the local density** of the system. Let’s explain in detail what this means, especially in the context of **critical opalescence** and **long-range correlations**.

1. **Density Fluctuations**:
When we talk about "density" in this context, we mean the **number density** of particles in a given volume. In simple terms, it's how densely packed the particles are in a particular region. The density can fluctuate due to thermal motion, and these fluctuations are typically small and localised in non-critical conditions.

However, as a system approaches the critical point of a phase transition (e.g., a liquid-vapour transition), the magnitude of these fluctuations grows dramatically.

#### What happens near the critical point?
- **Correlation Length ($\xi$)**: The correlation length $\xi$ represents the distance over which particles in the system are correlated or "communicate" with each other. Normally, this length is finite, and beyond this distance, the properties of the system in one region do not affect another region.
  
  Near the critical point, however, $\xi \to \infty$. This means that particles separated by very large distances (even macroscopic scales) are correlated with each other. The entire system essentially becomes **one correlated entity**.
  
- **Diverging Fluctuations**: As the correlation length grows, the fluctuations in the local density (i.e., how many particles are in a given region at any time) also grow larger. This happens because, when the correlation length is large, particles across large distances influence each other. As a result, fluctuations occur over all length scales, from small to very large.

- **Density Fluctuations**: Specifically, these are **fluctuations in the number of particles** in a given volume. Imagine regions in a fluid where the particle density becomes temporarily higher or lower than the average. At the critical point, these fluctuations become very large and occur over very large distances, leading to the system appearing "cloudy" or "milky" — this is the phenomenon of **critical opalescence**.

2. **What exactly is fluctuating?**
At the critical point, **density** is the key fluctuating quantity, but more generally, the term “fluctuations” can refer to:

- **Order Parameter Fluctuations**: Near a critical point, there is typically an **order parameter** that characterises the phase transition. For example, in the liquid-vapour transition, this order parameter could be the **difference in density** between the liquid and vapour phases. As the system approaches the critical point, the fluctuations in the order parameter become large.
  
  For example, in a fluid near the liquid-vapour critical point:
  - In the liquid phase, particles are densely packed.
  - In the vapour phase, particles are more spread out.
  
  Near the critical point, there is no sharp distinction between liquid and vapour phases. Instead, the system fluctuates between states that resemble both phases. You might see a region that has the density of a liquid next to a region that has the density of a vapour, and these regions appear and disappear rapidly.

3. **Why do these fluctuations occur?**
At the critical point, the system is in a delicate balance between two phases (e.g., liquid and vapour). The free energy landscape becomes very flat near the critical point, meaning that the system can fluctuate easily between different configurations with very little energy cost. These fluctuations span all length scales, leading to the phenomenon where large regions of the system can fluctuate in density.

4. **Critical Opalescence**:
The critical opalescence mentioned in the text is a direct consequence of these large fluctuations in density. Light passing through the fluid is scattered by the density fluctuations. Since the fluctuations occur over a wide range of length scales (from very small to very large), they scatter light across all wavelengths, leading to the "milky" appearance of the fluid near the critical point.

- **Why does light scatter?** Light is scattered by regions of varying density because the refractive index of the fluid depends on its density. When the density fluctuates significantly over large distances, light passing through the fluid encounters regions of different densities, leading to scattering in all directions.

Summary:

- **What is fluctuating near the critical point?**: Primarily, the **number density** (i.e., the number of particles per unit volume) fluctuates near the critical point.
- **Why do these fluctuations grow?**: As the correlation length $\xi$ becomes large, regions of the system separated by large distances become correlated, allowing large-scale fluctuations in density.
- **What’s the physical manifestation?**: These fluctuations lead to observable phenomena such as **critical opalescence**, where light is scattered by the large fluctuations in density, causing the fluid to appear milky or cloudy.

In short, the "fluctuations" near the critical point refer to **density fluctuations**, which are tied to the divergence of the correlation length, and these fluctuations grow larger and affect larger regions of the system as the critical point is approached.

</details>

---

From chapter 2.4.3, I decided to skim through this document as it became too technical to understand the entire thesis when it began with perturbation theory. Additionally, many concepts here are primarily from equilibrium statistical physics.

::: {.callout-note}
Chapter 2.4.5: *Wetting and Drying Phase Transitions* is very well written and full of useful knowledge. For the next project meeting, let's prepare a slide to go over this.
:::

[Wetting and Drying Phase Transitions](documents/Coe - Hydrophobicity Across Length Scales The Role of S-49-54.pdf)

I've been reading up to *Views of Hydrophobicity*, but I realised that I'm not sure how much of the stuff I learn from this thesis is related to active systems. So, I decided to review all articles given by my supervisors.

## 2024/10/14 - Colloquium: Inclusions, boundaries, and disorder in scalar active matter [@granek2024colloquium]

*Cited by 3*

Three different models of active matter are discussed here: ABP (active Brownian particles), RTP (run-and-tumble particles), and AOUP (active Ornstein-Uhlenbeck particles). ABP is a model that I'm going to investigate initially because it's the simplest particle model, but I want to be aware that there are other models. From page 3, what is interesting is this, since I saw somewhere that detailed balance doesn't hold for non-equilibrium systems:

::: {.callout-note}
On the contrary, in a harmonic potential AOUPs always have a steady state given by a centered Gaussian distribution (Szamel, 2014) and, somewhat surprisingly, their dynamics obey detailed balance (Fodor et al., 2016).
:::

I wanted to revisit the detailed balance algorithm more deeply, so I started reading *Understanding Molecular Simulation*.

## 2024/10/18 - 10/19 *On the Motion of Small Particles Suspended in Liquids at Rest Required by the Molecular-Kinetic Theory of Heat* [@einstein1905motion]

*Cited by 5001*

Since I am studying a model called active Brownian particles, a thought occurred to me: I should actually read where this understanding of Brownian motion came from!  This was done by the very man: Albert Einstein in 1905. I normally leave my iPad in my office so whenever I needed derivations, I used a pen and note, quite a classical way while reading a classical paper:

[My handwritten notes](documents/241019 Scan.pdf)

I'm so glad that I read this. It reminded me of how much I loved studying thermodynamics during my undergraduate studies. It was kind of like studying geometry (or topology). I consider a system subject to constraints and investigate how the particle behaves given the state variables. The paper wasn't straightforward to read, as I had to remind myself of several concepts such as the continuous form of entropy, perturbation theory, and Fourier transforms to find a solution for diffusion equations.

I really liked the way he displayed his logic. He considers a system: a pure solvent and suspended particles. These particles are confined by a semi-permeable wall, which only allows solvent to flow, not the solute. Then he thinks about the existing concept of osmotic pressure, which inhibits the solvent from moving to equalise concentrations. He then makes several *reasonable* assumptions, such as:

1. Suspended particles have slow and irregular motions due to the solvent's molecules.
2. The motions of those individual particles are independent of each other.

He then links the diffusion coefficient to two different contexts. The first is through the theory of diffusion, involving concepts of entropy and free energy. The second is through the theory of thermal molecular motion, where he considers the rate of change of density. By uniting these two theories, he arrives at the root-mean-square displacement of the suspended particle. Wow!

I'm so happy to have chosen a theoretical physics PhD. Thinking about how theories interconnect reveals a universal understanding. I’m looking forward to reading more of his articles.

## 2024/10/23 - Understanding Molecular Simulation, 3rd Ed [@frenkel2023understanding]

*Cited by 22954!*

### Chapter 1. Introduction

Before starting our project, I was a little concerned about whether computational simulation is a valid way to understand the fundamental order of nature. I thought that what we do as physicists is observe systems through experiments, theorise with idealised objects to gain predictive power, and then test the theory by observing again. This thought process was probably inspired by a Kantian perspective; I read his *Critique of Pure Reason* in high school, where he posits that causality is a presupposition of reasoning. For those purposes, idealised objects had to be mathematical (forms or images from Plato) and needed to represent what I directly observe in empirical reality.

![On page 2, Introduction](images/Understanding Molecular Simulation/Screenshot 2024-11-08 at 21.40.30.png)

I'm still not completely certain if computer simulation is truly the way to go, but as I work through it, I hope it will make more sense to me.

---

![On page 6, Introduction](images/Understanding Molecular Simulation/Screenshot 2024-10-23 at 16.38.54.png)

Quite an interesting discussion on what *'model'* means in science.

---

### Chapter 2. Thermodynamics and Statistical Mechanics

![On page 22, 2.1.2 Chemical Potential and Equilibrium](images/Understanding Molecular Simulation/Screenshot 2024-10-24 at 17.45.55.png)

Important equilibrium conditions!

---

![On page 22, 2.1.2 Chemical Potential and Equilibrium](images/Understanding Molecular Simulation/Screenshot 2024-11-11 at 11.50.22.png)

I'm not entirely sure why the extremum condition applies specifically to variations in $E_1$, $N_1$, and $V_1$. So, I asked ChatGPT:

<details>
<summary>Click to reveal the answer!</summary>

You're right to question this subtle point. 

The key insight lies in understanding that when we talk about maximising the total entropy $S_{\text{tot}} = S_1 + S_2$ of a closed system with fixed total values of energy, volume, and particle number, the only degrees of freedom available for changes are the distributions of $E$, $V$, and $N$ between the subsystems. 

Here's the reasoning in detail:

1. **Fixed Total Quantities Impose Constraints**: Because the total energy $E_{\text{tot}} = E_1 + E_2$, volume $V_{\text{tot}} = V_1 + V_2$, and particle number $N_{\text{tot}} = N_1 + N_2$ are fixed, any changes in $E_1$, $V_1$, or $N_1$ must be offset by corresponding changes in $E_2$, $V_2$, and $N_2$ to keep these sums constant.

2. **Entropy Extremum Condition**: In equilibrium thermodynamics, the Second Law requires that the total entropy of an isolated (or closed) system reaches a maximum (or an extremum) at equilibrium. However, since the system is closed and these total quantities are fixed, the system cannot simply increase or decrease its overall $E_{\text{tot}}$, $V_{\text{tot}}$, or $N_{\text{tot}}$. Instead, any adjustments to entropy must occur through *redistributions* of these quantities between subsystems.

3. **Redistribution as the Only Available Variation**: Thus, the extremisation of $S_{\text{tot}}$ must occur specifically with respect to the variables that can change within the subsystems while satisfying the overall conservation constraints. Since $E_{\text{tot}}$, $V_{\text{tot}}$, and $N_{\text{tot}}$ are fixed, the only available independent variations in $S_{\text{tot}}$ come from redistributing $E_1$, $V_1$, and $N_1$ (and equivalently $E_2$, $V_2$, and $N_2$).

4. **Derivative Vanishing Condition**: For $S_{\text{tot}} = S_1 + S_2$ to be maximised (or extremised) under these constraints, the derivatives of $S_{\text{tot}}$ with respect to $E_1$, $V_1$, and $N_1$ must vanish. This is because any non-zero derivative would indicate that entropy could be further increased by adjusting these redistributive variables, contradicting the requirement of equilibrium.

In summary, we require the extremum condition with respect to $E_1$, $V_1$, and $N_1$ specifically because these are the variables that can be redistributed between the subsystems within the constraint of fixed total quantities. This is why the extremum condition applies to these specific derivatives, and not to other parameters.

</details>

---

![On page 30, 2.2.3 Towards Classical Statistical Mechanics](images/Understanding Molecular Simulation/Screenshot 2024-10-25 at 23.34.58.png)

I’m not sure how to derive the transition from 2.2.17 to 2.2.18.

## 2024/11/2 - 11/8 *Physics and Beyond* [@heisenberg1971physics] {#sec-physics-and-beyond}

*Cited by 1832*

It's the first time I'm reading this in English; the title of the book in Korean is 부분과 전체, which might translate to *Parts and the Whole*. I've read it many times since I was in high school, and it has been highly influential to me as a physicist throughout my life. To capture some of the things I want to revisit later:

![On page 72, Fresh Fields (1926-1927)](images/Physics and Beyond/Screenshot 2024-11-04 at 08.43.51.png)

![On page 80, Fresh Fields (1926-1927)](images/Physics and Beyond/Screenshot 2024-11-04 at 09.10.47.png)

So far, having read up to page 89, it has all been interesting, except for Chapter 4 (*Lessons in Politics and History*). When the development of quantum mechanics was presented in Chapters 5 and 6, I felt my heart pounding as I read through, just as it did when I first read this book.

---

![On page 97, Atomic Physics and Pragmatism (1929)](images/Physics and Beyond/Screenshot 2024-11-04 at 20.20.52.png)

All these realms were so enjoyable to learn, but now, as a researcher, I wonder what realms are still open. I thought one of them was non-equilibrium statistical thermodynamics, but it appears to me that this theory can't be axiomatic, implying that all systems out of equilibrium must be described differently.

---

![On page 109, The Relationship between Biology, Physics and Chemistry (1930-1932)](images/Physics and Beyond/Screenshot 2024-11-04 at 21.20.51-down.png)

'Forms through which matter streams,' as Niels Bohr described, was the most profound characterisation of organisms I have encountered. I used to wonder, what about this structure remains? Does it actually persist, given that a person changes continuously as a child becomes an adult? Is it simply the name (person) we assign that represents this form of life? With this thought, is there really a distinction between animate and inanimate objects?

When I read *Timaeus* by Plato in high school, he described geometrical forms as the basis of earth, fire, water, and air, even though I found his idea peculiar (like everything is made of right triangles). At the time, I thought the properties that define those mathematical forms were responsible for things maintaining their structure in the way we perceive over time (since all forms change shape as time passes). My master's research was also focused on investigating this homeostasis in an ideal structure, not physical but something we perceive, in the context of foreign exchange markets. This line of thought was also captured well by Bohr’s question: "Can quantum mechanics explain nature's tendency to form structures through which matter with fixed chemical properties can stream for a limited time?"

---

![On page 128, Discussions about Language (1933)](images/Physics and Beyond/Screenshot 2024-11-05 at 09.48.38.png)

This passage about the discovery of antimatter was one of the reasons I chose a physics degree, along with my ambition to formally study general relativity. Although I didn’t score well in my particle physics exam in third year due to a part-time job over winter break, it remains one of the most exciting units I've taken. Here are the scores and ratings I gave to my third and fourth-year units with scores of at least 4:

5: Physics Research Project (83), Quantum Physics (88)  
4.5: Physics of Phase Transitions (85), Analytical Mechanics (66), Particle Physics (58)  
4: Quantum Information Theory (68), Foundations of Modern Physics (68), Methods of Theoretical Physics (63)  

It’s surprising that general relativity only scored 3.5, but it seems I didn’t enjoy the lecture style despite being fascinated by the content.

---

![On page 134, Discussions about Language (1933)](images/Physics and Beyond/Screenshot 2024-11-05 at 10.11.03.png)

This passage captures the reason I didn’t pursue a philosophy degree, despite having written about philosophy of science. What I realised was the fluidity of language, and at some point, reading philosophy felt like participating in a language game. Even when I wrote [*The Self and the World*](https://docs.google.com/document/d/1lGEIUBTY_RgLZfvyVO8nhSzJ_3SpVR0n1QEegImHv4g/edit?usp=sharing), I included definitions for the terms I used in the document. In contrast, I conceived mathematical objects as eternal, grounded solidly in axioms. I couldn’t imagine any language other than this to describe the hidden secrets of the universe as eternal representations, unchanging, unmoved, and concrete.

---

![On page 136, Discussions about Language (1933)](images/Physics and Beyond/Screenshot 2024-11-05 at 11.16.00.png)

Another book I feel closely connected to is the Tao Te Ching by Lao Tzu. It’s probably not an exaggeration to say that my whole life has been devoted to understanding the meaning of Tao, even though Lao Tzu paradoxically said, “The Tao that can be described is not the eternal Tao.” One day, I realised it is synonymous with the Truth or, in more Platonic terms, the formless. Ineffable. That’s when I wrote this diary: [Is "It" a Joke?](https://sohyuniverse.wordpress.com/2024/09/17/2023-03-27-is-it-a-joke/)

---

![On page 191, Toward a New Beginning (1941-1945)](images/Physics and Beyond/Screenshot 2024-11-05 at 23.29.36.png)

These chapters were not enjoyable to read: *Revolution and University Life (1933)*, *Individual Behavior in the Face of Political Disaster (1937-1941)*, *Toward a New Beginning (1941-1945)*. I’m certainly not interested in politics or wars. However, the ending of this third chapter is beautifully written; beauty transcends ideology, opinions, and dualistic views of the world.

---

![On page 195, The Responsibility of the Scientist (1945-1950)](images/Physics and Beyond/Screenshot 2024-11-06 at 10.04.36.png)

This reveals the reason why I chose the theme of *active matter*. I was curious about how this model suddenly emerged in the last 25 years. Why is there such a sudden interest in this concept? What possibilities does it offer for understanding nature? What contributions can I make to the circle of human knowledge?

---

While reading through the *Scientific and Political Disputes (1956-1957)* chapter, I realised I can only concentrate when scientific matters are being discussed, such as the conversations between Heisenberg and Wolfgang about the mathematical theory for elementary particles. 

![On page 230, The Unified Field Theory (1957-1958)](images/Physics and Beyond/Screenshot 2024-11-08 at 10.33.36.png)

However, everything that followed from the passage in the image was fascinating, especially when they started discussing symmetries (for example, the universe as a whole does not necessarily have to be symmetrical with respect to the operations under which natural laws remain invariant), relativistic quantum field theories, and more, which captivated me when I studied particle physics in my third year.

---

![On page 240, Elementary Particles and Platonic Philosophy (1961-1965)](images/Physics and Beyond/Screenshot 2024-11-08 at 17.04.56-down.png)

I have finally finished the book. Although the above image is not the last page, I love this book because it begins with Plato and ends with him. *Timaeus* was the book that shaped my worldview when I was in high school, and when I first encountered *Physics and Beyond*, I was thrilled to find someone in history who asked the same questions I had while reading *Timaeus*. Since then, my understanding has deepened, and I’ve come to see that all philosophy traces back to Plato. Last year, a decade later, I finally had a direct understanding of what Plato described through some interesting experiences.

Even beyond that passage in the screenshot, it’s truly worth rereading. People at that time were already considering how natural laws conform to different scales, touching on the concept of *emergence*, which is the field I’m now working in.

While reading this book, I rewatched the documentary series *Atom*, hosted by Jim Al-Khalili. I remember watching it for the first time in high school, loving it immensely, and being struck by statements from scholars like Paul Dirac, who said, “A physical law must possess mathematical beauty.” I was curious about what QED implied once I start learning about it at university, such as how all the matter we see is just reflections of virtual particles. That was a long time ago, and that naïveté and curiosity are what led me to study physics. I’d like to close with what Jim Al-Khalili said (even though I largely disagree with his interpretation of quantum mechanics in that chapter) at the end of *The Illusion of Reality* (the third episode): 

*In the last 100 years, we have peered deep inside the atom, the basic building block of the universe. And inside this tiny object, we have found a strange new world governed by exotic laws that at times seem to defy reason. Atoms present us with dizzying contradictions. They can behave both as particles or waves. They appear to be in more than one place at the same time. They force us to rethink what we mean by past and future, by cause and effect. And they tell us strange things about where the universe came from and where it's going. Pretty amazing stuff for something that's just a millionth of a millimetre across. That's why Niels Bohr, the father of atomic physics, once said that, when it comes to atoms, language can only be used as poetry. What's fascinating to me is that although we've learnt an incredible amount about atoms and their behaviour, our scientific journey has only just begun. Because although we know how a single atom or just a few atoms behave, the way trillions of them come together in concert to create the world around us is still largely a mystery. To give you one dramatic example, the atoms that make up my body are identical to the atoms in the rocks, the trees, the air, even the stars. And yet, they come together to create a conscious being who can ask the question, “What is an atom?” Explaining all that is surely the next great challenge in science.*

I realised that the “next great challenge” he mentioned is the same one I’ve been drawn to since studying physics: the emergence of consciousness. The most intriguing question, perhaps prompted by an interesting dream I had last night, is how our mind creates the world and perceives it simultaneously when we dream. When I'm awake, I can do this partially; for instance, when I think about walking down the corridor where my office is located, I can imagine it while experiencing it at the same time. It seems this ability expands to full capacity in dreams. Once I understand that mechanism, I might be able to discern, even when awake, whether I am still dreaming and creating the world while simultaneously perceiving it.

## 2024/11/9 - The Science of Interstellar [@thorne2014science]

*Cited by 265*

Today, I rewatched the film *Interstellar*, perhaps for the fourth time. I was deeply impressed by Nolan's previous film *Inception*, which I also probably watched five times already. Yesterday, I had to give *Inception* a 10-star rating for the first time in my life, as it just gets more interesting with every viewing. This 10-star rating is very special to me, as I have never rated any of the over 3,000 films I've watched so highly.

After this purely fantastical journey, I became curious about the science behind it. Then I realised that the scientific adviser for the film, Kip Thorne, actually wrote a book about it! I knew I had planned to read *Understanding Molecular Simulation* today, but since it’s the weekend, I decided to indulge in this investigation instead.

![On page 45, 4. Warped Time and Space, and Tidal Gravity](images/The Science of Interstella/Screenshot 2024-11-09 at 23.42.51.png)

What a beautiful way of interpreting general relativity! 

So far, having read up to Chapter 6, it has all been very interesting! But since Monday has arrived, it’s time to focus on my project again...

## 2024/11/14 - *Wetting Transition of Active Brownian Particles on a Thin Membrane* [@turci2021wetting]

*Cited by 20*

Unfortunately, not very interesting. The authors (my supervisors) investigated how ABPs interact with a planar surface with barrier strength $\epsilon_w$; decreasing this makes the surface resemble a thin penetrable (or porous) membrane. The research implications relate to artificial colloids or elementary swarm robots, but I’m not particularly interested in those systems.

![On page 2, Partial and complete wetting of droplets of active Brownian particles](images/241115 Reading Papers/Screenshot 2024-11-15 at 00.37.35.png)

The above image is a good summary of what the research is about, which coincidentally is from what I’m currently reading:

## 2024/11/14 - Partial and Complete Wetting of Droplets of Active Brownian Particles [@turci2024partial]

*Cited by 5*

It became quite technical from Chapter 2 onward, so I only read the introduction and the discussion & conclusions sections. I think I’m becoming increasingly uncertain about wanting to pursue my project. It doesn’t seem particularly mysterious or evoking my curiosity. I guess the highlight of this article was that, despite the inherent non-equilibrium mechanisms that engender phase separation and non-trivial flow patterns, the large-scale properties of the active model can be mapped onto an equilibrium one. This means that we can use techniques from equilibrium statistical mechanics, such as the renormalisation group, which I was hoping to learn deeply in my PhD. 

If there were a connection between surface phase transitions of vapour inside the cap of my RDA (rebuildable dripping atomiser), it might be more interesting and relating to my interests, but I think this is likely an equilibrium condition rather than something relevant to active matter. **I must ask next week.**

## 2024/11/15 - Motility-Induced Phase Separation [@cates2015motility]

*Cited by 1579 (Really surprising how much attention this topic has received!), also written by Michael Cates, who is the Lucasian Professor of Mathematics at the University of Cambridge*

This one, too, I only read the introduction and conclusion. I wanted to read this because MIPS keeps appearing in many papers. Essentially, this phenomenon occurs because active particles generically accumulate where they move more slowly. What’s really worth noting is that the run-and-tumble model is inspired by *E. coli*, while active Brownian particles are inspired by synthetic colloids. Am I interested in either system? No... I think I should have done this activity of skimming through many papers earlier to clarify what I’m truly interested in.

## 2024/11/15 - Proliferating Active Matter [@hallatschek2023proliferating] {#sec-proliferating-active-matter}

*Cited by 48*

I thought I should probably reread papers that inspired me to investigate active matter in my PhD almost a year ago. One particular article, even though it didn’t seem to attract much attention, was written by Keller: *‘Active Matter, Then and Now’* [@keller2016active]. I loved reading it so much that I probably read it three times. Skipping that one this time, I started reading *Proliferating Active Matter*, which I also really enjoyed. 

I remember the start of the article being quite strong, mentioning the book that I often reread, *What Is Life?* by Erwin Schrödinger, and also discussing the reshaping of the chessboard by the active matter paradigm (which was probably something to do with breaking mass-reversal symmetry?); as if I were playing the physics game like chess.

While reading (just before the chapter *Motile Proliferating Matter*), I got sidetracked because it wasn’t really interesting anymore. It seems my interests have shifted over the summer or possibly during the final stages of my MSci research project. It sort of makes sense when I think about it in terms of the timeline.

In January 2024, when I was applying for my PhD:

1. I wanted to pursue new physics. It appeared to me that two of my favourite fields (general relativity and quantum field theory) were already well-explored and largely complete.
2. I also wanted to gain more employability skills, as I hoped to work at Google DeepMind after my PhD. I thought a computationally driven project was essential for this.
3. Eager to formally learn about the renormalisation group, I looked into areas of new physics related to it. That’s when I discovered the emerging field of active matter. After reading Keller's account of its history and emergence, as well as McGivern's account of how it relates to cognition (which I should probably revisit tomorrow), I thought, *this is it!* This was the field I wanted to pursue! I met Nigel, who was such a brilliant and thoughtful person to talk with, and Francesco, who had already been amazing. I applied directly (and, in fact, only applied for this PhD, while my peers were applying for multiple positions).
4. In my second term, I was also in the process of interviewing with Samsung Electronics, as they invited me to take on a data scientist role (this happened in early January, I think?). The role and salary were very attractive. However, once I received the final offer, I had to think deeply about what I truly wanted to do. There were still unsolved mysteries I hoped to explore during my PhD. I was curious about fundamental questions like: *What is time? What is space?* I thought about relativity theory and how Einstein built his ideas around the concept of a universal clock, which doesn’t depend on the mechanisms of the clock itself. But these clocks are also within a gravitational field. Can we truly justify that reading a clock on an observer’s wrist is the correct way to measure time, given that all clocks are fundamentally based on electromagnetism?
5. So, I declined the job offer and decided to stay at university for another four years. Over the summer holidays, I continued reading neuroscience books (particularly on dreams and altered states of consciousness) and considered which topics would be interesting to investigate throughout my PhD.

Before sleeping, I started reading *Time and Quantum Clocks: A Review of Recent Developments* [@altaie2022time], which I found very interesting.

## 2024/11/17 - *Active Materials: Minimal Models of Cognition?* [@mcgivern2020active]

*Cited by 20*

The next morning, I felt compelled to reread this paper, as it was one of the key works that truly ignited my interest in the field of active matter.

![On page 8, 3. Minimal models (LGA stands for *lattice gas automata*)](images/241115 Reading Papers/Screenshot 2024-11-16 at 09.43.46-tile.png)

Yes! This was it! I was curious whether a simple model of active matter that I learn could be applied to biological systems by examining if they fall into the same universality class as a realistic model through the application of renormalisation group techniques.

Very interesting. I’m glad I read it once again. I wonder what my supervisors think about this article. I thought it would be useful to clarify whether I truly want to pursue the field of biophysics (as I’m not particularly interested in synthetic colloids), so I decided to revisit the biophysics lectures (see [2024/11/16 Watching Biophysics Lectures](notes.qmd#sec-biophysics-lectures)).

I thought it would be important to develop a good understanding of why active colloids or motile bacteria are significant. While exploring this, I found a list of nice articles by Philip Ball.

## 2024/11/20 - *Colloids Get Active* [@ball2013colloids]

*Cited by 5*

Philip Ball writes:  

> Bacterial swarms—the prototypical form of active matter—model the motile bacteria.  

which he mentioned is really important as active Brownian particles model the motile bacteria. I loved this very short *News & Views* article :) The cited paper also seems highly relevant, which begins with the abstract:  

> An active colloid is a suspension of particles that transduce free energy from their environment and use the energy to engage in *intrinsically non-equilibrium* activities such as growth, replication, and self-propelled motility. An obvious example of active colloids is a suspension of bacteria such as *Escherichia coli*, their physical dimensions being almost invariably in the colloidal range. Synthetic self-propelled particles have also become available recently, such as two-faced, or Janus, particles propelled by differential chemical reactions on their surfaces driving a self-phoretic motion. In these lectures, I give a pedagogical introduction to the physics of single-particle and collective properties of active colloids, focusing on self-propulsion. I will compare and contrast phenomena in suspensions of ‘swimmers’ with the behavior of suspensions of passive particles, where only Brownian motion (discovered by Robert Brown in granules from the pollen of the wild flower *Clarkia pulchella*) is relevant. I will pay particular attention to issues that pertain to performing experiments using these active particle suspensions, such as how to characterize the suspension’s swimming speed distribution, and include an appendix to guide physicists wanting to start culturing motile bacteria.

This was probably the first time I realised the significance of the colloidal scale!

## 2024/11/21 - From Clarkia to Escherichia and Janus: The physics of natural and synthetic active colloids [@poon2013clarkia]

*Cited by 74*

One video was introduced in this article about kinematic reversibility, really surprised!

<iframe width="560" height="315" src="https://www.youtube.com/embed/51-6QCJTAjU?si=7a3lQn3SGY1_Gp9D" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Even though the article was very interesting, it didn't seem closely related to my specific project, so I stopped reading after page 9 and skipped to the conclusion. However, I really liked how it also started with the basics of fluid dynamics.

In the conclusion, the author notes:  

> I have already mentioned one potential source of experimental difficulties for corresponding experiments using active particles: the rapid depletion of “fuel,” so that any putative novel collective effects need to be carefully scrutinized to rule out fuel depletion as a causative factor.  

I *think* this observation also applies to simulations, where particles are often assumed to have an infinite energy supply. And I especially loved the ending of the conclusion. Read it for yourself in the future again :)

## 2024/11/21 - *Animate Materials* [@ball2021animate]

*Cited by 12*

Philip Ball writes:  

> Ever since humans began constructing artifacts from materials—whether a stone bridge or a space rocket—we have regarded design as a fixed objective and have selected materials and structures that we believe will do the job as well as possible. Maintenance, repair, and alteration have then been regarded as additional tasks—necessary in most cases, but enacted post hoc. Typically, the aim has been to use materials that resist change: that will not break or corrode or deform. If these things happen, a whole new train of interventions is required. But for animate materials, responses to such changes are foreseen and accommodated from the outset.

I really like his articles (I should read some of his books!) because they make me imagine a lot of sci-fi scenes that I watched. The most immediate ones that come to mind are *Foundation* on Apple TV, where both human skin and spaceship exteriors are auto-healed, and the iconic opening scene of *Ghost in the Shell* where Major Kusanagi body is being constructed.

Also, Philip writes:

> One perspective would be to equate “animate” with an autonomous capacity to “act”—which is reflected in the perception that many of the properties considered desirable in animate materials are prefixed with “self-”: assembly, repair, sensing, diagnosing, reporting, protecting.

> One of the most obvious molecularscale differences between living and inanimate materials is the nature of the chemical bonding. Rocks and minerals tend to have rigid structures held together by strong (covalent or ionic) chemical bonds. In living systems, the molecules—such as proteins and nucleic acids—are generally dynamic in their conformation and interact with other molecules via weak interactions such as hydrogen bonding and hydrophobic forces. This enables information to be transmitted from place to place, and means that molecular entities can be assembled and disassembled on demand. The “looseness of structure” is surely also an essential feature for systems that are maintained away from equilibrium by a throughput of material and energy, and which can adapt to changes in the environment.

This is probably also the first time I realised the importance of hydrophobicity in non-equilibrium systems. Even though there was no huge overlap with active matter topics, it was a very interesting read.

But his another article, *Field work – the physics of sheep, from phase transitions to collective motion*, wasn't very exciting.